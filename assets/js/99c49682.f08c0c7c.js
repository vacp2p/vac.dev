"use strict";(self.webpackChunkvac_dev=self.webpackChunkvac_dev||[]).push([[6807],{10319:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"mdsecheck-method","metadata":{"permalink":"/rlog/mdsecheck-method","source":"@site/rlog/2025-02-28-mdsecheck-method.mdx","title":"The MDSECheck method: choosing secure square MDS matrices for P-SP-networks","description":"This article introduces MDSECheck method \u2014 a novel approach","date":"2025-02-28T23:00:00.000Z","formattedDate":"February 28, 2025","tags":[],"readingTime":17.11,"hasTruncateMarker":true,"authors":[{"name":"Aleksei Vambol","github":"AlekseiVambol","key":"aleksei"}],"frontMatter":{"title":"The MDSECheck method: choosing secure square MDS matrices for P-SP-networks","date":"2025-02-28T23:00:00.000Z","authors":"aleksei","published":true,"slug":"mdsecheck-method","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":4},"nextItem":{"title":"Vac 2024 Year in Review","permalink":"/rlog/2024-recap"}},"content":"This article introduces MDSECheck method \u2014 a novel approach \\nto checking square MDS matrices for unconditional security\\nas the components of affine permutation layers of P-SP-networks.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nMaximum distance separable (MDS) matrices play a significant role \\nin algebraic coding theory and symmetric cryptography.\\nIn particular, square MDS matrices are commonly used in \\naffine permutation layers of \\npartial substitution-permutation networks (P-SPNs). \\nThese are widespread designs of \\nthe modern symmetric ciphers and hash functions. \\nA classic example of the latter is Poseidon [[1]](#references), \\na well-known hash function used in zk-SNARK proving systems.\\n\\nSquare MDS matrices differ in terms of security \\nthat they are able to provide for P-SPNs.\\nThe use of some such matrices in certain P-SPNs may result in existence \\nof infinitely long subspace trails of small period for the latter, \\nwhich make them vulnerable to differential cryptanalysis [[2]](#references).\\n\\nTwo methods for security checking of square MDS matrices for P-SPNs \\nhave been proposed in [[2]](#references).\\nThe first one, which is referred to as the three tests method \\nin the rest of the article, is aimed at security checking for \\na specified structure of the substitution layer of a P-SPN.\\nThe second method, which is referred here as the sufficient test method, \\nhas been designed to determine whether a square MDS matrix satisfies \\na sufficient condition of being secure regardless of the structure of \\na P-SPN substitution layer, i.e. to check whether the matrix belongs to \\nthe class of square MDS matrices, which are referred to \\nas unconditionally secure in the current article.\\n\\nThis article aims to introduce MDSECheck method \u2014 \\na novel approach to checking square MDS matrices for unconditional security, \\nwhich has already been implemented in the Rust programming language as \\nthe library crate [[3]](#references).\\nThe next sections explain the notions mentioned above, \\ndescribe the MDSECheck method as well as its mathematical foundations, \\nprovide a brief overview of the MDSECheck library crate \\nand outline possible future research directions.\\n\\n## MDS matrix: how to define and construct\\n\\nAn $m$ x $n$ matrix $M$ over a finite field is called MDS, \\nif and only if for distinct $n$-dimensional column vectors $v_1$ and $v_2$ \\nthe column vectors $v_1 \\\\: | \\\\: M v_1$ and $v_2 \\\\: | \\\\: M v_2$, \\nwhere $|$ stands for vertical concatenation, \\ndo not coincide in $n$ or more components.\\nThe set of all possible column vectors $v \\\\: | \\\\: M v$ for \\nsome fixed matrix $M$ is a systematic MDS code, i.e. \\na linear code, which contains input symbols on their original positions \\nand achieves the Singleton bound.\\nThe latter property results in good error-correction capability.\\n\\nThere are several equivalent definitions of MDS matrices, \\nbut the next one is especially useful for constructing them \\ndirectly by means of algebraic methods.\\nA matrix over a finite field is called MDS, \\nif and only if all its square submatrices are nonsingular.\\nThe matrix entries and the matrix itself are also considered submatrices.\\n\\nOne of the most efficient and straightforward methods to directly construct \\nan MDS matrix is generating a Cauchy matrix [[4]](#references). \\nSuch an $m$ x $n$ matrix is defined using\\n$m$-dimensional vector $x$ and $n$-dimensional vector $y$, \\nfor which all entries in the concatenation of $x$ and $y$ are distinct.\\nThe entries of the Cauchy matrix are described by the formula \\n$M_{i, j} = 1 \\\\: / \\\\: (x_i - y_j)$.\\nIt is obvious that any submatrix of a Cauchy matrix is also a Cauchy matrix.\\nThe Cauchy determinant formula [[5]](#references) implies that \\nevery square Cauchy matrix is nonsingular.\\nThus, Cauchy matrices satisfy the second definition of MDS matrices.\\n\\n## Partial substitution-permutation networks\\n\\nDescribing SPNs in algebraic terms is convenient, \\nso this approach has been chosen for this article.\\nSPNs are designs of the symmetric cryptoprimitives, \\nwhich operate on an internal state, which is represented \\nas an $n$-dimensional vector over some finite field,\\nand update this state iteratively by means of \\nthe round transformations described below.\\n\\nEach round begins with an optional update of the internal state by \\nadding to its components some input data or extraction of \\nsome of these components as the output data.\\nThis optional step depends on the specific cryptoprimitive \\nand the current round number.\\nThe next step is called the nonlinear substitution layer\\nand lies in replacing the $i$-th component of the internal state \\nwith $S_i(c)$ for each $i \\\\in [1..n]$, where $c$ is the component value \\nand $S_i(x)$ is a nonlinear invertible function over the finite field.\\nThe function $S_i(x)$ is specific to the cryptoprimitive and called an S-Box. \\nThe final step, which is known as the affine permutation layer, \\nreplaces the internal state with $M X + c$, \\nwhere $X$ is the current internal state, \\n$M$ is a nonsingular square matrix and \\n$c$ is the vector of the round constants.\\nThe value of $c$ is specific to the cryptoprimitive \\nand the current round number, \\nwhile $M$ typically depends only on the cryptoprimitive.\\nThe data flow diagram for an SPN is given below.\\n```\\n..................................                         \\n   \u2502        \u2502        \u2502        \u2502                           \\n   \u25bc        \u25bc        \u25bc        \u25bc                           \\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \\n\u2502 Optional addition / extraction \u2502 <\u2500\u2500\u2500\u2500\u2500> Input / output\\n\u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518                         \\n   \u25bc        \u25bc        \u25bc        \u25bc                            \\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510                         \\n\u2502S\u2081(x)\u2502  \u2502S\u2082(x)\u2502  \u2502 ... \u2502  \u2502S\u2099(x)\u2502                         \\n\u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518                         \\n   \u25bc        \u25bc        \u25bc        \u25bc                            \\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \\n\u2502       Affine permutation       \u2502                         \\n\u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518                         \\n   \u25bc        \u25bc        \u25bc        \u25bc                            \\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \\n\u2502 Optional addition / extraction \u2502 <\u2500\u2500\u2500\u2500\u2500> Input / output\\n\u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518                         \\n   \u25bc        \u25bc        \u25bc        \u25bc                            \\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510                         \\n\u2502S\u2081(x)\u2502  \u2502S\u2082(x)\u2502  \u2502 ... \u2502  \u2502S\u2099(x)\u2502                         \\n\u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518                         \\n   \u25bc        \u25bc        \u25bc        \u25bc                            \\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \\n\u2502       Affine permutation       \u2502                         \\n\u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518                         \\n   \u25bc        \u25bc        \u25bc        \u25bc                            \\n..................................                         \\n```\\nPartial SPNs are modifications of SPNs, \\nwhere for certain rounds some S-Boxes are replaced with \\nthe identity functions to reduce computational efforts [[2]](#references).\\nFor example, the nonlinear substitution layers of the partial rounds of \\nPoseidon update only the first internal state component [[1]](#references).\\nIn the case of P-SPNs, security considerations commonly demand to choose $M$ \\nas a square MDS matrix, because these matrices provide \\nperfect diffusion property for the affine permutation layer [[6]](#references).\\nPossessing this property means ensuring that \\nany two $n$-dimensional internal states, \\nwhich differ in exactly $t$ components, \\nare mapped by the affine permutation layer to \\ntwo new internal states that differ in at least $n - t + 1$ components.\\n\\n## Square MDS matrix security check in the context of P-SPNs\\n\\nCertain square MDS matrices should not be used in certain P-SPNs \\nto avoid making them vulnerable to differential cryptanalysis,\\nsince it may exploit the existence of infinitely long subspace trails \\nof small period for vulnerable P-SPNs. [[2]](#references).\\nSuch matrices are called insecure with respect to particular P-SPNs.\\n\\nAn infinitely long subspace trail of period $l$ exists for a P-SPN, \\nif and only if there is a proper subspace \\nof differences of internal state vectors,\\nsuch that if for a pair of initial internal states \\nthe difference belongs to this subspace, \\nthen the difference for the new internal states, \\nwhich are obtained from the initial ones \\nby means of the same $l$-round transformation,\\nalso belongs to this subspace [[2]](#references).\\n\\nTwo methods for checking square MDS matrices for suitability for P-SPNs \\nin terms of existence of infinitely long subspace trails \\nhave been proposed in [[2]](#references).\\nThe three tests method is aimed at checking \\nwhether using a specified matrix for a P-SPN \\nwith a specified structure of the substitution layer \\nleads to existence of infinitely long subspace trails of period $p$ \\nfor this P-SPN for all $p$ no larger than a given $l$.\\nThe sufficient test method has been designed to determine\\nwhether a square MDS matrix satisfies a sufficient condition \\nof non-existence of infinitely long subspace trails of period $p$ \\nfor P-SPNs using this matrix for all $p$ no larger than a specified $l$.\\n\\nThe sufficient test method is a direct consequence of \\nTheorem 8 in [[2]](#references) and consists in checking that \\nthe minimal polynomial of the $p$-th power of the tested matrix \\nhas maximum degree and is irreducible for all $p \\\\in [1..l]$.\\nThe aforesaid sufficient non-existence condition is satisfied by the matrix, \\nif and only if all the checks yield positive results. \\n\\nIt is convenient to define \\nthe unconditional P-SPN security level of the square MDS matrix as follows: \\nthis level is $l$ for the matrix $M$, \\nif and only if the minimal polynomials of $M$, $M^2$, $...$, $M^l$ \\nhave maximum degree and are irreducible, \\nbut for $M^{l \\\\: + \\\\: 1}$ the minimal polynomial does not have this property.\\nUsing this definition, the purpose of the sufficient test method \\ncan be described as checking whether \\nthe unconditional P-SPN security level of the specified matrix \\nis no less than a given bound.\\n\\n## MDSECheck method: getting rid of the matrix powers\\n\\nThe MDSECheck method, whose name is derived from \\nthe words \\"MDS\\", \\"security\\", \\"elaborated\\" and \\"check\\", \\nhas the same purpose as the sufficient test method, \\nbut achieves it differently.\\nThe differences of the first method from the latter \\nand approaches to implementing them can be described as follows:\\n\\n1. Computation and verification of minimal polynomials \\nof $M^2$, $M^3$, $...$, $M^l$, where $M$ is \\nthe tested $n$ x $n$ matrix over $GF(q)$ \\nand $l$ is the security level bound,\\nhas been replaced with checks for the corresponding powers \\nof a root of the characteristic polynomial of $M$ \\nfor non-presence in nontrivial subfields of $GF(q^n)$.\\n\\n   1. The non-presence check is performed without \\nstraightforward consideration of all nontrivial subfields of $GF(q^n)$.\\nThe root is checked only for non-presence in the subfields \\n$GF(q^{n \\\\: / \\\\: p_1})$, $GF(q^{n \\\\: / \\\\: p_2})$, $...$, \\n$GF(q^{n \\\\: / \\\\: p_d})$, where $p_1$, $p_2$, $...$, $p_d$ \\nare all prime divisors of $n$.\\n\\n   1. The non-presence check reuses some data computed during \\nthe checking for irreducibility the minimal polynomial of $M$, \\nwhich in this case coincides with $f(y)$ \\ndesignating the characteristic polynomial of $M$.\\nThe values of $y^{q^{n \\\\: / \\\\: p_j}} \\\\mod f(y)$ are saved \\nfor each $j \\\\in [1..d]$ during the irreducibility check \\nto replace exponentiations with sequential computations \\nof $(y^i)^{q^{n \\\\: / \\\\: p_j}} \\\\mod f(y)$ from \\n$(y^{(i \\\\: - \\\\: 1)})^{q^{n \\\\: / \\\\: p_j}} \\\\mod f(y)$ \\nas its product with $y^{q^{n \\\\: / \\\\: p_j}} \\\\mod f(y)$.\\n\\n1. The check of the minimal polynomial of $M$ \\nfor irreducibility and maximum degree \\nis performed without unconditional computation of this polynomial.\\nThis computation has been replaced with the Krylov method fragment, \\nwhich consists in building and solving \\nonly one system of linear equations over $GF(q)$.\\nIf $M$ has an irreducible minimal polynomial of maximum degree, \\nthen its coefficients are trivially determined from the system solution.\\nIf the system is degenerate, \\nthen the minimal polynomial of $M$ does not have such properties.\\n\\nThe correctness of the first distinctive feature can be proven as follows.\\nVerifying that the minimal polynomial of a matrix \\nis of maximum degree and irreducible \\nis equivalent to verifying that \\nthe characteristic polynomial of this matrix is irreducible, \\nbecause the minimal polynomial divides the characteristic one.\\nAlso, it is trivially proven that for a matrix with such a minimal polynomial \\nit is equal to the characteristic polynomial.\\nThus, the required checks for the matrices $M^2$, $M^3$, $...$, $M^l$ \\ncan be done by checking their characteristic polynomials for irreducibility.\\n\\nLet $M$ be $n$ x $n$ matrix over $GF(q)$, \\nwhose minimal polynomial is of maximum degree and irreducible.\\nThe statements in the previous paragraph imply that $f(y)$, \\nwhich is the $n$-degree characteristic polynomial of $M$, is irreducible.\\nConsider $M$ over the extension field $GF(q^n)$, \\nwhich is the splitting field of $f(y)$.\\nLet $\u03b1 \\\\in GF(q^n)$ be a root of $f(y)$.\\nAccording to standard results from the Galois field theory, \\n$\u03b1$, $\u03b1^q$, $\u03b1^{q^2}$, $...$, $\u03b1^{q^{n \\\\: - \\\\: 1}}$ \\nare distinct roots of $f(y)$ [[7]](#references).\\nThus, these powers of $\u03b1$ are $n$ distinct eigenvalues of $M$.\\nHence, due to matrix similarity properties, there is some matrix $S$ \\nsuch that $S M S^{-1} = D$, where $D$ is the diagonal matrix, \\nwhose nonzero elements are \\n$\u03b1$, $\u03b1^q$, $\u03b1^{q^2}$, $...$, $\u03b1^{q^{n \\\\: - \\\\: 1}}$.\\nTherefore, $S M^i S^{-1} = D^i$, \\nso the roots of the characteristic polynomial of $M^i$ \\nare $\u03b1^i$, $(\u03b1^q)^i$, $(\u03b1^{q^2})^i$, $...$, $(\u03b1^{q^{n \\\\: - \\\\: 1}})^i$.\\nIf the minimal polynomial of $\u03b1^i$ has degree less than $n$, \\nthen the characteristic polynomial of $M^i$ is divisible \\nby this minimal polynomial, \\nwhile $\u03b1^i$ lies in some nontrivial subfield of $GF(q^n)$.\\nOne of the fields isomorphic to this subfield is a residue class ring \\nof polynomials modulo the minimal polynomial of $\u03b1^i$ [[7]](#references).\\nIf the minimal polynomial of $\u03b1^i$ is of degree $n$, \\nthen the characteristic polynomial of $M^i$ \\nequals this minimal polynomial and therefore is irreducible, \\nwhile $\u03b1^i$ does not lie in any nontrivial subfield of $GF(q^n)$.\\nIn this case, $1$, $\u03b1^i$, $(\u03b1^i)^2$, $...$, $(\u03b1^i)^{n \\\\: - \\\\: 1}$ \\nare linearly independent as distinct roots of \\nan irreducible polynomial over a finite field [[7]](#references), \\nso any field containing $\u03b1^i$ has at least $q^n$ elements \\nand therefore cannot be a trivial subfield of $GF(q^n)$.\\nThus, checking the characteristic polynomials of \\nthe matrices $M^2$, $M^3$, $...$, $M^l$ for irreducibility \\nis equivalent to verifying that $\u03b1^2$, $\u03b1^3$, $...$, $\u03b1^l$ \\ndo not lie in any nontrivial subfield of $GF(q^n)$.\\n\\nThe last sentences of the two previous paragraphs imply the following:\\nverifying that the minimal polynomials of $M^2$, $M^3$, $...$, $M^l$ \\nare of maximum degree and irreducible can be performed \\nby verifying that the corresponding powers of a root of \\nthe characteristic polynomial of the $n$ x $n$ matrix $M$ over $GF(q)$ \\ndo not belong to any nontrivial subfield of $GF(q^n)$. $\\\\blacksquare$\\n\\nThe approaches to implementing the first distinctive feature \\ncan be explained and proven to be correct as follows.\\nSince $GF(q^w)$ is a nontrivial subfield of $GF(q^u)$ \\nif and only if $w$ divides $u$ and $w < u$ [[7]](#references), \\nthe presence of some $\u03b5$ in $GF(q^h)$, \\nwhich is a nontrivial subfield of $GF(q^n)$, \\nimplies that $\u03b5 \\\\in GF(q^{n \\\\: / \\\\: \u03bd})$ for some prime $\u03bd$ dividing $n$, \\nbecause $h$ divides the quotient of $n$ and some of its prime factors.\\nThus, checking that some value does not belong to subfields \\n$GF(q^{n \\\\: / \\\\: p_1})$, $GF(q^{n \\\\: / \\\\: p_2})$, $...$, \\n$GF(q^{n \\\\: / \\\\: p_d})$, \\nwhere $p_1$, $p_2$, $...$, $p_d$ are all prime divisors of $n$, \\nis equivalent to checking this value for \\nnon-presence in nontrivial subfields of $GF(q^n)$.\\n\\nChecking for irreducibility the minimal polynomial of $M$ \\nis performed by means of Algorithm 2.2.9 in [[8]](#references)\\nand consists in sequential computation of \\n$y^p \\\\mod f(y)$, $y^{p^2} \\\\mod f(y)$, $...$, \\n$y^{p^{\\\\lfloor n \\\\: / \\\\: 2 \\\\rfloor}} \\\\mod f(y)$ \\nand checking that $GCD(y^{p^i} \\\\mod f(y) \\\\: - \\\\: y, f(y)) = 1$ \\nfor each $i \\\\in [1..\\\\lfloor n \\\\: / \\\\: 2 \\\\rfloor]$, \\nwhere $f(y)$ is the characteristic polynomial of $M$ \\nand coincides with the minimal polynomial in this case.\\nThe optimized root non-presence check is performed \\nby checking that for each $i \\\\in [2..l]$ for each $j \\\\in [1..d]$ \\nthe value of $((y^i)^{q^{n \\\\: / \\\\: p_j}} - y^i) \\\\mod f(y)$ is nonzero.\\nThis approach is based on the following standard results \\nfrom the Galois field theory [[7]](#references):\\n\\n- $GF(q^n)$ is isomorphic to the residue class ring of \\nunivariate polynomials in $y$ modulo $f(y)$, \\nbecause at this point $f(y)$ is known to be irreducible, \\nand some root of $f(y)$ is mapped by this isomorphism to \\nthe residue class the polynomial $y$ in this ring.\\n\\n- All elements of a finite field $GF(q^w)$ and only they \\nare roots of $y^{q^w} - y$.\\n\\nThe expression $((y^i)^{q^{n \\\\: / \\\\: p_j}} - y^i) \\\\mod f(y)$ \\ncan be rewritten as $((y^{q^{n \\\\: / \\\\: p_j}})^i - y^i) \\\\mod f(y)$,\\nwhich can be computed without exponentiation as the product of \\n$(y^{(i \\\\: - \\\\: 1)})^{q^{n \\\\: / \\\\: p_j}} \\\\mod f(y)$ and \\n$y^{q^{n \\\\: / \\\\: p_j}} \\\\mod f(y)$, \\nwhich has been saved during the irreducibility check.\\n\\nThe second distinctive feature can be \\nexplained and proven to be correct in following way.\\nThe $n$ x $n$ matrix $M$ does not have a minimal polynomial of maximum degree, \\nif some Krylov subspace of order $n$ for it is not $n$-dimensional.\\nIndeed, the minimal polynomial of the matrix is divisible \\nby the minimal polynomial of the restriction of \\nthis linear operator to an arbitrary subspace, \\nand in the considered case the latter polynomial has degree less than $n$,\\nbecause the degree of the minimal polynomial of a linear operator cannot\\nexceed the dimension of the subspace the operator acts on.\\nThus, an unconditional computation of the minimal polynomial of $M$ \\nis not required to determine \\nwhether this polynomial is irreducible and has maximum degree.\\nUsing this computation has been replaced with the Krylov method fragment,\\nwhich consists in choosing any nonzero $n$-dimensional column vector $v$ \\nand solving the system of linear equations $A X = b$, \\nwhere $A$ is an $n$ x $n$ matrix, \\nwhose columns are $v$, $M v$, $M^2 v$, $...$, $M^{n \\\\: - \\\\: 1} v$, \\nand $b$ is $M^n v$.\\nIf $A$ is singular, \\nthe minimal polynomial of $M$ is reducible or does not have maximum degree, \\nso checking $M$ has been accomplished; \\notherwise, $f(y)$, which is the minimal and characteristic polynomial of $M$, \\ncan be expressed as $y^n - X_{n \\\\: - \\\\: 1} y^{n \\\\: - \\\\: 1} - \\nX_{n \\\\: - \\\\: 2} y^{n \\\\: - \\\\: 2} - \u2026 - X_1 y - X_0$.\\n\\nThe steps of MDSECheck method can be summarized as follows:\\n\\n1. The square MDS matrix $M$ over $GF(q)$ \\nand the unconditional P-SPN security level bound $l$ are received as inputs.\\n\\n1. The Krylov method fragment is used to compute the minimal polynomial of $M$.\\nIf the computation fails, then $M$ is not unconditionally secure, \\nso the check of $M$ is complete.\\nIf it succeeds, then the minimal polynomial has maximum degree \\nand, therefore, coincides with the characteristic polynomial of $M$.\\n\\n1. Algorithm 2.2.9 is used \\nto check for irreducibility the minimal polynomial of $M$,\\nwhich is also the characteristic polynomial of $M$ in this case.\\nSome data computed during this step is saved to be reused at the next one.\\nIf the polynomial is reducible, then the check of $M$ is complete, \\nbecause $M$ has been found to be not unconditionally secure.\\n\\n1. The values of $\u03b1^2$, $\u03b1^3$, $...$, $\u03b1^l$, \\nwhere $\u03b1$ is a root of the characteristic polynomial of $M$, \\nare sequentially checked for non-presence in \\nnontrivial subfields of $GF(q^n)$ as described above.\\nIf $\u03b1^i$ belongs to some nontrivial subfield of $GF(q^n)$, \\nthen the unconditional P-SPN security level of $M$ is $i \\\\: - \\\\: 1$, \\nso the check of $M$ is complete.\\nIf all the values do not belong to such a subfield, \\nthen the unconditional P-SPN security level is at least $l$.\\n\\n## MDSECheck library crate: implementation in Rust\\n\\nThe library crate [[3]](#references) provides tools for \\ngenerating random square Cauchy MDS matrices over prime finite fields \\nand applying the MDSECheck method \\nto check such matrices for unconditional security. \\nThe used data types of field elements and polynomials are provided by \\nthe crates ark-ff [[9]](#references) and ark-poly [[10]](#references).\\nThe auxiliary tools in the crate modules are accessible as well.\\n\\nGenerating by means of this crate a 10 x 10 MDS matrix, \\nwhich is defined over the BN254 scalar field [[11]](#references) \\nand has unconditional P-SPN security level is 1000, \\ntakes less than 60 milliseconds on average \\nfor the laptop with the processor Intel\xae Core\u2122 i9-14900HX, \\nwhose maximum clock frequency is 5.8 GHz.\\n\\n## Conclusion\\n\\nThe MDSECheck method proposed in this article is a novel approach \\nto checking square MDS matrices for unconditional security \\nas the components of affine permutation layers of P-SPNs.\\nIt has been implemented as a practical library crate \\nfor generating unconditionally secure square MDS matrices \\nfor P-SPNs over prime finite fields.\\n\\nThe future research directions may include \\ntheoretical and experimental studies of performance of approaches,\\nwhich use the MDSECheck method \\nto generate unconditionally secure square MDS matrices for P-SPNs.\\n\\n## References\\n\\n1. L. Grassi, D. Khovratovich, C. Rechberger, A. Roy, M. Schofnegger. \\"[POSEIDON: A New Hash Function for Zero-Knowledge Proof Systems (Updated Version)](https://eprint.iacr.org/2019/458.pdf)\\".\\n1. L. Grassi, C. Rechberger, M. Schofnegger. \\"[Proving Resistance Against Infinitely Long Subspace Trails: How to Choose the Linear Layer](https://eprint.iacr.org/2020/500.pdf)\\".\\n1. The page \\"[mdsecheck](https://crates.io/crates/mdsecheck)\\" on crates.io.\\n1. Y. Kumar, P. Mishra, S. Samanta, K. Chand Gupta, A. Gaur. \\"[Construction of all MDS and involutory MDS matrices](https://arxiv.org/pdf/2403.10372)\\".\\n1. The page \\"[Value of Cauchy Determinant](https://proofwiki.org/wiki/Value_of_Cauchy_Determinant)\\" on proofwiki.org.\\n1. T. Silva, R. Dahab \\"[MDS Matrices for Cryptography](https://www.ic.unicamp.br/~reltech/PFG/2021/PFG-21-43.pdf)\\".\\n1. S. Huczynska, M. Neunh\xf6ffer. \\"[Finite Fields](http://www.math.rwth-aachen.de/~Max.Neunhoeffer/Teaching/ff2012/ff2012.pdf)\\"\\n1. R. Crandall, C. Pomerance. \\"[Prime Numbers: A Computational Perspective](http://thales.doa.fmph.uniba.sk/macaj/skola/teoriapoli/primes.pdf)\\" (2nd edition).\\n1. The page \\"[ark-ff](https://crates.io/crates/ark-ff)\\" on crates.io.\\n1. The page \\"[ark-poly](https://crates.io/crates/ark-poly)\\" on crates.io.\\n1. The page \\"[ark-bn254](https://crates.io/crates/ark-bn254)\\" on crates.io."},{"id":"2024-recap","metadata":{"permalink":"/rlog/2024-recap","source":"@site/rlog/2024-12-20-Vac-2024.md","title":"Vac 2024 Year in Review","description":"In this post, we recap Vac\'s achievements in 2024 and look forward to 2025.","date":"2025-01-09T18:30:00.000Z","formattedDate":"January 9, 2025","tags":[],"readingTime":8.205,"hasTruncateMarker":true,"authors":[{"name":"Vac","key":"Vac"}],"frontMatter":{"title":"Vac 2024 Year in Review","date":"2025-01-09T18:30:00.000Z","authors":"Vac","published":true,"slug":"2024-recap","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"The MDSECheck method: choosing secure square MDS matrices for P-SP-networks","permalink":"/rlog/mdsecheck-method"},"nextItem":{"title":"Vac 101: Climbing Merkle Trees","permalink":"/rlog/climbing-merkle-trees"}},"content":"In this post, we recap Vac\'s achievements in 2024 and look forward to 2025.\\n\\n\x3c!--truncate--\x3e\\n\\nWith 2024 now behind us and a new year ahead, \\nVac is proud to reflect on the milestones and breakthroughs that defined another year of researching and developing free and open digital public goods for the [Institute of Free Technology](https://free.technology/) and wider web3 ecosystem.\\n\\nVac comprises various subteams and service units, each with its own focus. \\nBelow, we celebrate each unit\'s achievements and look forward to its 2025 plans.\\n\\n## Nescience\\nNescience is our state separation architecture that aims to enable private transactions and provide a general-purpose execution environment for classical applications. \\n\\n### Highlights\\nThis year, the Nescience state separation architecture moved from exploration to real progress, \\ntaking significant steps towards building a functional and reliable system. \\nThe team focused on turning ideas into something real, \\ntesting the proposed architecture, \\nand understanding its strengths and weaknesses.\\n\\n* ZkVM exploration and benchmarks  \\n  * Published [deep reviews of 23 existing zkVMs](https://vac.dev/rlog/zkVM-explorations/)  \\n  * [Benchmarked the performance of the six zkVMs](https://vac.dev/rlog/zkVM-testing/) that best fit Nescience  \\n* Defined the NSSA architecture  \\n  * Brought clarity to NSSA\u2019s design and explained the system\u2019s architecture [in a lengthy exploratory blog post](https://vac.dev/rlog/Nescience-state-separation-architecture/)   \\n* Built the sandboxed testnet  \\n  * Designed the first version of the node specification  \\n  * All core components (execution types, UTXOs, cryptographic primitives) implemented and being tested  \\n  * Testing the performance of all execution types in various scenarios \\n\\nWe also made progress on the essential parts of NSSA\u2019s system, including:\\n\\n* Key protocol for secure key management  \\n* Execution types and circuits for reliable computation  \\n* UTXO specification to manage state transitions effectively  \\n* Cryptography module to ensure privacy and security\\n\\n### Looking forward\\nIn 2025, the Nescience team plans to double down on what works, fix what doesn\u2019t, and push NSSA closer to real-world use.\\n\\n* Sandboxed testnet data analysis \u2013 the sandboxed testnet will be our primary data source that we will analyse to identify issues, limitations, and areas for improvement.   \\n* Expanding the node \u2013 expand sandboxed components into a full node implementation with rigorous testing and iterative optimization (to bridge the gap between proof of concept and production readiness).   \\n* Finalizing the architecture and RFC \u2013 after completing NSSA\u2019s architecture, we will draft an RFC to ensure transparency and enable greater collaboration with the broader ecosystem.   \\n* Testing real-life scenarios \u2013 applying NSSA to diverse, practical use cases to assess its adaptability, performance, and impact.   \\n* Ongoing optimization \u2013 ensure NSSA is robust, efficient, and ready to scale. \\n\\n## Token Economics (TKE)\\nThe TKE Service Unit works closely with IFT portfolio projects to design and implement crypto-economic incentive structures. \\n\\n### Highlights\\n* Formalized and implemented [Codex](https://codex.storage/) economic incentives in the Litepaper and simulations  \\n* Orchestrated Status Network incentive structure and smart contract implementation  \\n* Started building [Nomos\u2019s](https://nomos.tech/) economic model  \\n* Consulted and provided analysis of incentives for the Logos Operators ordinals project  \\n* Drove discussions on the economic sustainability of [Waku](https://waku.org/);\\nhelped define RLN membership and its payment mechanism\\n\\n### Looking forward\\nIn 2025, TKE will continue to support IFT portfolio projects, \\nworking toward economic sustainability while strengthening relationships within the organization. \\nAdditionally, the service unit aims to continue building its external reputation through partnerships and publications of relevant work on the [Vac forum](https://forum.vac.dev/).  \\n\\n## Quality Assurance (QA)\\nThe QA Service Unit focuses on the development and execution of comprehensive test plans, \\nincluding implementing unit and interoperability testing. \\n\\n### Highlights\\n* Matured Waku interoperability testing framework with coverage for all major protocols and features  \\n* Began collaboration with Nomos, contributing to unit and integration testing  \\n* Partnered with the [Status](https://status.app/) team to test message reliability under unstable network conditions\\n\\n### Looking forward\\n* Extend collaboration with the Waku team on go-waku bindings and message reliability testing  \\n* Cement working relationship with the Nomos team through the building of an E2E testing framework for higher-level node validation  \\n* Work closely with Status\u2019s QA team to enhance the functional testing framework  \\n* Continue work on nim-libp2p testing  \\n* Expand collaboration to additional projects\\n\\n## RFC\\nThe RFC Service Unit takes on the responsibility of shepherding and editing specifications for IFT projects.\\nThe unit acts as a linchpin for ensuring standardized and interoperable protocols within the IFT ecosystem.\\n\\n### Highlights\\n* Working to implement RFC culture across the IFT ecosystem  \\n* Began editorial work for several IFT portfolio projects: Status, Nomos, Waku, and Codex.  \\n* Reworked our standards with regard to writing RFCs to a consensus-oriented specification system\\n\\n### Looking forward\\n* Continue to implement RFC culture across the IFT ecosystem   \\n* Broaden the number of RFCs produced\\n\u2013 particularly for IFT portfolio projects nearing public releases  \\n* Include new projects with the [rfc-index](https://rfc.vac.dev/)\\n* Encourage external projects requiring RFCs to establish relationships with the service unit \\n\\n## Applied Cryptography and ZK (ACZ)\\nThe ACZ Service Unit focuses on cryptographic solutions and zero-knowledge proofs, \\nenhancing the security, privacy, and trustworthiness of IFT portfolio projects \\nand contributing to the overall integrity and resilience of the decentralized web ecosystem. \\n\\n### Highlights\\n* Researched a libp2p mix protocol and first proof-of-concept implementation (including ping and GossipSub over mix)  \\n* Researched a decentralized version of MLS (message layer security) with a first proof of concept  \\n* Released Zerokit [v0.6.0](https://github.com/vacp2p/zerokit/releases/tag/v0.6.0)\\nand [v0.5.0](https://github.com/vacp2p/zerokit/releases/tag/v0.5.0)  \\n* Added [gnark RLN implementation](https://github.com/vacp2p/gnark-rln)  \\n* Released Stealth Address Kit [v0.3.1](https://github.com/vacp2p/stealth-address-kit/releases/tag/v0.3.1),\\n[v0.2.0](https://github.com/vacp2p/stealth-address-kit/releases/tag/v0.2.0),\\nand [v0.1.0](https://github.com/vacp2p/stealth-address-kit/releases/tag/v0.1.0)  \\n* Published:   \\n  * [Verifying RLN Proofs in Light Clients with Subtrees](https://vac.dev/rlog/rln-light-verifiers/)  \\n  * [RLN-v3: Towards a Flexible and Cost-Efficient Implementation](https://vac.dev/rlog/rln-v3/)\\n\\n### Looking forward\\n* Ensure libp2p mix protocol is production-ready\\nand support with the publishing of a paper and blog posts  \\n* Ensure decentralized MLS is production-ready\\nand support with the publishing of a paper and blog posts \\n* Begin explorations of additional research topics  \\n* Release [Zerokit v0.7](https://github.com/vacp2p/zerokit/issues/271) and future versions\\n\\n## P2P\\nThe P2P Service Unit specializes in peer-to-peer technologies \\nand develops nim-libp2p, improves the libp2p GossipSub protocol, and assists IFT portfolio projects with the integration of P2P network layers.\\n\\n### Highlights\\n* Analysis and work on libp2p GossipSub improvements  \\n* Published:   \\n  * [Libp2p GossipSub IDONTWANT Message Performance Impact](https://vac.dev/rlog/gsub-idontwant-perf-eval/)  \\n* PR to libp2p specifications about specific lib2p GossipSub improvements we researched and tested https://github.com/libp2p/specs/pull/654\\n\\n### Looking forward\\n* Add new features to nim-libp2p:\\nQUIC transport, web transport  \\n* Update specifications for libp2p GossipSub,\\naiming to significantly improve its performance \\n\\n## Distributed Systems Testing (DST)\\nThe DST Service Unit\u2019s primary objective is to assist IFT portfolio projects in understanding the scaling behavior of their nodes within larger networks. \\nBy conducting thorough regression testing, the DST unit helps ensure the reliability and stability of projects.\\n\\n### Highlights\\n* DST compute resources transitioned from a hosted environment to a dedicated Vac Lab,\\nenabling better customization of resources and adding significantly more compute power\\n\u2013 enabled much higher and more stable simulations (several hundred nodes to several thousand) and enhanced environmental control.  \\n* Maintained monthly regression simulations for both Waku and Nim-libp2p,\\nhelping us to detect several issues and ensure that future versions do not introduce new ones.  \\n* Successfully simulated and obtained results for all Waku protocols, relaying feedback to the team.\\n\\n### Looking forward\\n* More testing and simulations for Codex and Nomos  \\n* Develop useful tools for all IFT portfolio projects \u2013 e.g. a Log Parser tool and data dashboard\\n\\n## Nim\\nSeveral IFT portfolio projects use the Nim ecosystem for its efficiency. \\nThe Nim Service Unit is responsible for the development and maintenance of Nim tooling.\\n\\n### Highlights\\n* Released Nim-libp2p ([v1.7.1](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.7.1),\\n[v1.7.0](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.7.0),\\n[v1.6.0](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.6.0),\\n[v1.5.0](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.5.0),\\n[v1.4.0](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.4.0),\\n[v1.3.0](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.3.0),\\n[v1.2.0](https://github.com/vacp2p/nim-libp2p/releases/tag/v1.2.0))  \\n* Introduced SAT solver to the Nimble package manager that significantly improves dependency resolution  \\n* Nim VSCode Extension\\n* Stabilized Nim Language Server\\n\\n## Smart Contracts (SC)\\nVac\'s Smart Contracts Service Unit ensures the smart contracts deployed across the various IFT portfolio projects are secure, robust, and aligned with project requirements. \\n\\n### Highlights\\n* Deployed the SNT staking protocol testnet following Status\'s [governance vote](https://our.status.im/snt-vote-results/) to develop SNT staking and Status Network\\n* Wrote specifications for [Codex\'s architectural components](https://github.com/codex-storage/codex-contracts-eth/tree/master/certora/specs) and [Status\'s staking contracts](https://github.com/vacp2p/staking-reward-streamer/tree/main/certora/specs)\\n* Delivered several learn-up sessions on a variety of topics for IFT contributors, including:\\n  * Stealth addresses\\n  * Tokenized vaults\\n  * Rental NFTs\\n  * Merkle trees\\n  * Account abstraction\\n  * EVM deep dive\\n\\n### Looking forward\\n* Deploy the SNT staking protocol on the Status Network testnet\\n* Encourage community security audits via contests\\n* Provide smart contract consultation services for IFT portfolio products\\n* Engage in more learn-up sessions to promote org-wide knowledge sharing. \\n  \\n## Heading into 2025\\nThis year has seen Vac involved with many research, development, and testing undertakings in support of IFT portfolio projects. \\nThe digital public goods that emerge from our efforts not only support the organization itself but are open and free to use by any project that would benefit. \\n\\nAs we move into 2025, we aim to nurture a stronger RFC culture across the IFT to encourage greater collaboration and knowledge sharing among portfolio projects. \\nOur goal is to serve as an internal conduit of expertise within the organization, supported by a strong RFC culture, maintaining a repository of internal knowledge creation, and identifying and facilitating IFT project synergies. \\nSuch an approach should lead to greater efficiencies across the organization. \\n\\nWe also aim to establish a diverse research community around Vac, and our efforts in this regard are already underway. \\nIn the final quarter of 2024, Vac stepped up its collaboration with the libp2p community and made a concerted effort to engage the community on the [Vac forum](https://forum.vac.dev/). \\nIn 2025, we aim to continue working closely with those communities to which we already have ties, such as the libp2p, Ethereum, and Nim ecosystems. \\n\\nWe look forward to continuing our journey with you\\\\!  \\n\\n*Follow [Vac on X](https://x.com/vacp2p), join us in the [Vac Discord](https://discord.gg/FPSXQ9afJE), or take part in the discussions on the [Vac forum](https://forum.vac.dev/) to stay up to date with our research and development progress.*"},{"id":"climbing-merkle-trees","metadata":{"permalink":"/rlog/climbing-merkle-trees","source":"@site/rlog/2024-12-30-merkle-tree.mdx","title":"Vac 101: Climbing Merkle Trees","description":"In this post, we introduce a crucial data structure used throughout web3.","date":"2024-12-30T12:00:00.000Z","formattedDate":"December 30, 2024","tags":[],"readingTime":9.67,"hasTruncateMarker":true,"authors":[{"name":"Marvin","github":"jonesmarvin8","key":"marvin"}],"frontMatter":{"title":"Vac 101: Climbing Merkle Trees","date":"2024-12-30T12:00:00.000Z","authors":"marvin","published":true,"slug":"climbing-merkle-trees","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":4},"prevItem":{"title":"Vac 2024 Year in Review","permalink":"/rlog/2024-recap"},"nextItem":{"title":"Large Message Handling in GossipSub: Potential Improvements","permalink":"/rlog/gsub-largemsg-improvements"}},"content":"In this post, we introduce a crucial data structure used throughout web3.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nA large amount of data is swapped between users on a blockchain in the form of transactions.\\nOver the entire life of a blockchain,\\nthe storage space required to maintain a copy of every transaction becomes untenable for most users.\\nHowever, the integrity of a blockchain relies on a large pool of users that can validate the blockchain\'s history from its inception to its present state.\\nThe data representing the blockchain\'s state is compressed.\\nThis compression addresses the issue of scalability that would otherwise greatly restrict the pool of users.\\n\\nData compression alone is not the end goal.\\nAs mentioned, it is essential for users to be able to validate the blockchain\'s history.\\nThe property of compression and validation was solved in Bitcoin by the use of Merkle trees.\\nMerkle trees were introduced first by Ralph Merkle in his dissertation [[1](https://www.ralphmerkle.com/papers/Thesis1979.pdf)].\\nA Merkle tree is a data structure that compresses a digest of data to a constant size while still providing a method for proving membership of elements of the digest.\\nA previous rlog[[2](https://vac.dev/rlog/rln-light-verifiers/)] described how Merkle trees with their proof of membership could be used for lightweight clients for RLN. \\n\\n## Tree structure\\n\\nA tree is a special data structure that organizes nodes so that there is exactly one path between any two nodes.\\nThe trees that we consider can be arranged in layers with multiple nodes (children) merged into a single node (parent) in the preceding layer.\\nA single node exists in the base layer;\\nthis special node is called the root node.\\nThe highest level of the tree consists of childless nodes called leaves.\\n\\n![Image 1](/img/vac101/vac101_tree.png)\\n\\nA binary tree has one additional property:\\neach nonleaf node has exactly two children nodes.\\nThat is, we assume that nodes in a binary tree are either a parent node with two children or a leaf.\\nAs strange as it sounds, each child node has exactly one parental node.\\n\\n![Image 2](/img/vac101/vac101_binary_tree.png)\\n\\nA binary tree with $2^n$ leaves consists of $n+1$ layers.\\nAdditionally, such a tree has $2^{n+1}-1$ nodes.\\n\\n## Merkle trees\\n\\nA Merkle tree is a specialized tree in which each node contains the evaluation of a hash function.\\nMerkle trees are usually taken to have a binary tree structure.\\nAs such, the presentation we provide in this section will be for binary trees.\\n\\n### Construction\\nIn this section, we show how Merkle trees are constructed to compress a digest $D$.\\nSuppose that the digest $D$ consists of $2^n$ entries;\\nwe assume that the digest $D$ has this many entries since a Merkle tree is a binary tree.\\nAdditionally, each digest can be padded to ensure that $D$ has the desired number of entries.\\n\\nEach leaf of the Merkle tree contains the hash of a digest entry.\\nEach parent node contains the hash of the concatenation of their child nodes.\\nThrough this iterative construction, we reach the root of the tree.\\nThe value contained in the root node is called the root hash.\\nThe root hash is a compressed representation of the digest $D$.\\n\\n![Image 3](/img/vac101/vac101_merkle_tree.png)\\n\\nEach node in the Merkle tree is computed by taking a hash.\\nSince a binary tree with $2^n$ leaves has $2^{n+1}-1$ nodes,\\nthen we need to evaluate $2^{n+1}-1$ hashes to construct the Merkle tree.\\n\\n### Merkle tree intregrity\\n\\nA large quantity of data can be compressed to a single hash value.\\nA natural question to ask is: could a clever party find another digest that yields a Merkle tree with the same root hash?\\nIf possible, this would compromise the ledger since the blockchain\'s history could be altered.\\nFortunately, Merkle trees are quite secure.\\nIn fact, Merkle trees can be used to both bind and hide a digest.\\n\\nThe Merkle tree is able to bind a digest with one of the properties of hash functions (see our previous Vac 101 [[3](https://vac.dev/rlog/vac101-fiat-shamir#hash-functions)] for information on hash functions).\\nA hash function is collision resistant; it is infeasible for a malicious party to find two values share the same hash value.\\n\\nThis collision resistance property, essentially, fixes the input to each leaf and into their parent, their parent\'s parent, and so on.\\n\\nIn certain applications,\\nit may be desirable for the digest of a Merkle tree to be kept confidential.\\nThis is achieved with the preimage resistant property of hash functions.\\nA hash function is preimage resistant provided that it is difficult to reverse the hashing operation. \\nIt would be necessary for a malicious party to find preimages to each node starting from the root node to determine the original digest.\\n\\nNow, we see that Merkle trees are secured structures that are tamper resistant.\\n\\n### Proof of membership\\nAn interesting and critical property of Merkle trees is their ability to prove that any piece of data is part of its digest.\\nThis can be done with logarithmic storage and logarithmic computation time.\\n\\nSuppose that we want to show that data $\\\\ell$ is part of the Merkle tree\'s digest.\\nAdditionally, suppose that $\\\\mathsf{hash}$ is the hash function used to construct the tree.\\nWe assume that the hash function $\\\\mathsf{hash}$ can be computed in constant-time for any input.\\n\\nSuppose that a prover provides data $\\\\ell$ to a verifier,\\nand tells the verifier that $\\\\ell$ corresponds to the $i$th leaf of the Merkle tree.\\nFor the verifier to be convinced that $\\\\ell$ is part of the digest, he needs to be able to construct the tree\'s root hash using $\\\\mathsf{hash}$, $i$, $\\\\ell$ and some additional information from the prover.\\nSpecifically, the prover must provide the sibling hashes for each value that the verifier can compute.\\nThis enables the verifier to compute the parents of the siblings that the prover provides and the values that he was able to produce himself.\\nThe last of the computed parents is the root.\\n\\nThe leaf index $i$ indicates whether a hash value provided by the prover is a left or right sibling.\\nThis is done by looking at the binary expansion of $i$.\\n\\nThe verifier can compute the leaf $h_0 = \\\\mathsf{hash}(\\\\ell)$.\\nNext, using $h_0$\'s sibling, $h\'_0$, provided by the prover,\\nthe verifier can compute $h_1 = \\\\mathsf{hash}(h_0 \\\\|h\'_0)$ or $h_1 = \\\\mathsf{hash}(h\'_0 \\\\| h_0)$\\ndepending on whether $h\'_0$ is a left or right sibling.\\nThis pathing continues until the verifier either successfully computes the root hash (in $n+1$ hashes) or fails to do so.\\n\\nThe prover has to provide $n$ sibling nodes for the proof of membership.\\n\\nThere is a key detail that is essential for the proof of membership to be secure.\\nThe root hash has to be provided to the verifier prior to the selection of data $\\\\ell$.\\nOtherwise, the prover could generate a series of hash values (with the corresponding root hash) to forge a proof of membership.\\n\\n#### Capped proof of membership\\nPolygon provides an implementation [[4](https://github.com/0xPolygonZero/plonky2/blob/main/plonky2/src/hash/merkle_tree.rs)] of a shortened proof of membership with a slight modification.\\nA specific layer of the Merkle tree is published instead of just the root hash.\\nBy doing this, a capped proof of membership is just the path from leaf to the published layer.\\n\\n## Extensions of Merkle trees\\n\\nMerkle trees can be extended in multiple ways.\\nIn this section, we explore a select few of these extensions.\\n\\n### Sparse Merkle trees\\n\\nA sparse Merkle tree (SMT) is a special Merkle tree that can be used to represent digests with nonconsecutive entries.\\nSpecifically, each digest entry has a particular leaf index.\\nFor simplicity, we assume that the index value is computed by taking the hash of the entry.\\nWe note that this is a sorted SMT.\\n\\nLet $n$ denote the number of bits that a hash value can possess. This means that our SMT can have at most $2^n$ leaves.\\n\\nAn SMT is treated as a Merkle tree in which each entry is placed in the leaf corresponding to its hash value, and the other entries have a $\\\\mathsf{null}$ marker inserted in.\\nThis means that we can prove membership in the way described.\\nHowever, we can also prove nonmembership of an element by showing that $\\\\mathsf{null}$ is located in the element\'s hash location.\\nThe crucial difference between a sorted and unsorted SMT is that the unsorted variant cannot be used to prove nonmembership.\\n\\nWe can take advantage of the sparse nature of SMTs to provide shortened proofs.\\nSpecifically, it is unlikely for entries to cluster together.\\nThus, it is efficient to maintain a list of values:\\n\\n|Null values |\\n|---|\\n|$d_0 := \\\\mathsf{Hash(null)},$|\\n|$d_1 := \\\\mathsf{Hash(d_0 \\\\|\\\\| d_0)},$|\\n|$d_2 := \\\\mathsf{Hash(d_1 \\\\|\\\\| d_1)},$|\\n|$\\\\vdots$|\\n|$d_{n-1} := \\\\mathsf{Hash(d_{n-2} \\\\|\\\\| d_{n-2})}.$|\\n\\nEach of the $d_i$\'s represents the root hash of a Merkle tree with $2^i$ leaves containing $\\\\mathsf{null}$.\\nThese values can be used to shorten the time needed to construct an SMT and the length of proofs.\\n\\n### Proof of nonmembership\\n\\nIn the first Vac 101 [[5](https://vac.dev/rlog/vac101-membership-with-bloom-filters-and-cuckoo-filters)], we examined Bloom and Cuckoo filters that could be used for proof of membership and nonmembership.\\nHowever, the proof of membership may result in false positives due to collisions.\\nThis would affect nonmembership proofs as well.\\nSparse Merkle trees can be adapted to provide greater assurance that a given piece of data is not a member of the digest.\\n\\nWhy is sorting essential?\\nThe sorting mechanism of data can be arbitrarily chosen.\\nHowever, it is essential that there are no gaps in the ordering.\\nThe maximum number of elements that could ever exist in the digest must be known.\\nA simple method for this is to use a hash function to provide fingerprints to the data.\\nEach hash using either SHA-256 or Keccak has 256-bits.\\nOur entire digest could consist of a maximum of $2^{256}$ entries.\\nThis assumes that our digest does not contain collisions.\\n\\nThe fingerprint of a piece of data $\\\\ell$ indicates which leaf of the SMT it is contained in.\\nThis means that a nonmembership of $\\\\ell$ in the SMT becomes a matter of proving that $\\\\mathsf{null}$ is contained in $\\\\ell$\'s location. \\n\\nIt is crucial for the SMT to be sorted. \\nOtherwise, a malicious party can append the entry $\\\\ell$ to a random location.\\nThis allows for the malicious party to provide contradictory proofs that prove both membership and nonmembership.\\nWe note that the requirement that an SMT is sorted may be too strong of an assumption in centralized cases.\\nHowever, sortedness is a necessary property of SMTs for decentralized systems.\\n\\n### Verkle Trees\\nA proof of membership grows in length as the Merkle tree grows.\\nThe most obvious approach to remedy this scalability issue is to use Merkle trees in which each node has more than two children.\\nHowever, this does not fix the issue.\\nA proof of membership in a $k$-nary Merkle tree [[6](https://math.mit.edu/research/highschool/primes/materials/2018/Kuszmaul.pdf)] (each node has $k$ children) has a proof size $\\\\log_k(n)(k-1)$.\\nThe multiple $k-1$ is the number of silbings that a node has on each layer.\\nHence, the proof size grows faster than a logarithmic function of the digest size.\\n\\nAn alternate approach is to use a different data structure: Verkle trees [[6](https://math.mit.edu/research/highschool/primes/materials/2018/Kuszmaul.pdf)].\\nA Verkle tree replaces hash functions with polynomial commitments [[7](https://ethresear.ch/t/using-polynomial-commitments-to-replace-state-roots/7095), [8](https://dankradfeist.de/ethereum/2020/06/16/kate-polynomial-commitments.html)].\\nWe will explore Verkle trees in a future Vac 101 edition.\\n\\n## References\\n\\n- 1. [Secrecy, Authentication, and Public Key Systems](https://www.ralphmerkle.com/papers/Thesis1979.pdf) \\n- 2. [Verifying RLN Proofs in Light Clients with Subtrees](https://vac.dev/rlog/rln-light-verifiers/)\\n- 3. [Vac 101: Transforming an Interactive Protocol to a Noninteractive Argument](https://vac.dev/rlog/vac101-fiat-shamir#hash-functions)\\n- 4. [Capped merkle tree in Plonky2](https://github.com/0xPolygonZero/plonky2/blob/main/plonky2/src/hash/merkle_tree.rs)\\n- 5. [Vac 101: Membership with Bloom Filters and Cuckoo Filters](https://vac.dev/rlog/vac101-membership-with-bloom-filters-and-cuckoo-filters)\\n- 6. [Verkle Trees](https://math.mit.edu/research/highschool/primes/materials/2018/Kuszmaul.pdf)\\n- 7. [Using polynomial commitments to replace state roots](https://ethresear.ch/t/using-polynomial-commitments-to-replace-state-roots/7095)\\n- 8. [KZG polynomial commitments](https://dankradfeist.de/ethereum/2020/06/16/kate-polynomial-commitments.html)\\n- 9. [O1 labs\' Verkle Tree repo](https://github.com/o1-labs/verkle-tree)"},{"id":"gsub-largemsg-improvements","metadata":{"permalink":"/rlog/gsub-largemsg-improvements","source":"@site/rlog/2024-10-31-gsub-largemsg-improvements.mdx","title":"Large Message Handling in GossipSub: Potential Improvements","description":"Large Message Handling in GossipSub: Potential Improvements","date":"2024-10-31T12:00:00.000Z","formattedDate":"October 31, 2024","tags":[],"readingTime":19.385,"hasTruncateMarker":true,"authors":[{"name":"Umar Farooq","github":"ufarooqstatus","key":"farooq"}],"frontMatter":{"title":"Large Message Handling in GossipSub: Potential Improvements","date":"2024-10-31T12:00:00.000Z","authors":"farooq","published":true,"slug":"gsub-largemsg-improvements","categories":"research","discuss":"https://forum.vac.dev/t/large-message-handling-in-gossipsub-potential-improvements/375","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Vac 101: Climbing Merkle Trees","permalink":"/rlog/climbing-merkle-trees"},"nextItem":{"title":"Libp2p GossipSub IDONTWANT Message Performance Impact","permalink":"/rlog/gsub-idontwant-perf-eval"}},"content":"Large Message Handling in GossipSub: Potential Improvements \\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Motivation\\r\\nThe challenge of large message transmissions in GossipSub leads to longer than expected network-wide message dissemination times (and relatively higher fluctuations). \\r\\nIt is particularly relevant for applications that require on-time, network-wide dissemination of large messages, \\r\\nsuch as Ethereum and Waku [[1](https://eips.ethereum.org/EIPS/eip-4844),[2](https://docs.waku.org/research/research-and-studies/message-propagation/)]. \\r\\n\\r\\n\\r\\nThis matter has been extensively discussed in the libp2p community [[3](https://github.com/libp2p/rust-libp2p/pull/3666),\\r\\n[4](https://github.com/sigp/lighthouse/pull/4383),\\r\\n[5](https://hackmd.io/X1DoBHtYTtuGqYg0qK4zJw),\\r\\n[6](https://github.com/status-im/nim-libp2p/issues/850),\\r\\n[7](https://github.com/vacp2p/nim-libp2p/pull/911),\\r\\n[8](https://github.com/vacp2p/nim-libp2p/issues/859)], \\r\\nand numerous improvements have been considered (or even incorporated) for the GossipSub protocol to enable efficient large-message propagation \\r\\n[[3](https://github.com/libp2p/rust-libp2p/pull/3666),\\r\\n[7](https://github.com/vacp2p/nim-libp2p/pull/911),\\r\\n[9](https://hackmd.io/@gRwfloEASH6NWWS_KJxFGQ/B18wdnNDh),\\r\\n[10](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.2.md?plain=1#L52)].\\r\\n\\r\\n## Problem description\\r\\n\\r\\nSending a message to N peers involves approximately $\\\\lceil \\\\log_D(N) \\\\rceil$ rounds,\\r\\nwith approximately $(D-1)^{X-1} \\\\times D$ transmissions in each round,\\r\\nwhere $X, D$ represent the round number and mesh size. \\r\\n\\r\\nTransmitting to a higher number of peers (floodpublish) can theoretically reduce latency by increasing the transmissions in each round to $(D-1)^{X-1} \\\\times (F+D)$, \\r\\nwhere $F$ represents the number of peers included in floodpublish. \\r\\n\\r\\nThis arrangement works fine for relatively small/moderate message sizes. \\r\\nHowever, as message sizes increase, significant rises and fluctuations in network-wide message dissemination time are seen. \\r\\n\\r\\nInterestingly, a higher $D$ or $F$ can also degrade performance in this situation. \\r\\n\\r\\nSeveral aspects contribute to this behavior: \\r\\n\\r\\n1. Ideally, a message transmission to a single peer concludes in $\\\\tau_1 = \\\\frac {L}{R}+P$ (ignoring any message processing time), \\r\\nwhere $L, R, P$ represent message size, data rate, and link latency. \\r\\nTherefore, the time required for sending a message on a 100Mbps link with 100ms latency \\r\\njumps from $\\\\tau_1^{10KB} = 100.8ms$ for a 10KB message to $\\\\tau_1^{1MB} = 180ms$ for a 1MB message. \\r\\nFor $D$ peers, the transmission time increases to $\\\\tau_D^{1MB} = (80 \\\\times D) + 100ms$, \\r\\ntriggering additional queuing delays (proportional to the transmission queue size) during each transmission round.  \\r\\n\\r\\n2. In practice, $\\\\tau_1^{1MB}$ sometimes rises to several hundred milliseconds, \\r\\nfurther exaggerating the abovementioned queuing delays.\\r\\nThis rise is because TCP congestion avoidance algorithms usually limit maximum in-flight bytes in a round trip time (RTT) \\r\\nbased on the congestion window ($C_{wnd}$) and maximum segment size (MSS) to approximately ${C_{wnd} \\\\times MSS}$, \\r\\nwith $C_{wnd}$ rising with the data transfer for each flow. \\r\\nConsequently, sending the same message through a newly established (cold) connection takes longer. \\r\\nThe message transfer time lowers as the $C_{wnd}$ grows. \\r\\nTherefore, performance-improvement practices such as floodpublish, frequent mesh adjustment, and lazy sending \\r\\ntypically result in longer than expected message dissemination times for large messages (due to cold connections). \\r\\nIt is also worth mentioning that some TCP variants reset their $C_{wnd}$ after different periods of inactivity.\\r\\n\\r\\n3. Theoretically, the message transmission time to D peers $(\\\\tau_D)$ remains the same \\r\\neven if the message is relayed sequentially to all peers or simultaneous transmissions are carried out, \\r\\ni.e., $\\\\tau_D = \\\\sum_{i=1}^{D} \\\\tau_i$\\r\\nHowever, sequential transmissions finish early for individual peers, allowing them to relay early.\\r\\nThis may result in quicker network-wide message dissemination. \\r\\n\\r\\n\\r\\n4. A realistic network comprises nodes with dissimilar capabilities (bandwidth, link latency, compute, etc.).\\r\\nAs the message disseminates, it\'s not uncommon for some peers to receive it much earlier than others. \\r\\nEarly gossip (IHAVE announcements) may bring in many IWANT requests to the early receivers (even from peers already receiving the same message), \\r\\nwhich adds to their workload.  \\r\\n\\r\\n5. A busy peer (with a sizeable outgoing message queue) will enqueue (or simultaneously transfer) newly scheduled outgoing messages. \\r\\nAs a result, already scheduled messages are prioritized over those published by the peer itself, \\r\\nintroducing a significant initial delay to the locally published messages. \\r\\nEnqueuing IWANT replies to the outgoing message queue can further exaggerate the problem. \\r\\nThe lack of adaptiveness and standardization in outgoing message prioritization are key factors that can lead to noticeable inconsistency \\r\\nin message dissemination latency at each hop, even in similar network conditions.\\r\\n\\r\\n6. Message size directly contributes to peers\' workloads in terms of processing and transmission time.\\r\\nIt also raises the probability of simultaneous redundant transmissions to the same peer, \\r\\nresulting in bandwidth wastage, congestion, and slow message propagation to the network. \\r\\nMoreover, the benefits of sequential message relaying can be compromised by prioritizing slow (or busy) peers.\\r\\n\\r\\n7. Most use cases necessitate validating received messages before forwarding them to the next-hop peers.\\r\\nFor a higher message transfer time $(\\\\tau )$, this store-and-forward delay accumulates across the hops traveled by the message.  \\r\\n\\r\\n## Possible improvements\\r\\n\\r\\n\\r\\n### 1. Minimizing transfer time for large messages\\r\\n\\r\\nThe impact of message size and achievable data rate on message transmit time $\\\\tau$ is crucial \\r\\nas this time accumulates due to the store-and-forward delay introduced at intermediate hops. \\r\\n\\r\\nSome possible improvements to minimize overall message dissemination latency include:\\r\\n\\r\\n#### a. Message fragmentation\\r\\n\\r\\nIn a homogeneous network, network-wide message dissemination time (ignoring any processing delays) \\r\\ncan be simplified to roughly $\\\\delta \\\\approx \\\\delta_{Tx} + P_h$, where $\\\\delta_{Tx}$ represents accumulative message transmit time denoted as $\\\\delta_{Tx} = \\\\frac{S}{R} \\\\times h$, \\r\\nwith $S, R$ being the data size and data rate, \\r\\nand $h, P_h$ being the number of hops in the longest path and message propagation time through the longest path. \\r\\n \\r\\nPartitioning a large message into n fragments reduces a single fragment transmit time to $\\\\frac{\\\\delta_{Tx}}{n}$. \\r\\nAs a received fragment can be immediately relayed by the receiver (while the sender is still transmitting the remaining fragments), \\r\\nit reduces the transmit time to $\\\\delta_{Tx} = \\\\frac{S}{R} \\\\times \\\\frac{2h-1}{n}$. \\r\\n\\r\\nThis time reduction is mainly attributed to the smaller store-and-forward delay involved in fragment transmissions.\\r\\n\\r\\nHowever, it is worth noting that many applications require each fragment to be individually verifiable.\\r\\nAt the same time, message fragmentation allows a malicious peer to never relay some fragments of a message, \\r\\nwhich can lead to a significant rise in the application\'s receive buffer size. \\r\\n\\r\\nTherefore, message fragmentation requires a careful tradeoff analysis between time and risks.\\r\\n\\r\\n#### b. Message staggering\\r\\n\\r\\nConsidering the same bandwidth, the time $\\\\tau_D$ required for sending a message to D peers stays the same, \\r\\neven if we relay to all peers in parallel or send sequentially to the peers, i.e., $\\\\tau_D = \\\\sum_{i=1}^{D} \\\\tau_i$. \\r\\n\\r\\nHowever, sequential relaying results in quicker message reception at individual peers ($\\\\tau_1 \\\\approx \\\\frac{\\\\tau_D}{D}$) due to bandwidth concentration for a particular peer. \\r\\nSo, the receiver can start relaying early to its mesh members while the original sender is still sending the message to other peers. \\r\\n\\r\\n\\r\\nAs a result, after every $\\\\frac{\\\\tau_D}{D}$ milliseconds, \\r\\nthe number of peers receiving the message increases by $2^X\\\\ \\\\forall\\\\ X \\\\in \\\\{0, D-1\\\\}$ and by $\\\\sum_{k=X-D}^{X-1} \\\\lambda_k\\\\ \\\\forall\\\\ X \\\\geq D$. \\r\\nHere, $X$ represents message transmission round $X = i \\\\cdot \\\\frac{\\\\tau_D}{D} \\\\mid i \\\\in \\\\mathbb{N}_0$, and $\\\\lambda_k$ represents the number of peers that received the message in round $k$.\\r\\n\\r\\nIt is worth noting that a realistic network imposes certain constraints on staggering for peers. \\r\\nFor instance, in a network with dissimilar peer capabilities, \\r\\nplacing a slow peer (also in cases where many senders simultaneously select the same peer) at the head of the transmission queue \\r\\nmay result in head-of-line blocking for the message queue. \\r\\n\\r\\nAt the same time, early receivers get many IWANT requests, increasing their workload.\\r\\n\\r\\n#### c. Message prioritization for slow senders\\r\\n\\r\\nA slow peer often struggles with a backlog of messages in the outgoing message queue(s) for mesh members. \\r\\nAny new message transmission at this stage (especially the locally published messages) gets delayed. \\r\\nAdaptive message-forwarding can help such peers prioritize traffic to minimize latency for essential message transfers. \\r\\n\\r\\nFor instance, any GossipSub peer will likely receive every message from multiple senders, \\r\\nleading to redundant transmissions [[11](https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921)]. \\r\\nImplementing efficient strategies (only for slow senders) like lazy sending \\r\\nand prioritizing locally published messages/IWANT replies over already queued messages \\r\\ncan help minimize outgoing message queue sizes and optimize bandwidth for essential message transfers. \\r\\n\\r\\nA peer can identify itself as a slow peer by using any bandwidth estimation approach \\r\\nor simply setting an outgoing message queue threshold for all mesh members. \\r\\n\\r\\n\\r\\nEliminating/deprioritizing some messages can lower a peer\'s score, \\r\\nbut it also earns the peer an overall better score by achieving some early message transfers.  \\r\\nFor instance, sending many near-first messages can only save a peer from a deficit penalty. \\r\\nOn the other hand, sending only one message (assuming MeshMessageDeliveriesThreshold defaults to 1) \\r\\nas the first delivered message can add to the accumulative peer score. \\r\\n\\r\\n### 2. Mitigating transport issues\\r\\n\\r\\nCongestion avoidance algorithms used in various TCP versions directly influence achievable throughput and message transfer time \\r\\nas maximum unacknowledged in-flight bytes are based on the congestion window $(C_{wnd})$ size. \\r\\n\\r\\n\\r\\nRapid adaptation of $C_{wnd}$ to the available network conditions can help lower message dissemination latency. \\r\\n\\r\\nTherefore, selecting a more suitable TCP variant like BBR, \\r\\nwhich is known for its ability to dynamically adjust the congestion window based on network conditions, \\r\\ncan significantly enhance GossipSub\'s performance. \\r\\n\\r\\nAt the same time, parameters like receive window scaling and initial $C_{wnd}$ also impact message transfer time, \\r\\nbut these are usually OS-specific system-wide choices.\\r\\n\\r\\nOne possible solution is to raise $C_{wnd}$ by exchanging data over the newly established connection.\\r\\nThis data may involve useful details like peer exchange information and gossip to build initial trust, \\r\\nor GossipSub can use some dummy data to raise $C_{wnd}$ to a reasonable level.\\r\\n\\r\\nIt\'s important to understand that some TCP variants reset $C_{wnd}$ after specific periods of inactivity [[12](https://datatracker.ietf.org/doc/html/rfc2581#section-4.1)].\\r\\nThis can lead to a decline in TCP\'s performance for applications \\r\\nthat generate traffic after intervals long enough to trigger the resetting of the congestion window. \\r\\n\\r\\nImplementing straightforward measures like transport-level ping-pong messages can effectively mitigate this problem [[13](https://github.com/libp2p/specs/pull/558)]. \\r\\n\\r\\n \\r\\nThe limitations faced with $C_{wnd}$ scaling also impact some performance optimizations in GossipSub. \\r\\nFor instance, floodpublishing is an optimization relying on additional transmissions by the publisher to minimize message dissemination latency. \\r\\n\\r\\nHowever, a small $C_{wnd}$ value in (new/cold) TCP connections established with floodpublish peers significantly increases message transmission time \\r\\n[[4](https://github.com/sigp/lighthouse/pull/4383)]. \\r\\nUsually, these peers also receive the same message from other sources during this time, wasting the publisher\'s bandwidth. \\r\\n\\r\\nThe same is the case with IWANT replies. \\r\\n\\r\\nMaintaining a bigger mesh (with warm TCP connections) and relaying to $D$ peers can be a better alternative to this problem. \\r\\n\\r\\n### 3. Eliminating redundant transmissions\\r\\n\\r\\nFor every received packet, a peer makes roughly $D$ transmissions to contribute its fair share to the spread of messages. \\r\\nHowever, the fact that many recipients had already received the message (from some other peer) \\r\\nmakes this message propagation inefficient. \\r\\n\\r\\nAlthough the $D$-spread is attributed to quicker dissemination and resilience against non-conforming peers, \\r\\nmany potential solutions can still minimize redundant transmissions \\r\\nwhile preserving the resilience of GossipSub. \\r\\n\\r\\n\\r\\nThese solutions, ranging from probabilistic to more knowledgeful elimination of messages from the outgoing message queue, \\r\\nnot only address the issue of redundancy but also provide an opportunity for bandwidth optimization,\\r\\nespecially for resource-constrained peers.\\r\\n\\r\\nFor instance, an IDONTWANT message, a key component of GossipSub (v1.2) [[10](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.2.md?plain=1#L52)], \\r\\ncan significantly reduce redundant transmissions. \\r\\n\\r\\nIt allows any node to notify its mesh members that it has already received a message, \\r\\nthereby preventing them from resending the same message. \\r\\nThis functionality is useful when a node receives a message larger than a specified threshold. \\r\\n\\r\\nIn such cases, the node promptly informs its mesh peers about the successful reception of the message by sending IDONTWANT messages. \\r\\n\\r\\nIt\'s important to note that an IDONTWANT message is essentially an IHAVE message, but with a crucial difference, \\r\\ni.e., IHAVEs are only transmitted during the heartbeat intervals, whereas IDONTWANTs are sent immediately after receiving a large message. \\r\\n\\r\\nThis prompt notification helps curtail redundant large message transmissions without compromising the GossipSub resilience.  \\r\\n\\r\\nHowever, the use of IDONTWANT messages alone has an inherent limitation. \\r\\nFor instance, a peer can only send an IDONTWANT after receiving the complete message. \\r\\n\\r\\nA large message transmission consumes significant time.\\r\\nFor example, transmitting a 1MB message at 100 Mbps bandwidth may consume 80 to several hundred milliseconds (depending upon $C_{wnd}$ and latency). \\r\\n\\r\\nAs a result, other mesh members may also start transmitting the same message during this interval. \\r\\nA few potential solutions include: \\r\\n\\r\\n#### a. Staggering with IDONTWANT messages\\r\\n\\r\\nAs previously discussed, [staggering](#b-message-staggering) can significantly reduce network-wide message dissemination latency. \\r\\nThis is primarily due to the relatively smaller store-and-forward delays that are inherent in this approach.\\r\\n\\r\\nUsing both staggering and IDONTWANT messages can further enhance efficiency by reducing redundant transmissions.\\r\\nThis is because a node only saturates its bandwidth for a small subset of mesh peers, \\r\\nleading to early transmissions and prompt IDONTWANT message notifications to the mesh members.\\r\\n\\r\\nIt is worth highlighting that staggering can be implemented in various ways.\\r\\n\\r\\nFor example, it can be applied to peers (peer staggering)\\r\\nwhere a node sequentially relays the same message to all peers one by one. \\r\\n\\r\\nAlternatively, a node can send a different message to every peer (message staggering or rotational sending), \\r\\nallowing IDONTWANTs for other messages to arrive during this time. \\r\\nThe message staggering approach is beneficial when several messages are introduced to the network within a short interval of time.\\r\\n\\r\\nAs the peers in staggered sending are sequentially covered \\r\\n(with a faster speed due to bandwidth concentration), this leads to another problem. \\r\\n\\r\\nThe early covered peers send IHAVE (during their heartbeat intervals) for the messages they have received. \\r\\nIHAVE announcements for newly received large messages trigger IWANTs from nodes \\r\\n(including those already receiving the same message),\\r\\nleading to an additional workload for early receivers [[14](https://github.com/vacp2p/nim-libp2p/issues/1101)]. \\r\\n\\r\\nPotential solutions to mitigate these problems include:\\r\\n\\r\\n\\r\\n1) Defering IHAVE announcements for large messages. \\r\\n\\r\\nDeferring IHAVE announcements can indirectly prioritize message transmission to the mesh peers over IWANT replies. \\r\\nHowever, deciding on a suitable deferred interval is crucial for optimal performance. \\r\\nOne possible solution is to generate IHAVEs only after the message is relayed to all the mesh peers.\\r\\n\\r\\n2) Defering IWANT requests for messages that are currently being received. \\r\\n\\r\\nThis requires [prior knowledge of msgIDs](#b-imreceiving-message) for the messages under reception. \\r\\nKnowing the message length is also essential in deciding a suitable defer interval \\r\\nto handle situations where a sender starts sending a message and never completes the transmission.\\r\\n\\r\\n3) Not issuing IWANT for a message if at least $K$ peers have transmitted IDONTWANT for the same message\\r\\n(as this indicates that these peers will eventually relay this message).\\r\\n\\r\\nHowever, this approach can inadvertently empower a group of non-conforming mesh peers to send IDONTWANT for a message and never complete message transmission. \\r\\nA delayed IWANT, along with negative peer scoring, can remedy this problem.  \\r\\n\\r\\n#### b. IMReceiving message\\r\\n\\r\\nA peer can issue an IDONTWANT only after it has received the entire message. \\r\\nHowever, a large message transmission may take several hundred milliseconds to complete. \\r\\nDuring this time, many other mesh members may start relaying the same message. \\r\\n\\r\\nTherefore, the probability of simultaneously receiving the same message from multiple senders increases with the message size, \\r\\nsignificantly compromising the effectiveness of IDONTWANT messages.\\r\\n\\r\\nSending a short preamble (containing msgID and length) before the message transmission can provide valuable information about the message. \\r\\nIf a receiver is already receiving the same message from another sender, \\r\\nthe receiver can request to defer this transmission by sending a brief IMReceiving message.\\r\\n\\r\\nAn IDONTWANT from the receiver will indicate successful message reception. Otherwise, the waiting sender can initiate transmission after a specific wait interval.\\r\\n\\r\\nHowever, waiting for IMReceiving after sending the preamble can delay the message transmission. \\r\\nOn the other hand, proceeding with message transfer (after sending the preamble) leads to another problem: \\r\\nit is difficult to cancel ongoing message transmission after receiving IMReceiving for the same message.\\r\\n\\r\\n\\r\\nTo streamline this process, a peer can immediately send an IMReceiving message (for every received preamble), \\r\\nurging other mesh peers to defer sending the same message [[15](https://forum.vac.dev/t/large-message-handling-idontwant-imreceiving/281),\\r\\n[16](https://forum.vac.dev/t/idontwant-message-impact/283)]. \\r\\n\\r\\nThe other peers can send this message if IDONTWANT is not received from the receiver during the wait interval.\\r\\nThis approach can boost IDONTWANT benefits by considering ongoing transmissions for large messages. \\r\\n\\r\\nWhile IMReceiving messages can bring about substantial improvements in terms of latency and bandwidth utilization, \\r\\nit\'s crucial to be aware of the potential risks. \\r\\n\\r\\nA malicious user can exploit this approach to disrupt message transmission \\r\\neither by never completing a message or by intentionally sending a message at an extremely slow rate to numerous peers. \\r\\n\\r\\n\\r\\nThis could ultimately result in network-wide slow message propagation. \\r\\n\\r\\nHowever, carefully calibrating the deferring interval (based on message size) and negative peer scoring can help mitigate these risks.\\r\\n\\r\\n#### c. IDONTWANT message with reduced forwarding\\r\\n\\r\\nIt is common for slow peers to pile up outgoing message queues, \\r\\nespecially for large message transfers.\\r\\nThis results in a significant queuing delay for outgoing messages. \\r\\nReduced message forwarding can help decrease the workload of slower peers. \\r\\n\\r\\nOn receiving a message longer than the specified threshold, \\r\\na slow peer can relay it to only $K \\\\in D$ peers and send an IDONTWANT message to all the peers in $D$.\\r\\n\\r\\nIn this arrangement, the IDONTWANT message serves an additional purpose: \\r\\nto promptly announce data availability, reinforcing redundancy in the presence of adversaries. \\r\\n\\r\\nWhen a peer receives an IDONTWANT for an unseen message, \\r\\nit learns about the new message and can request it by sending an IWANT request without waiting for the heartbeat (gossip) interval. \\r\\nAs a result, a significantly smaller number of transmissions is sufficient for propagating the message to the entire network. \\r\\n\\r\\nThis approach conserves peer bandwidth by minimizing redundant transmissions \\r\\nwhile ensuring GossipSub resilience at the cost of one RTT (for missing peers). \\r\\n\\r\\nInterestingly, curtailing queuing delays can also help lower network-wide message dissemination latency (for huge messages).\\r\\n\\r\\n\\r\\nHowever, finding an appropriate value for $K$ is crucial for optimal performance. \\r\\nA smaller $K$ saves peer bandwidth, while a larger $K$ achieves quicker spread until outgoing message queues pile up. \\r\\nSetting $K = D_{low}$ can be one option. \\r\\n\\r\\nIt is worth mentioning that such behavior may negatively impact peer scoring (by missing message delivery rewards from $D-K$ peers). \\r\\nHowever, a minimized workload enables early message dissemination to the remaining peers. \\r\\nThese early transmissions and randomized $K$ set selection can help achieve an overall better peer score.\\r\\n\\r\\n### 4. Message prioritization\\r\\n\\r\\nDespite the standardized specifications of the GossipSub protocol, \\r\\nthe message forwarding mechanisms can significantly impact network-wide message dissemination latency and bandwidth utilization. \\r\\n\\r\\nIt is worth mentioning that every node is responsible for transmitting different types of packets, \\r\\nincluding control messages, locally published messages, messages received from mesh members, IWANT replies, etc. \\r\\n\\r\\nAs long as traffic volume is lower than the available data rate, \\r\\nthe message forwarding mechanisms yield similar results due to negligible queuing delays. \\r\\n\\r\\nHowever, when the traffic volume increases and exceeds the available peer bandwidth (even for short traffic bursts), \\r\\nthe outgoing message queue(s) sizes rise, potentially impacting the network\'s performance. \\r\\n\\r\\nIn this scenario, FIFO-based traffic forwarding can lead to locally published messages being placed at the end of the outgoing message queue, \\r\\nintroducing a queuing delay proportional to the queue size. \\r\\nThe same applies to other delay-sensitive messages like IDONTWANT, PRUNE, etc. \\r\\n\\r\\nOn the other hand, the segregation of traffic into priority and non-priority queues can potentially starve low-priority messages. \\r\\nOne possible solution is to use weighted queues for a fair spread of messages.\\r\\n\\r\\nMessage prioritization can be a powerful tool to ensure that important messages reach their intended recipients on time \\r\\nand allow for customizable message handling. \\r\\n\\r\\nFor example, staggering between peers and messages can be better managed by using priority queues. \\r\\nHowever, it is important to note that message prioritization also introduces additional complexity to the system, \\r\\nnecessitating sophisticated algorithms for better message handling.   \\r\\n\\r\\n### 5. Maximizing benefits from IWANT messages\\r\\nDuring heartbeat intervals, GossipSub nodes transmit IHAVE messages (carrying IDs of seen messages) to the peers not included in the full-message mesh. \\r\\nThese peers can use IWANT messages to request any missing messages. \\r\\nA budget counter ensures these messages never exceed a specified threshold during each heartbeat interval.\\r\\n\\r\\nThe IHAVE/IWANT messages are a crucial tool in maintaining network connectivity. \\r\\nThey bridge the information gap between nearby and far-off peers, \\r\\nensuring that information can be disseminated to peers outside the mesh. \\r\\nThis function is essential in protecting against network partitions and indirectly aids in safeguarding against Sybil and eclipse attacks. \\r\\n\\r\\n\\r\\nHowever, it is essential to understand that high transmission times for large messages \\r\\nrequire careful due diligence when using IWANT messages for reasons not limited to:\\r\\n\\r\\n1) A large message reception may take several hundred milliseconds to complete. \\r\\nDuring this time, an IHAVE message announcing the same message ID will trigger an IWANT request.\\r\\n\\r\\n2) A peer can send IWANT requests for the same message to multiple nodes, \\r\\nleading to simultaneous transmissions of the same message.\\r\\n\\r\\n3) Replying to (potentially many) IWANT requests can delay the transmission of the same message to mesh peers, \\r\\nresulting in lower peer scores and slower message propagation.\\r\\n\\r\\nA few possible solutions to mitigate this problem may include:\\r\\n\\r\\n\\r\\n1) Issuing IHAVE announcements only after the message is delivered to many mesh peers.\\r\\n\\r\\n2) Allocating a volume-based budget to service IWANT requests during each heartbeat interval.\\r\\n\\r\\n3) Deferring IWANT requests for messages that are currently being received. \\r\\n\\r\\n4) Deferring IWANT requests if at least $K$ IDONTWANTs are received for the same message.\\r\\n\\r\\n5) A large message transmission can yield high $C_{wnd}$; preferring such peers during mesh maintenance can be helpful.\\r\\n\\r\\n## Summary\\r\\nThis study investigates the pressing issue of considerable fluctuations and rises in network-wide dissemination times for large messages. \\r\\n\\r\\n\\r\\nWe delve into multiple factors, \\r\\nsuch as increased message transmit times, store-and-forward delays, congestion avoidance mechanisms, and prioritization between messages, \\r\\nto establish a comprehensive understanding of the problem. \\r\\n\\r\\nThe study also explores the performance of optimization efforts \\r\\nlike floodpublishing, IHAVE/IWANT messages, and message forwarding strategies in the wake of large message transmissions.\\r\\n\\r\\nA key finding is that most congestion avoidance algorithms lack optimization for peer-to-peer networks. \\r\\nCoupling this constraint with increased message transmission times \\r\\nresults in notable store-and-forward delays accumulating at each hop. \\r\\n\\r\\nFurthermore, the probabilistic message-forwarding nature of GossipSub further exacerbates the situation \\r\\nby utilizing a considerable share of available bandwidth on redundant transmissions. \\r\\n\\r\\n\\r\\nTherefore, approaches focused on eliminating redundant transmissions \\r\\n(IDONTWANT, IMReceiving, lazy sending, etc.) can prove helpful. \\r\\nAt the same time, strategies aimed at reducing store-and-forward delays \\r\\n(fragmentation, staggering, prioritization, etc.) can prove beneficial. \\r\\n\\r\\n\\r\\nIt is worth mentioning that many of the strategies suggested in this post are ideas at different stages. \\r\\nSome of these have already been explored and discussed to some extent [[5](https://hackmd.io/X1DoBHtYTtuGqYg0qK4zJw),\\r\\n[17](https://forum.vac.dev/t/iwant-messages-may-have-negative-impact-on-message-dissemination-latency-for-large-messages/366),\\r\\n[18](https://vac.dev/rlog/gsub-idontwant-perf-eval/)].\\r\\nWe are nearing the completion of a comprehensive performance evaluation of these approaches and will soon share the results of our findings. \\r\\n \\r\\nPlease feel free to join the discussion and leave feedback regarding this post in the \\r\\n[VAC forum](https://forum.vac.dev/t/large-message-handling-in-gossipsub-potential-improvements/375).\\r\\n\\r\\n\\r\\n## References\\r\\n[1] EIP-4844: Shard Blob Transactions. Retrieved from https://eips.ethereum.org/EIPS/eip-4844\\r\\n\\r\\n[2] Message Propagation Times With Waku-RLN. Retrieved from https://docs.waku.org/research/research-and-studies/message-propagation/\\r\\n\\r\\n[3] Lenient Flood Publishing. Retrieved from https://github.com/libp2p/rust-libp2p/pull/3666\\r\\n\\r\\n[4] Disable Flood Publishing. Retrieved from https://github.com/sigp/lighthouse/pull/4383\\r\\n\\r\\n[5] GossipSub for Big Messages. Retrieved from https://hackmd.io/X1DoBHtYTtuGqYg0qK4zJw\\r\\n\\r\\n[6] GossipSub: Lazy Sending. Retrieved from https://github.com/status-im/nim-libp2p/issues/850\\r\\n\\r\\n[7] GossipSub: Limit Flood Publishing. Retrieved from https://github.com/vacp2p/nim-libp2p/pull/911\\r\\n\\r\\n[8] GossipSub: Lazy Prefix Detection. Retrieved from https://github.com/vacp2p/nim-libp2p/issues/859\\r\\n\\r\\n[9] Potential Gossip Improvement List for EIP4844. Retrieved from https://hackmd.io/@gRwfloEASH6NWWS_KJxFGQ/B18wdnNDh\\r\\n\\r\\n[10] GossipSub Specifications v1.2: IDONTWANT Message. Retrieved from https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.2.md?plain=1#L52\\r\\n\\r\\n[11] Number of Duplicate Messages in Ethereum\u2019s GossipSub Network. Retrieved from https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921\\r\\n\\r\\n[12] TCP Congestion Control: Re-starting Idle Connections. Retrieved from https://datatracker.ietf.org/doc/html/rfc2581#section-4.1\\r\\n\\r\\n[13] PING/PONG Control Messages. Retrieved from https://github.com/libp2p/specs/pull/558\\r\\n\\r\\n[14] IHAVE/IWANT Message Impact. Retrieved from https://github.com/vacp2p/nim-libp2p/issues/1101\\r\\n\\r\\n[15] Large Message Handling IDONTWANT + IMReceiving Messages. Retrieved from https://forum.vac.dev/t/large-message-handling-idontwant-imreceiving/281\\r\\n\\r\\n[16] IDONTWANT Message Impact. Retrieved from https://forum.vac.dev/t/idontwant-message-impact/283\\r\\n\\r\\n[17] IWANT Message Impact. Retrieved from https://forum.vac.dev/t/iwant-messages-may-have-negative-impact-on-message-dissemination-latency-for-large-messages/366\\r\\n\\r\\n[18] IDONTWANT Message Performance. Retrieved from https://vac.dev/rlog/gsub-idontwant-perf-eval/"},{"id":"gsub-idontwant-perf-eval","metadata":{"permalink":"/rlog/gsub-idontwant-perf-eval","source":"@site/rlog/2024-10-28-gsub-idontwant-perf-eval.mdx","title":"Libp2p GossipSub IDONTWANT Message Performance Impact","description":"This post provides quick insights into the IDONTWANT message performance and highlights minor tweaks that can further contribute to performance gains.","date":"2024-10-28T12:00:00.000Z","formattedDate":"October 28, 2024","tags":[],"readingTime":3.38,"hasTruncateMarker":true,"authors":[{"name":"Umar Farooq","github":"ufarooqstatus","key":"farooq"}],"frontMatter":{"title":"Libp2p GossipSub IDONTWANT Message Performance Impact","date":"2024-10-28T12:00:00.000Z","authors":"farooq","published":true,"slug":"gsub-idontwant-perf-eval","categories":"research","discuss":"https://forum.vac.dev/t/libp2p-gossipsub-idontwant-message-performance-impact/374","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Large Message Handling in GossipSub: Potential Improvements","permalink":"/rlog/gsub-largemsg-improvements"},"nextItem":{"title":"Vac 101: Transforming an Interactive Protocol to a Noninteractive Argument","permalink":"/rlog/vac101-fiat-shamir"}},"content":"This post provides quick insights into the IDONTWANT message performance and highlights minor tweaks that can further contribute to performance gains.\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Overview\\r\\n\\r\\n[IDONTWANT](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.2.md?plain=1#L52) messages are introduced to curtail redundant transmissions without compromising resilience. \\r\\nCutting down on duplicates can potentially render two significant advantages:\\r\\n\\r\\n\\r\\n1. Reducing bandwidth requirements \\r\\n\\r\\n2. Reducing message dissemination time (latency)\\r\\n\\r\\n\\r\\nFor IDONTWANTs to be effective, they must be received and processed by the sender before the sender starts relaying the respective message. \\r\\n\\r\\n[Duplicates investigation](https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921#arrival-time-of-duplicates-9) reveals that \\r\\nthe average time difference between the first message arrival and the first duplicate arrival is higher than the average round trip time in Ethereum\'s GossipSub network. \\r\\n\\r\\nThis allows for timely IDONTWANT reception and canceling of many duplicate transmissions, \\r\\nshowing a potential for a significant drop in bandwidth utilization. \\r\\nOn the other hand, lowering message dissemination time is only possible by minimizing queuing delays at busy peers. \\r\\n\\r\\n## Experiments\\r\\n\\r\\nWe conducted a series of experiments with different arrangements (changing heartbeat_interval and message size) \\r\\nto precisely identify the impact of IDONTWANT messages on bandwidth utilization and message dissemination time. \\r\\n\\r\\nThe experiments are performed on nim-libp2p using the [shadow simulator](https://github.com/vacp2p/dst-gossipsub-test-node/pull/4). \\r\\nThe peer bandwidth and link latency are uniformly set between 50-150 Mbps and 40-130 milliseconds in five stages. \\r\\n\\r\\nIn all experiments, ten messages are transmitted to the network, i.e., \\r\\nten peers (publishers) are selected as the message transmitters. \\r\\nEvery publisher transmits exactly one message, \\r\\nand inter-packet spacing (delay) is set to four seconds for each published message. \\r\\nFor a fair assessment, we ensure that the publishers are uniformly selected from each bandwidth class. \\r\\n\\r\\nAt the start of each experiment, two additional messages are transmitted to increase the TCP $C_{wnd}$.\\r\\nThese messages are not included in latency computations. \\r\\n\\r\\nThe simulation details are presented in the table below.\\r\\n\\r\\n\\r\\n| **Parameter** | **Value** | **Parameter** | **Value** |\\r\\n| ------------- | --------- | ------------- | --------- |\\r\\n| Peers | 2000 | Publishers | 10 |\\r\\n| Peer bandwidth | 50-150 Mbps | Link latency | 40-130 ms |\\r\\n| Message size | 1KB, 50KB, 500KB, 1MB | $D$ | 8 |\\r\\n| Heartbeat interval | 700ms, 1000ms, 1500ms | $D_{low}$ | 6 |\\r\\n| FloodPublish | False | $D_{high}$ | 12 |\\r\\n| Gossip factor | 0.05 | Muxer | yamux |\\r\\n\\r\\n\\r\\n## Findings\\r\\n\\r\\nWe use bandwidth utilization and latency as evaluation metrics. \\r\\nBandwidth utilization represents total network-wide traffic (including gossip and other control messages). \\r\\nLatency refers to network-wide message dissemination time. \\r\\nThe total number of IWANT requests and the number of message transmissions saved by IDONTWANT messages are also presented for detailed insights.\\r\\n\\r\\n\\r\\nExperiments reveal that IDONTWANT messages yield a noticeable (up to 21%) drop in bandwidth utilization. \\r\\nA higher drop is seen with a higher heartbeat interval. \\r\\nInterestingly, a relatively low bandwidth reduction (12-20%) is seen for 1MB messages, \\r\\ncompared to 500KB messages (18-21%).   \\r\\n\\r\\n\\r\\n| ![Image 1](/img/idontwanttest/BW_700ms.png) | ![Image 2](/img/idontwanttest/BW_1000ms.png) | ![Image 3](/img/idontwanttest/BW_1500ms.png) |\\r\\n|-----------------------------|-----------------------------|-----------------------------|\\r\\n\\r\\n\\r\\nThis is because downloading a large message may consume several hundred milliseconds. \\r\\nDuring this time, a receiver will likely \\r\\n[generate multiple IWANT requests](https://forum.vac.dev/t/iwant-messages-may-have-negative-impact-on-message-dissemination-latency-for-large-messages/366) \\r\\nfor the same message, increasing bandwidth utilization. \\r\\n\\r\\nMoreover, a peer can generate \\r\\n[IDONTWANTs only after it has finished downloading the message](https://forum.vac.dev/t/large-message-handling-idontwant-imreceiving/281). \\r\\nA longer download time will result in simultaneous reception of the same message from other mesh members. \\r\\n\\r\\n\\r\\n| ![Image 1](/img/idontwanttest/IWANT_Requests.png) | ![Image 2](/img/idontwanttest/IDONTWANT_Saves.png) |\\r\\n|-----------------------------|-----------------------------|\\r\\n\\r\\n\\r\\nThese IWANT requests mainly overwhelm early message receivers, \\r\\nwhich can negatively impact message dissemination time on some occasions. \\r\\nTherefore, a similar message dissemination time is seen with and without IDONTWANT messages. \\r\\n\\r\\n\\r\\n| ![Image 1](/img/idontwanttest/Lat_700ms.png) | ![Image 2](/img/idontwanttest/Lat_1000ms.png) | ![Image 3](/img/idontwanttest/Lat_1500ms.png) |\\r\\n|-----------------------------|-----------------------------|-----------------------------|\\r\\n\\r\\nSimilar results are seen on our large-scale deployment runs \\r\\n([running Waku nodes in Kubernetes](https://zealous-polka-dc7.notion.site/Nim-libp2p-v1-5-0-regression-testing-August-2024-25edba733c704ccaa411919555c5db1a)).\\r\\n\\r\\nPlease feel free to join the discussion and leave feedback regarding this post in the\\r\\n[VAC forum](https://forum.vac.dev/t/libp2p-gossipsub-idontwant-message-performance-impact/374).\\r\\n\\r\\n\\r\\n## References\\r\\n\\r\\n- [GossipSub Specifications v1.2](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.2.md)\\r\\n- [GossipSub v1.2: IDONTWANT Control Message](https://github.com/libp2p/specs/pull/548)\\r\\n- [Number Duplicate Messages in Ethereum\u2019s Gossipsub Network](https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921)\\r\\n- [IWANT Messages Impact on Latency ](https://forum.vac.dev/t/iwant-messages-may-have-negative-impact-on-message-dissemination-latency-for-large-messages/366)\\r\\n- [Large Message Handling (IDONTWANT + IMReceiving)](https://forum.vac.dev/t/large-message-handling-idontwant-imreceiving/281)\\r\\n- [IDONTWANT Message Impact Before/After Message Validation](https://forum.vac.dev/t/idontwant-message-impact/283)\\r\\n- [GossipSub for Big Messages](https://hackmd.io/X1DoBHtYTtuGqYg0qK4zJw#2)\\r\\n- [Regression Test Results: nim-libp2p](https://zealous-polka-dc7.notion.site/Nim-libp2p-v1-5-0-regression-testing-August-2024-25edba733c704ccaa411919555c5db1a)"},{"id":"vac101-fiat-shamir","metadata":{"permalink":"/rlog/vac101-fiat-shamir","source":"@site/rlog/2024-10-15-vac101-fiat-shamir.mdx","title":"Vac 101: Transforming an Interactive Protocol to a Noninteractive Argument","description":"In this post, we introduce a common technique used to convert interactive protocols to their noninteractive variant.","date":"2024-10-15T12:00:00.000Z","formattedDate":"October 15, 2024","tags":[],"readingTime":10.66,"hasTruncateMarker":true,"authors":[{"name":"Marvin","github":"jonesmarvin8","key":"marvin"}],"frontMatter":{"title":"Vac 101: Transforming an Interactive Protocol to a Noninteractive Argument","date":"2024-10-15T12:00:00.000Z","authors":"marvin","published":true,"slug":"vac101-fiat-shamir","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Libp2p GossipSub IDONTWANT Message Performance Impact","permalink":"/rlog/gsub-idontwant-perf-eval"},"nextItem":{"title":"zkVM Testing Report: Evaluating Zero-Knowledge Virtual Machines for Nescience","permalink":"/rlog/zkVM-testing"}},"content":"In this post, we introduce a common technique used to convert interactive protocols to their noninteractive variant.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nThe set of interactive protocols form a class of protocols that consist of communication between two parties: the Prover and the Verifier.\\nThe Prover tries to convince the Verifier of a given claim.\\nFor example, the Prover may want to convince the Verifier that she owns a specific Unspent Transaction Output (UTXO);\\nthat is, the Prover possesses the ability to spend the UTXO.\\nIn many instances, there is information that the Prover does not wish to reveal to the Verifier.\\nIn our example, it is critical that the Prover does not provide the Verifier with the spending key associated with her UTXO.\\nIn addition to the Prover\'s claim and secret data, there is additional data, public parameters, that the claimed statement is expressed in terms of.\\nThe public parameters can be thought of as the basis for all similar claims.\\n\\nIn an interactive protocol, the Prover and the Verifier are in active communication.\\nSpecifically, the Prover and the Verifier exchange messages so that the Verifier can validate the Prover\'s claim.\\nHowever, this communication is not practical for many applications.\\nIt is necessary that any party can verify the Prover\'s claim in decentralized systems.\\nIt is impractical for the Prover to be in active communication with a large number of verifying parties.\\nInstead, it is desirable for the Prover to generate a proof on their own that can convince any party.\\nTo achieve this, it is necessary for the Prover to generate the Verifier\'s messages in such a way\\nthat the Prover cannot manipulate the Verifier\'s messages for her benefit.\\nThe Fiat-Shamir heuristic [1](https://dl.acm.org/doi/10.5555/36664.36676) is used for this purpose.\\nEven though much of our discussion will focus on $\\\\Sigma$-protocols,\\nthe Fiat-Shamir heuristic is not limited to $\\\\Sigma$-protocols.\\nThe Fiat-Shamir heuristic has been applied to zk-SNARKs, but the security in this setting has been the subject of discussion and research in recent years.\\nBlock et al. [2](https://eprint.iacr.org/2023/1071) provide the first formal analysis of Fiat-Shamir heuristic in zk-SNARKs.\\n\\n## Sigma Protocols\\n\\nA $\\\\Sigma$-protocol is a family of interactive protocols that consists of three publicly transmitted messages between the Prover and the Verifier.\\nIn particular, the protocol has the following framework:\\n\\n|Prover| | Verifier|\\n|----|----|----|\\n| | $\\\\stackrel{\\\\mathsf{commitment}}{\\\\longrightarrow}$| |\\n| | $\\\\stackrel{\\\\mathsf{challenge}}{\\\\longleftarrow}$| |\\n| | $\\\\stackrel{\\\\mathsf{response}}{\\\\longrightarrow}$| |\\n\\nThese three messages form the protocol\'s transcript: $(\\\\mathsf{commitment}, \\\\mathsf{challenge}, \\\\mathsf{response})$.\\nThe Verifier uses all three of these messages to validate the Prover\'s original claim.\\nThe Verifier\'s challenge should be selected uniform random from all possible challenges.\\nBased on this selection, a dishonest Prover can only convince the Verifier with a negligible probability.\\n\\n\\n### The Schnorr Protocol\\nThe Schnorr protocol [3](https://link.springer.com/chapter/10.1007/0-387-34805-0_22) is usually the first $\\\\Sigma$-protocol that one studies.\\nAdditionally, the Schnorr protocol can be used as an efficient signature scheme.\\nThe Schnorr protocol provides a framework that enables the Prover to convince the Verifier that: for group elements $g$ and $X$,\\nthe Prover knows the power to raise $g$ to obtain $X$.\\nSpecifically, the Prover possesses some integer $x$ so that $X = g^x$.\\nCryptographic resources may use either multiplicative or additive notation for groups;\\nwe will use multiplicative notation.\\nBriefly, the element $g$ being combined with itself in multiplicative notation is $g \\\\cdot g = g^2$,\\nwhile in additive notation it is $g + g = 2g$.\\nWe assume that our group is of prime order $p$, and is sufficiently large to satisfy the discrete logarithm assumption.\\n\\nThe Schnorr protocol proceeds as follows:\\n\\n|Prover| | Verifier|\\n|----|----|----|\\n|$t \\\\in_R \\\\mathbb{Z}_p$, $T := g^t$| $\\\\stackrel{T}{\\\\longrightarrow}$| |\\n|  | $\\\\stackrel{c}{\\\\longleftarrow}$ |$c \\\\in_R \\\\mathbb{Z}_p$|\\n| $z := t + xc$ | $\\\\stackrel{z}{\\\\longrightarrow}$ | |\\n| | | output 1 provided $g^z \\\\stackrel{?}{=} T X^c$|\\n\\n\\n### Chaum-Pedersen protocol\\nA tuple of group elements $(U,V,W)$ is a DH-triple if and only if there exists some $x \\\\in \\\\mathbb{Z}_p$ so that $V = g^x$ and $W = U^x$.\\nThe Chaum-Pedersen protocol provides a framework that enables a Prover to convince a Verifier that she possesses such a $x$ for a claimed DH-triple $(U,V,W)$.\\nThe Chaum-Pedersen protocol proceeds as follows:\\n\\n|Prover| | Verifier|\\n|----|----|----|\\n|$t \\\\in_R \\\\mathbb{Z}_p$, $T := g^t$, $S := U^t$| $\\\\stackrel{T,S}{\\\\longrightarrow}$| |\\n|  | $\\\\stackrel{c}{\\\\longleftarrow}$ | $c \\\\in_R \\\\mathbb{Z}_p$|\\n| $z := t + xc$ | $\\\\stackrel{z}{\\\\longrightarrow}$ | |\\n| | | output 1 provided $g^z \\\\stackrel{?}{=} T V^c$ and $U^z \\\\stackrel{?}{=} SW^c$|\\n\\n## Hash Functions\\nCryptographic hash functions serve as the backbone to the Fiat-Shamir heuristic.\\nA hash function, $\\\\mathsf{Hash}$, is a special function that takes in an arbitrary binary string and outputs a binary string of a predetermined fixed length.\\nSpecifically,\\n$\\\\mathsf{Hash} : \\\\{0,1\\\\}^* \\\\rightarrow \\\\{0,1\\\\}^n$.\\n\\nThe security of cryptographic hash functions will rely on certain tasks being computationally infeasible.\\nA task is computationally infeasible provided that there is no deterministic algorithm that can conclude the task in polynomial-time.\\n\\nA cryptographic hash function satisfies the following properties:\\n- **Succinct**: The hash function should be easy to compute; the hash $\\\\mathsf{Hash}({\\\\bf{b}})$ can be efficiently computed for any binary string ${\\\\bf{b}}$.\\n- **Preimage Resistance**: It should be computationally infeasible to work backwards given the output of a hash function. Let ${\\\\bf{y}}$ be a binary string of length $n$.\\n    It should be \'impossible\' to find some binary string ${\\\\bf{x}}$ so that ${\\\\bf{y}} = \\\\mathsf{Hash}({\\\\bf{x}})$. \\n- **Collision Resistance**: It should be difficult to find two strings that hash to the same value.\\nIt should be computationally infeasible to find two binary strings ${\\\\bf{x}_1}$ and ${\\\\bf{x}_2}$ so that $\\\\mathsf{Hash}({\\\\bf{x}_1}) = \\\\mathsf{Hash}({\\\\bf{x}_2}).$\\n\\nA related class of functions is one-way functions.\\nA one-way function satisfies the first two conditions of a cryptographic hash function (succinct and preimage resistance).\\nAll cryptographic hash functions are a one-way functions.\\nHowever, one-way functions do not necessarily satisfy collision-resistance.\\nWe will simply refer to cryptographic hash functions as hash functions for the rest of this blog.\\nCommonly used hash functions include SHA-256 [5](https://www.cs.princeton.edu/~appel/papers/verif-sha.pdf),\\nKeccak [6](https://keccak.team/keccak_specs_summary.html), and Poseidon [7](https://eprint.iacr.org/2019/458).\\n\\n## The Fiat-Shamir heuristic\\nThe Fiat-Shamir heuristic is the technique used to convert an interactive protocol to a noninteractive protocol.\\nThis is done by replacing each of the Verifier\'s messages with a hashed value.\\nSpecifically, the Prover generates the Verifier\'s message by evaluating the hash function $\\\\mathsf{Hash}$\\nwith the concatenation of all public values that appear in the protocol thus far.\\nIf $m_0, \\\\dots, m_t$ denote the public values in the protocol thus far,\\nthen the Verifier\'s message is computed as $m_{t+1} := \\\\mathsf{Hash}(m_0|| \\\\cdots || m_t)$.\\n\\nSince $\\\\mathsf{Hash}$ can be efficiently computed, and the messages $m_0, \\\\dots, m_t$ are public, then any verifying party can compute $m_{t+1}$.\\nCritically, since $\\\\mathsf{Hash}$ is preimage resistant and collision resistant,\\nthe Prover cannot manipulate her choices of the messages $m_0,\\\\dots, m_t$ to influence the message $m_{t+1}$.\\nHence, verifying parties can trust that $m_{t+1}$ is sufficiently random with respect to the preceding messages.\\n\\nThere are two variants of the Fiat-Shamir heuristic: weak and strong.\\nThe weak variant uses all of the publicly traded messages in computing the Verifier\'s messages but does not include the public parameters.\\nHowever, in the strong variant all of the publicly traded messages and public parameters are used to compute the Verifier\'s messages.\\nWe will provide a discussion on issues that can arise from using the weak Fiat-Shamir heuristic.\\n\\n### Schnorr Protocol with the strong Fiat-Shamir\\n\\nWhen the strong Fiat-Shamir heuristic is applied to the Schnorr protocol, the message $c = \\\\mathsf{Hash}(g||X||T)$.\\nThis choice of $c$ provides security since it should be computationally infeasible to find collisions for the outputs of $\\\\mathsf{Hash}$.\\nThus, $c$ fixes the group elements $g$, $X$ and $T$.\\n\\nThe elements that would be omitted in the hash by applying weak Fiat-Shamir heuristic are $g$ and $X$.\\n\\n### Chaum-Pedersen Protocol with the strong Fiat-Shamir\\n\\nThe message $c = Hash(g||U||V||W||T||S)$ when the Prover applies the strong Fiat-Shamir heuristic to the Chaum-Pedersen protocol.\\nThe properties of $\\\\mathsf{Hash}$ fixes the generator $g$ and the Prover\'s statement $(U,V,W)$.\\n\\n## Improper use of the Fiat-Shamir heuristic\\n\\nThe Fiat-Shamir heuristic appears to be a fairly straightforward technique to implement.\\nHowever, a subtle but serious issue that can occur in the application of the Fiat-Shamir heuristic has been a point of discussion for the past few years.\\nThe issue concerns what messages are included in the hash.\\nIn particular, are the public parameters used to compute the hash value?\\n\\nBernhard et al. [8](https://eprint.iacr.org/2016/771.pdf) provide a discussion of Fiat-Shamir heuristic restricted to $\\\\Sigma$-protocols.\\nIn particular, Bernhard et al. discuss the pitfalls of the weak Fiat-Shamir heuristic.\\nRecall that the strong Fiat-Shamir heuristic requires that the public parameters are included in the calculations of the Verifier\'s messages while the weak version does not.\\nThe inclusion of the public parameters in the hash evaluations fixes these public values for the entire protocol.\\nThis means that the Prover cannot retroactively change them.\\n\\nThe issues with the differences in the variants of the Fiat-Shamir heuristics has persisted since Bernhard et al.\'s paper. \\nIn recent years, auditors from [Trail of Bits](https://www.trailofbits.com/) and [OpenZeppelin](https://www.openzeppelin.com/) have \\nreleased blogs ([9](https://blog.trailofbits.com/2022/04/13/part-1-coordinated-disclosure-of-vulnerabilities-affecting-girault-bulletproofs-and-plonk/),\\n [10](https://blog.trailofbits.com/2022/04/14/the-frozen-heart-vulnerability-in-giraults-proof-of-knowledge/),\\n [11](https://blog.trailofbits.com/2022/04/15/the-frozen-heart-vulnerability-in-bulletproofs/), [12](https://blog.trailofbits.com/2022/04/18/the-frozen-heart-vulnerability-in-plonk/), [13](https://blog.openzeppelin.com/the-last-challenge-attack))\\n and papers ([14](https://eprint.iacr.org/2023/691), [15](https://eprint.iacr.org/2024/398)) \\n describing specific vulnerabilities in zero-knowledge papers and repositories associated with the use of the weak Fiat-Shamir heuristic.\\n\\nTrail of Bits coined the term **FROZEN Heart** to describe the use of weak Fiat-Shamir heuristic.\\nFrozen comes from the phrase \\"FoRging Of ZEro kNowledge proofs\\",\\nand Fiat-Shamir is the \\"heart\\" of transforming an interactive protocol to noninteractive protocol.\\n\\nNow, we examine how weak Fiat-Shamir affects the Schnorr protocol and Chaum-Pedersen protocol.\\n\\n### Schnorr protocol with the weak Fiat-Shamir heuristic\\nFor Schnorr, we will examine two variants:\\nthe first where we only include the Prover\'s claim $X$ but not the public parameter $g$, and\\nthe second where we include the public parameter $g$ but not the Prover\'s claim $X$.\\n\\nSince we omit the generator $g \\\\in \\\\mathbb{G}$ from the computation for the message $c$ in our first approach,\\nthen $c = \\\\mathsf{Hash}(X||T)$.\\n\\nNow, a malicious Prover can complete the transcript for the Schnorr protocol by selecting any $z \\\\in_R \\\\mathbb{Z}_p$.\\nSince, $g$ is not fixed as it was not included in the computation of $c$.\\nBut, the malicious Prover needs the transcript $(T,c,z)$ to satisfy $g^z = TX^c$.\\nHence, the malicious Prover can compute the generator $g = (TX^c)^{z^{-1}}.$\\n\\nIn our second approach, we omit the group element $X \\\\in \\\\mathbb{G}$ from the computation for the challenge $c$.\\nThat is, $c = \\\\mathsf{Hash}(g||T)$.\\n\\nAs with the previous example, the malicious Prover takes a Schnorr transcript $(T,c,z)$ where $z \\\\in_R \\\\mathbb{Z}_p$.\\nIt is necessary for the malicious Prover to find a value $X$ so that $g^z = TX^c$.\\nThis can be acheived by computing $X = (g^z T^{-1})^{c^{-1}}$.\\n\\n### Chaum-Pedersen protocol with the Fiat-Shamir heuristic\\nThe Verifier\'s message $c = Hash(T,S)$ when weak Fiat-Shamir heuristic is applied.\\nThe Prover\'s triple $(U,V,W)$ and the generator $g$ are not fixed by $c$.\\nAs such, a malicious Prover can generate values for $U,V,W$, and $g$ that satisfy the Verifier\'s identity checks.\\nIn the case of a malicious Prover, $T$ and $S$ are randomly group elements instead of being computed using a value $t$ that the Prover selected.\\nThis means a malicious Prover must randomly select the value $z$ as well.\\n\\nGiven the values that have been fixed so far, each of the Verifier\'s identities consists of two unknowns.\\nHence, it is necessary to select one of these unknowns from each identity so that a malicious Prover can compute the last value.\\nFor instances, suppose that the malicious Prover randomly selects $V$ and $W$.\\nThe malicious Prover can compute $g = (T V^c)^{1/z}$ and $V = (SW^c)^{1/z}$.\\nThus, the malicious Prover has a claimed statement $(U,V,W)$ for generator $g$ that passes the Verifier\'s identities using weak Fiat-Shamir heuristic.\\n\\nThe omission of any of the values $U,V,W,$ and $g$ in the computation of $c$ allows a malicious Prover to forge a proof.\\n\\n## Conclusion\\nThe Fiat-Shamir heuristic is an essential technique to convert an interactive protocol to a variant that does not require communication.\\nAdditionally, careful application of this technique is necessary to maintain the integrity of the system. \\n\\n### References\\n- 1. [How to Prove Yourself: Practical Solutions to Identification and Signature Problems](https://dl.acm.org/doi/10.5555/36664.36676)\\n- 2. [Fiat-Shamir Security of FRI and Related SNARKs](https://eprint.iacr.org/2023/1071)\\n- 3. [Efficient Identification and Signatures for Smart Cards](https://link.springer.com/chapter/10.1007/0-387-34805-0_22)\\n- 4. [Wallet Databases with Observers](https://link.springer.com/content/pdf/10.1007/3-540-48071-4_7.pdf)\\n- 5. [Verification of a Cryptographic Primitive: SHA-256](https://www.cs.princeton.edu/~appel/papers/verif-sha.pdf)\\n- 6. [Keccak specifications summary](https://keccak.team/keccak_specs_summary.html)\\n- 7. [Poseidon: A New Hash Function for Zero-Knowledge Proof Systems](https://eprint.iacr.org/2019/458)\\n- 8. [How not to Prove Yourself: Pitfalls of the Fiat-Shamir Heuristic and Applications to Helios](https://eprint.iacr.org/2016/771.pdf)\\n- 9. [Frozen Heart - Part 1](https://blog.trailofbits.com/2022/04/13/part-1-coordinated-disclosure-of-vulnerabilities-affecting-girault-bulletproofs-and-plonk/)\\n- 10. [Frozen Heart - Part 2](https://blog.trailofbits.com/2022/04/14/the-frozen-heart-vulnerability-in-giraults-proof-of-knowledge/)\\n- 11. [Frozen Heart - Part 3](https://blog.trailofbits.com/2022/04/15/the-frozen-heart-vulnerability-in-bulletproofs/)\\n- 12. [Frozen Heart - Part 4](https://blog.trailofbits.com/2022/04/18/the-frozen-heart-vulnerability-in-plonk/)\\n- 13. [The Last Challenge Attack Blog](https://blog.openzeppelin.com/the-last-challenge-attack)\\n- 14. [Weak Fiat-Shamir Attacks on Modern Proof Systems](https://eprint.iacr.org/2023/691)\\n- 15. [The Last Challenge Attack](https://eprint.iacr.org/2024/398)"},{"id":"zkVM-testing","metadata":{"permalink":"/rlog/zkVM-testing","source":"@site/rlog/2024-09-26-Zkvm-testing.mdx","title":"zkVM Testing Report: Evaluating Zero-Knowledge Virtual Machines for Nescience","description":"Following our initial exploration of zkVMs in our previous blog post [1],","date":"2024-09-26T12:00:00.000Z","formattedDate":"September 26, 2024","tags":[],"readingTime":11.985,"hasTruncateMarker":true,"authors":[{"name":"Moudy","github":"moudyellaz","key":"moudy"}],"frontMatter":{"title":"zkVM Testing Report: Evaluating Zero-Knowledge Virtual Machines for Nescience","date":"2024-09-26T12:00:00.000Z","authors":"moudy","published":true,"slug":"zkVM-testing","categories":"research","discuss":"https://forum.vac.dev/t/zkvm-testing-report-evaluating-zero-knowledge-virtual-machines-for-nescience/","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Vac 101: Transforming an Interactive Protocol to a Noninteractive Argument","permalink":"/rlog/vac101-fiat-shamir"},"nextItem":{"title":"Exploring zkVMs: Which Projects Truly Qualify as Zero-Knowledge Virtual Machines?","permalink":"/rlog/zkVM-explorations"}},"content":"\x3c!--truncate--\x3e\\n\\n# Introduction\\n\\nFollowing our initial exploration of zkVMs in our previous blog post [[1](https://vac.dev/rlog/zkVM-explorations/)], \\nwe have conducted a series of tests to identify the most suitable zkVM for the Nescience architecture [[2](https://vac.dev/rlog/Nescience-state-separation-architecture)]. \\nThis post outlines the testing process, results, and conclusions. Additionally, the full test suite and scripts can be found \\non our GitHub page [[3](https://github.com/vacp2p/nescience-zkvm-testing)], allowing others to replicate the results or explore the candidates further. \\nPlease note that we chose not to use hardware acceleration in our benchmarks, as our project is aimed at a broad audience. \\nParticularly, we cannot assume AVX512 support by default, as it is typically available only in high-end CPUs.\\n\\nWe\'ve shortlisted the following zkVMs for testing:\\n\\n- SP1 [[4](https://blog.succinct.xyz/introducing-sp1/)]\\n- RISC0 [[5](https://www.risczero.com/zkvm)]\\n- Nexus [[6](https://docs.nexus.xyz/)]\\n- ZkMIPS [[7](https://docs.zkm.io/zkm-architecture)]\\n- ZkWASM [[8](https://delphinuslab.com/zk-wasm/)]\\n- Valida [[9](https://delendum.xyz/writings/2023-05-10-zkvm-design.html)]\\n\\n\\n# Why these candidates?\\n\\nWhen narrowing down the zkVMs, we focused on key factors:\\n\\n- True zero-knowledge functionality: The zkVMs had to demonstrate or be close to demonstrating the ability to generate and verify zero-knowledge proofs.\\n- Performance baselines: We sought zkVMs with solid benchmarks in performance, particularly in speed and efficiency.\\n- Specific functionalities: For Nescience, functionalities like lookup tables, precompiles, and recursive capabilities are critical. \\n\\nWe need a zkVM that supports these to enable robust project development.\\n\\n\\n# Preliminary information on the candidates\\n\\n1. SP1 is a performant, open-source zkVM that verifies the execution of arbitrary Rust (or any LLVM-compiled language) programs. \\nSP1 utilizes Plonky3, enabling recursive proofs and supporting a wide range of cryptographic algorithms, including ECC-based ones like Groth16. \\nWhile it supports aggregation, it appears not to support zero knowledge in a conventional manner.\\n\\n2. RISC0 zkVM allows one to prove the correct execution of arbitrary Rust code. Built on a RISC-V architecture, it is inherently adaptable \\nfor implementing standard cryptographic hash functions such as SHA-256 and ECDSA. RISC0 employs STARKs, providing a security level of 98 bits. \\nIt supports multiple programming languages, including C and Rust, thanks to its compatibility with LLVM and WASM.\\n\\n3. Nexus is a modular, extensible, open-source, highly parallelized, prover-optimized, and contributor-friendly zkVM written in Rust. \\nIt focuses on performance and security, using the Nova folding scheme, which is particularly effective for recursive proofs. \\nNexus also supports precompiles and targeted compilation, and besides Rust, it offers C++ support.\\n\\n4. ZkMIPS is a general verifiable computing infrastructure based on Plonky2 and the MIPS microarchitecture, aiming to empower Ethereum \\nas a global settlement layer. It can run arbitrary Rust code as well. Notably, zkMIPS is the only zkVM in this list that utilizes the MIPS opcode set.\\n\\n5. ZkWASM adheres to and supports the unmodified standard WASM bytecode specification. Since Rust code can be compiled to WASM bytecode, \\none could theoretically run any Rust code on a zkWASM machine, providing flexibility and broad language support.\\n\\n6. Valida is a STARK-based virtual machine aiming to improve upon the state of the art in several categories:\\n    - Code reuse: The VM has a RISC-inspired instruction set, simplifying the targeting of conventional programming languages. \\n    A backend compiler is being developed to compile LLVM IR to the Valida ISA, enabling the proving of programs written in Rust, \\n    Go, C++, and others with minimal to no changes in source code.\\n    - Prover performance: Engineered to maximize prover performance, Valida is compatible with a 31-bit field, restricted to degree 3 constraints, \\n    and features minimal instruction decoding. It operates directly on memory without general-purpose registers or a dedicated stack, \\n    utilizing newer lookup arguments to reduce trace overhead involved in cross-chip communication.\\n    - Extensibility: Designed to be customizable, Valida can easily be extended to include an arbitrary number of user-defined instructions. \\n    Procedural macros are used to construct the desired machine at compile time, avoiding any runtime penalties.\\n\\nValida appears to be in the early stages of development but already showcases respectable performance metrics.\\n\\n# Testing plan\\n\\nTo thoroughly evaluate each zkVM, we devised a two-stage testing process:\\n\\n- Stage 1: Arithmetic operations\\n\\n    The first phase focused on evaluating the zkVMs\u2019 ability to handle basic arithmetic operations: addition, subtraction, multiplication, \\n    division, modulus division, and square root calculations. We designed the test around heptagonal numbers, which required zkVMs to process \\n    multiple arithmetic operations simultaneously. By using this method, we could measure efficiency and speed in handling complex mathematical calculations \u2013 \\n    a crucial element for zkVM performance. \\n\\n- Stage 2: Memory consumption\\n\\n    For the second phase, we evaluated each zkVM\u2019s ability to manage memory under heavy loads. We tested several data structures, including lists, \\n    hash maps, deques, queues, BTreeMaps, hash sets, and binary heaps. Each zkVM underwent tests for the following operations:\\n\\n    - Insert: How quickly can the zkVM add data to structures?\\n    - Delete: Does the zkVM handle memory release effectively?\\n    - Append: Can the zkVM efficiently grow data structures?\\n    - Search: How fast and efficient is the zkVM when retrieving stored data?\\n\\nThe purpose of this stage was to identify any memory bottlenecks and to determine whether a zkVM could manage high-intensity tasks efficiently, \\nsomething vital for the Nescience project\u2019s complex, data-heavy processes.\\n\\n# Machine specifications\\n\\nThe tests were conducted on the following hardware configuration:\\n\\n- CPU: AMD EPYC 7713 \\"Milan\\" 64-core processor (128 threads total)\\n- RAM: 600GiB DDR4 3200MHz ECC RAM, distributed across 16 DIMMs\\n- Host OS: Proxmox 8.3\\n- Hypervisor: KVM\\n- Network layer: Open vSwitch\\n- Machine model: Supermicro AS-2024US-TRT\\n\\n# Results\\n\\n### 1. SP1\\n\\nSP1 does not provide zero-knowledge capability in its proofs but delivers respectable performance, though slightly behind its main competitor. \\nMemory leaks were minimal, staying below the 700 KB threshold. Interestingly, SP1 consumed more RAM during the basic arithmetic \\ntest than in memory allocation tests, showcasing the team\'s effective handling of memory under load. In the basic test, \\nallocations were primarily in the 9-16 B, 33-64 B, and 65-128 B ranges. For memory allocations, most fell into the 129-256 B range.\\n- Stage 1: Hept 100 test\\n    - Proof size: 3.108 MB\\n    - Proof time: 16.95 seconds\\n\\n| ![Image 1](/img/zkvmtest/general11.png) | ![Image 2](/img/zkvmtest/alloc11.png) | ![Image 3](/img/zkvmtest/tempalloc11.png) | ![Image 4](/img/zkvmtest/consumed11.png) | ![Image 5](/img/zkvmtest/sizes11.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n- Stage 2: Vec 10000 test\\n    - Proof size: 3.17 MB\\n    - Proof time: 20.85 seconds\\n\\n| ![Image 1](/img/zkvmtest/general12.png) | ![Image 2](/img/zkvmtest/alloc12.png) | ![Image 3](/img/zkvmtest/tempalloc12.png) | ![Image 4](/img/zkvmtest/consumed12.png) | ![Image 5](/img/zkvmtest/sizes12.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\n---\\n### 2. RISC0\\nRISC0 stands out with exceptional performance in proof size and generation time, ranking among the best \\n(with the exception of Valida and zkWASM\'s basic test). It also handles memory well, with minor leaks under 0.5 MB \\nand controlled RAM consumption staying below 2.2 GB. RISC0\'s memory allocations were consistent, primarily in the 17-32 B and 33-64 B ranges.\\n\\n- Stage 1: Hept 100 test\\n    - Proof size: 217.4 KB\\n    - Proof time: 9.73 seconds\\n\\n| ![Image 1](/img/zkvmtest/general21.png) | ![Image 2](/img/zkvmtest/alloc21.png) | ![Image 3](/img/zkvmtest/tempalloc21.png) | ![Image 4](/img/zkvmtest/consumed21.png) | ![Image 5](/img/zkvmtest/sizes21.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\n- Stage 2: Vec 10000 test\\n     - Proof size: 217.4 KB\\n    - Proof time: 16.63 seconds\\n\\n| ![Image 1](/img/zkvmtest/general22.png) | ![Image 2](/img/zkvmtest/alloc22.png) | ![Image 3](/img/zkvmtest/tempalloc22.png) | ![Image 4](/img/zkvmtest/consumed22.png) | ![Image 5](/img/zkvmtest/sizes22.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\nBased on these results, RISC0 is a solid candidate for Nescience.\\n\\n---\\n### 3. Nexus\\nNexus\' performance offers interesting insights into folding scheme-based zkVMs. Surprisingly, proof sizes remained consistent \\nregardless of workload, with no significant memory leaks (under 700 KB). However, while RAM consumption increased slightly with higher \\nworkloads (up to 1.2 GB), Nexus performed poorly during memory allocation tests, making it unsuitable for our use case.\\n\\n- Allocation details:\\n    - Basic test: Most allocations concentrated in 65-128 B\\n    - Memory-heavy test: Allocations in the 129-256 B range\\n\\n- Stage 1: Hept 100 test\\n     - Proof size: 46 MB\\n     - Proof time: 12.06 seconds\\n\\n| ![Image 1](/img/zkvmtest/general31.png) | ![Image 2](/img/zkvmtest/alloc31.png) | ![Image 3](/img/zkvmtest/tempalloc31.png) | ![Image 4](/img/zkvmtest/consumed31.png) | ![Image 5](/img/zkvmtest/sizes31.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\n- Stage 2: Vec 10000 test\\n     - Proof size: 46 MB\\n     - Proof time: 56 minutes\\n     \\n| ![Image 1](/img/zkvmtest/general32.png) | ![Image 2](/img/zkvmtest/alloc32.png) | ![Image 3](/img/zkvmtest/tempalloc32.png) | ![Image 4](/img/zkvmtest/consumed32.png) | ![Image 5](/img/zkvmtest/sizes32.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\n---\\n### 4. ZkMIPS\\nZkMIPS presents an intriguing case. While it shows good results in terms of proof size and generation time during the basic test, \\nthese come at the cost of significant RAM usage and memory leaks. The memory allocation test revealed a concerning 6.7 GB memory leak, \\nwith 0.5 GB leaked during the basic test. Despite this, RAM consumption (while high at 17+ GB) remains stable under higher workloads. \\nAllocation sizes are spread across several ranges, with notable concentrations in the 17-32 B, 65-128 B, and 257-512 B slots.\\n\\n- Stage 1: Hept 100 test\\n    - Proof size: 4.3 MB\\n    - Proof time: 9.32 seconds\\n\\n| ![Image 1](/img/zkvmtest/general41.png) | ![Image 2](/img/zkvmtest/alloc41.png) | ![Image 3](/img/zkvmtest/tempalloc41.png) | ![Image 4](/img/zkvmtest/consumed41.png) | ![Image 5](/img/zkvmtest/sizes41.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\n- Stage 2: Vec 10000 test\\n     - Proof size: 4.898 MB\\n     - Proof time: 42.57 seconds\\n\\n| ![Image 1](/img/zkvmtest/general42.png) | ![Image 2](/img/zkvmtest/alloc42.png) | ![Image 3](/img/zkvmtest/tempalloc42.png) | ![Image 4](/img/zkvmtest/consumed42.png) | ![Image 5](/img/zkvmtest/sizes42.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\nThis zkVM provides mixed results with strong proof generation but concerning memory management issues.\\n\\n---\\n### 5. ZkWASM\\nZkWASM, unfortunately, performed poorly in both stages regarding proof size and generation time. RAM consumption was particularly high, \\nexceeding 7 GB in the basic test, and an astounding 57 GB during memory allocation tests. Despite its impressive memory usage, \\nthe proof sizes were relatively large at 18 KB and 334 KB respectively. Allocation sizes were mainly concentrated in the 33-64 B range, \\nwith neighboring slots contributing small but notable amounts.\\n\\n- Stage 1: Hept 100 test\\n     - Proof size: 18 KB\\n     - Proof time: 42.7 seconds\\n\\n| ![Image 1](/img/zkvmtest/general51.png) | ![Image 2](/img/zkvmtest/alloc51.png) | ![Image 3](/img/zkvmtest/tempalloc51.png) | ![Image 4](/img/zkvmtest/consumed51.png) | ![Image 5](/img/zkvmtest/sizes51.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|\\n\\n- Stage 2: Vec 10000 test\\n     - Proof size: 334 KB\\n     - Proof time: 323 seconds\\n\\n| ![Image 1](/img/zkvmtest/general52.png) | ![Image 2](/img/zkvmtest/alloc52.png) | ![Image 3](/img/zkvmtest/tempalloc52.png) | ![Image 4](/img/zkvmtest/consumed52.png) | ![Image 5](/img/zkvmtest/sizes52.png) |\\n|------------------------|------------------------|------------------------|------------------------|------------------------|    \\n\\n---\\n### 6. Valida\\nValida delivered impressive results in proof generation speed and size, with a proof size of 280 KB and a proof time of < 1 second. \\nHowever, profiling was not possible due to Valida\'s limited Rust support. Valida currently compiles Rust using the LLVM backend, \\ntranspiling LLVM IR to leverage its C/C++ implementation, which leads to errors when handling Rust-specific data structures or dependencies. \\nAs a result, complex memory interactions couldn\'t be tested, and using Valida with Rust code is currently not advisable. \\nA GitHub issue has been opened to address this.\\n\\n---\\n## Summary table\\n### Stage 1\\n\\n| zkVM   | Proof time | Proof size | Peak RAM consumption | Memory leaked |\\n|--------|------------|------------|----------------------|---------------|\\n| SP1    | 16.95 s    | 3.108 MB   | 2.1 GB               | 656.8 KB      |\\n| RISC0  | 9.73 s     | 217.4 KB   | 1.9 GB               | 470.5 KB      |\\n| Nexus  | 12.06 s    | 46 MB      | 9.7 MB               | 646.5 KB      |\\n| ZkMIPS | 9.32 s     | 4.3 MB     | 17.3 GB              | 453.8 MB      |\\n| ZkWASM | 42.7 s     | 18 KB      | 8.2 GB               | 259.4 KB      |\\n| Valida | < 1 s      | 280 KB     | N/A                  | N/A           |\\n\\n---\\n\\n### Stage 2\\n\\n| zkVM   | Proof time | Proof size | Peak RAM consumption | Memory leaked |\\n|--------|------------|------------|----------------------|---------------|\\n| SP1    | 20.85 s    | 3.17 MB    | 1.9 GB               | 616 KB        |\\n| RISC0  | 16.63 s    | 217.4 KB   | 2.3 GB               | 485.3 KB      |\\n| Nexus  | 56 m       | 46 MB      | 1.9 GB               | 616 KB        |\\n| ZkMIPS | 42.57 s    | 4.898 MB   | 18.9 GB              | 6.9 GB        |\\n| ZkWASM | 323 s      | 334 KB     | 58.8 GB              | 259.4 KB      |\\n| Valida | N/A        | N/A        | N/A                  | N/A           |\\n\\n---\\n# Summary\\n\\n\\nAfter an extensive evaluation of six zkVM candidates for the Nescience project, RISC0 emerged as the top choice. \\nIt excels in both proof generation time and size while maintaining a reasonable memory footprint. With strong zero-knowledge \\nproof capabilities and support for multiple programming languages, it aligns well with our project\'s needs for privacy, \\nperformance, and flexibility. Its overall balance between performance and efficiency makes it the most viable zkVM at this stage.\\n\\nValida, while promising with its potential for high prover performance, is still in early development and suffers from Rust integration issues. \\nThe current LLVM IR transpilation limitations mean it cannot handle complex memory interactions, disqualifying it for now. \\nHowever, once its development matures, Valida could become a strong alternative, and we plan to revisit it as it evolves.\\n\\nSP1, though initially interesting, failed to meet the zero-knowledge proof requirement. Its performance in arithmetic operations was \\nrespectable but insufficient to justify further consideration given its lack of ZK functionality \u2013 critical for our privacy-first objectives.\\n\\nNexus demonstrated consistent proof sizes and manageable memory usage, but its lackluster performance during memory-intensive tasks and \\nits proof size (especially for larger workloads) disqualified it from being a top contender. While zkMIPS delivered solid proof times, \\nthe memory issues were too significant to ignore, making it unsuitable.\\n\\nFinally, zkWASM exhibited the poorest results, struggling both in proof size and generation time. Despite its potential for WASM bytecode support, \\nthe excessive RAM consumption (up to 57 GB in the memory test) rendered it impractical for Nescience\u2019s use case.\\n\\nIn conclusion, RISC0 is the best fit for Nescience at this stage, but Valida remains a future candidate as its development progresses.\\n\\nIn the future, we plan to compare RISC0 and SP1 with CUDA acceleration. Ideally, by that time, more zkVMs will include similar acceleration capabilities, \\nenabling a fairer and more comprehensive comparison across platforms.\\n\\n\\nWe\u2019d love to hear your thoughts on our zkVM testing process and results! Do you agree with our conclusions, or do you think we missed a promising zkVM? \\nWe\u2019re always open to feedback, insights, and suggestions from the community. \\n\\nJoin the discussion and share your perspectives on \\n[our forum](https://forum.vac.dev/t/zkvm-testing-report-evaluating-zero-knowledge-virtual-machines-for-nescience/) or try out the \\ntests yourself through our [GitHub page](https://github.com/vacp2p/nescience-zkvm-testing)!\\n\\n\\n\\n\\n\\n\\n\\n\\n# References\\n\\n[1] Exploring zkVMs: Which Projects Truly Qualify as Zero-Knowledge Virtual Machines? Retrieved from https://vac.dev/rlog/zkVM-explorations/\\n\\n[2] Nescience: A User-Centric State-Separation Architecture. Retrieved from https://vac.dev/rlog/Nescience-state-separation-architecture\\n\\n[3] Our GitHub Page for zkVM Testing. Retrieved from https://github.com/vacp2p/nescience-zkvm-testing\\n\\n[4] Introducing SP1: A performant, 100% open-source, contributor-friendly zkVM. Retrieved from https://blog.succinct.xyz/introducing-sp1/\\n\\n[5] The first general purpose zkVM. Retrieved from https://www.risczero.com/zkvm\\n\\n[6] The Nexus 2.0 zkVM. Retrieved from https://docs.nexus.xyz/\\n\\n[7] ZKM Architecture. Retrieved from https://docs.zkm.io/zkm-architecture\\n\\n[8] ZK-WASM. Retrieved from https://delphinuslab.com/zk-wasm/\\n\\n[9] Valida zkVM Design. Retrieved from https://delendum.xyz/writings/2023-05-10-zkvm-design.html"},{"id":"zkVM-explorations","metadata":{"permalink":"/rlog/zkVM-explorations","source":"@site/rlog/2024-08-27-Zkvm.mdx","title":"Exploring zkVMs: Which Projects Truly Qualify as Zero-Knowledge Virtual Machines?","description":"The blockchain space is rapidly evolving, and with it, new technologies are emerging that promise enhanced privacy, scalability, and security.","date":"2024-08-27T12:00:00.000Z","formattedDate":"August 27, 2024","tags":[],"readingTime":23.53,"hasTruncateMarker":true,"authors":[{"name":"Moudy","github":"moudyellaz","key":"moudy"}],"frontMatter":{"title":"Exploring zkVMs: Which Projects Truly Qualify as Zero-Knowledge Virtual Machines?","date":"2024-08-27T12:00:00.000Z","authors":"moudy","published":true,"slug":"zkVM-explorations","categories":"research","discuss":"https://forum.vac.dev/t/exploring-zkvms-which-projects-truly-qualify-as-zero-knowledge-virtual-machines/317","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"zkVM Testing Report: Evaluating Zero-Knowledge Virtual Machines for Nescience","permalink":"/rlog/zkVM-testing"},"nextItem":{"title":"Nescience: A User-Centric State-Separation Architecture","permalink":"/rlog/Nescience-state-separation-architecture"}},"content":"\x3c!--truncate--\x3e\\n\\n\\n# Introduction\\n\\nThe blockchain space is rapidly evolving, and with it, new technologies are emerging that promise enhanced privacy, scalability, and security. \\nAs decentralized systems grow in complexity and usage, the need for secure and private computation has never been greater.\\nZero-knowledge virtual machines (zkVMs) are one such innovation, allowing for computations to be proven correct without revealing the underlying data. \\nZkVMs have enormous implications for privacy-preserving applications, decentralized finance (DeFi), and other blockchain-based use cases.\\nHowever,  as the term \\"zkVM\\" becomes more widely adopted, it is critical to distinguish between projects that truly satisfy the stringent requirements of a zkVM and those that do not.\\n\\n\\n# What is a zkVM?\\nA zkVM is a virtual machine that combines the principles of cryptographic proof generation and privacy preservation with the computational model \\nof traditional virtual machines. Essentially, a zkVM enables the execution of arbitrary programs while generating cryptographic proofs\u2014specifically, zero-knowledge proofs (ZKPs)\u2014that \\ncan verify the correctness of these computations without revealing any sensitive information. This ensures that computations can be trusted while protecting the privacy of the data involved. \\nThe key characteristics of a zkVM include:\\n\\n- Proof generation: The ability to produce ZKPs that verify the correct execution of programs. There are several types of cryptographic techniques used in zkVMs to \\ngenerate these proofs, such as zk-SNARKs, zk-STARKs, and recursive proofs. A zkVM\u2019s ability to generate these proofs determines how effectively it can ensure the integrity of computations \\nin a privacy-preserving manner.\\n- Privacy preservation: The system must maintain privacy, ensuring that only the proof is revealed, not the underlying computation or data. Privacy-preserving zkVMs allow users to maintain \\nconfidentiality without compromising the security or verifiability of their operations. However, not all zkVMs achieve the same level of privacy. Some may focus more on proof generation \\nand scalability while deprioritizing privacy features, which can limit their use in certain privacy-sensitive applications.\\n- Scalability and performance: zkVMs should offer scalable and efficient computation, leveraging advanced cryptographic techniques like zk-SNARKs, zk-STARKs, or recursive proofs. \\nA zkVM\'s performance must also be measured in terms of latency (time to generate and verify a proof) and throughput (number of computations processed within a certain time frame).\\n- Verifiable computation: The zkVM should be able to prove the execution of arbitrary programs in a secure and verifiable manner. Verifiable computation ensures that zkVMs can be deployed \\nacross a wide range of applications, from DeFi to private data-sharing platforms and more.\\n\\n# Why zkVMs matter\\n\\nThe rise of zkVMs is a crucial development for the future of blockchain and decentralized technologies. As more systems require the ability to scale while maintaining privacy and trust, \\nzkVMs provide a powerful solution. They offer the potential to reshape the way decentralized applications (dapps) handle sensitive information, enabling them to be both efficient and private.\\n\\nIt is essential to distinguish between projects that fully realize the potential of zkVMs and those that do not. In the remainder of this post, we evaluate several zkVM projects, analyzing \\nwhether they satisfy the criteria for being classified as zkVMs based on our research.\\n\\n# Our methodology\\n\\nWe analyzed each project\u2019s documentation, source code, and available benchmarks to determine whether they meet the definition of a zkVM. \\nOur criteria focus on the key capabilities of zkVMs\u2014proof generation, privacy, scalability, and integration with existing systems.\\n\\n# ZkVM project analysis\\n\\n## 1. [SP1]\\n- Overview: SP1 [[1](https://blog.succinct.xyz/introducing-sp1/)] is a developer-friendly zkVM designed to enable ZKP execution for LLVM-based languages like C, C++, Rust, and others. It supports a RISC-V-like instruction set architecture (ISA), \\nwhich makes it compatible with various programming languages compiled through LLVM.\\n- Main focus: The main focus of SP1 is scalability, open-source contributions, and accessibility for developers. It prioritizes performance over privacy, \\nmaking it a good fit for environments where privacy isn\'t the primary concern.\\n- Privacy: Not explicitly mentioned, making it less suitable for privacy-preserving applications.\\n- Performance: SP1 has demonstrated up to 5.4x better performance than similar zkVMs like RISC0 for specific computations such as Fibonacci sequence generation.\\n- Integration: SP1 is highly adaptable for rollups, light client verifiers, oracles, and even web2 projects like verifying the originality of images.\\n- Conclusion: Yes, SP1 is a zkVM, but it does not prioritize zero-knowledge privacy, focusing more on scalability and performance.\\n\\n\\n## 2. [Nexus]\\n- Overview: Nexus [[2](https://docs.nexus.xyz/)] is a highly modular zkVM designed to process up to a trillion CPU cycles per second. It relies on RISC-V instructions for computation, making it extensible and scalable. \\nHowever, it currently lacks full ZKP capabilities due to its use of Spartan proofs.\\n- Main focus: Nexus focuses on high performance and scalability, aiming to create an efficient execution environment for computationally intensive tasks.\\n- Privacy: Although zero-knowledge privacy isn\'t the primary feature of Nexus, the project hints at potential privacy enhancements in the future.\\n- Performance: Nexus has a high theoretical throughput, but it has yet to demonstrate benchmarks on zero-knowledge privacy.\\n- Integration: Nexus is a good fit for high-performance environments that do not necessarily require full privacy.\\n- Conclusion: Yes, Nexus qualifies as a zkVM in terms of scalability and proof generation, but it does not yet achieve full zero-knowledge privacy.\\n\\n## 3. [RISC0]\\n- Overview: Risc0 [[3]( https://www.risczero.com/zkvm)] is a general-purpose zkVM with strong developer support. It allows for the execution of Rust and C code on a RISC-V virtual machine \\nand generates zk-SNARK and zk-STARK proofs for these computations.\\n- Main focus: Risc0 is focused on ease of use for developers by abstracting away the complexities of circuit generation, making it accessible for a wide range of use cases.\\n- Privacy:  Full zero-knowledge privacy is supported via zk-SNARK and zk-STARK proofs, with Groth16 used for constant-size proof generation.\\n- Performance: Risc0 offers strong benchmarks across different hardware setups, making it one of the most versatile zkVMs in terms of performance and scalability.\\n- Integration: Risc0 integrates with several ecosystems, including Ethereum, and supports verifiable execution of Rust-based programs.\\n- Conclusion: Yes, Risc0 qualifies as a zkVM, offering a balance of developer usability, scalability, and privacy.\\n\\n## 4. [Powdr]\\n- Overview: Powdr [[4](https://docs.powdr.org/)] is a toolkit for creating custom zkVMs. It allows developers to select from various front-end and back-end components to create zkVMs tailored to specific needs.\\n- Main focus: Powdr is focused on providing a modular architecture for zkVM creation. It enables flexibility by allowing the combination of different ZK-proof backends like Halo2 or Valida.\\n- Privacy: Powdr itself does not generate ZKPs, but it facilitates the creation of zkVMs that do.\\n- Performance: The performance depends on the components chosen by the developer, as Powdr itself is more of a framework.\\n- Integration: Powdr is highly customizable and can integrate with existing zkVM frameworks to extend their capabilities.\\n- Conclusion: No, Powdr is not a zkVM itself, but it is a powerful tool for building customized zkVMs with different privacy and performance needs.\\n\\n## 5. [ZkMIPS]\\n- Overview: ZkMIPS [[5](https://docs.zkm.io/zkm-architecture)] uses zk-STARKs to ensure privacy during computation, ensuring that private inputs are preserved while still proving correctness.\\n- Performance: ZkMIPS is built for scalability, though explicit benchmarks are not widely published.\\n- Integration: ZkMIPS can be integrated into systems that rely on MIPS architecture, making it versatile for legacy codebases that require privacy.\\n- Conclusion: Yes, zkMIPS is a zkVM focused on scalability and privacy for MIPS-based architectures.\\n\\n## 6. [Valida]\\n- Overview: Valida [[6]( https://delendum.xyz/writings/2023-05-10-zkvm-design.html)] is a performance-oriented zkVM that generates proofs for programs using a custom ISA designed to optimize zkVM implementation. \\nIt uses Plonky3 for its proof system.\\n- Main focus: Valida is centered around optimizing prover performance and extensibility, making it a valuable tool for generating proofs efficiently.\\n- Privacy: While Valida is focused on performance, it does not prioritize zero-knowledge privacy as much as other zkVMs.\\n- Performance: Valida has benchmarks indicating its performance advantages in proving computations quickly, particularly through parallel processing.\\n- Integration: Valida is specialized and may not integrate as seamlessly into general-purpose systems, as it is optimized for performance over broad applicability.\\n- Conclusion: Yes, Valida qualifies as a zkVM based on proof generation, but its lack of focus on privacy makes it less suitable for privacy-first use cases.\\n\\n## 7. [Jolt]\\n- Overview: Jolt [[7](https://a16zcrypto.com/posts/article/building-jolt/)] is a zkVM built to optimize prover performance using a modified Hyrax polynomial commitment system. It relies on RISC-V instructions for computation \\nbut falls short of full zero-knowledge capabilities.\\n- Main focus: Jolt\'s main goal is to optimize the speed of proving program execution, making it suitable for high-performance applications where privacy isn\'t the primary concern.\\n- Privacy: Jolt does not fully achieve zero-knowledge privacy due to the choice of polynomial commitment schemes.\\n- Performance: Jolt offers strong performance, with benchmarks highlighting its ability to process proofs efficiently.\\n- Integration: Jolt can be integrated with systems that prioritize speed over privacy, particularly where rapid proof generation is essential.\\n- Conclusion: Yes, Jolt qualifies as a zkVM based on proof generation, though it does not provide full zero-knowledge privacy.\\n\\n## 8. [ZkWASM]\\n- Overview: ZkWASM [[8](https://delphinuslab.com/zk-wasm/)] is a zkVM designed to execute WebAssembly (WASM) code in a privacy-preserving and scalable manner. It uses zk-SNARKs to prove the correctness of WASM \\nprogram execution while ensuring privacy.\\n- Main focus: ZkWASM focuses on scalability and privacy for WebAssembly, making it ideal for dapps that require verifiable computation without compromising privacy.\\n- Privacy: Full zero-knowledge privacy is provided through zk-SNARKs, ensuring that the execution of WASM programs remains confidential.\\n- Performance: ZkWASM is optimized for running WASM programs efficiently, with offchain computation and onchain verification to enhance performance.\\n- Integration: ZkWASM is ideal for dapps, particularly those that use WebAssembly and require verifiable execution.\\n- Conclusion: Yes, zkWASM qualifies as a zkVM, providing strong privacy, scalability, and verifiable execution for WebAssembly code.\\n\\n## 9. [Aleo]\\n- Overview: Aleo\'s [[9](https://aleo.org/blog/)] snarkVM converts code into Aleo instructions, which are then compiled into bytecode executable on its zkVM. Aleo emphasizes building private, scalable dapps.\\n- Main focus: Aleo prioritizes privacy and scalability for dapps, providing a robust framework for developers building private dapps.\\n- Privacy: Aleo offers full privacy through zk-SNARK proofs, making it suitable for building fully private applications.\\n- Performance: Aleo focuses on scalability through efficient proof systems, though detailed performance benchmarks are not widely available.\\n- Integration: Aleo is built for privacy-first dapps and integrates with other zkVM-based systems.\\n- Conclusion: Yes, Aleo qualifies as a zkVM, offering a comprehensive solution for private and scalable dapps.\\n\\n## 10. [Ola]\\n- Overview: Ola [[10](https://github.com/Sin7Y/olavm-whitepaper-v2/tree/master)] is a ZK-friendly, high-performance layer-2 (L2) rollup platform that is still under development. It is designed to execute computations offchain while generating \\nvalidity proofs for these computations, ensuring that they are correctly executed without compromising security. \\n- Privacy: Ola does not specifically prioritize privacy in the same way that zkVMs do. While it leverages ZKPs for scalability, its focus is on proving the correctness of \\ntransactions and computations rather than ensuring that the data remains private.\\n- Performance:  Ola is designed to achieve high performance, particularly in terms of transaction throughput.\\n- Integration: Ola is designed to be interoperable with various layer-1 blockchains. The platform supports a hybrid ZK-rollup architecture and is expected to include bridges for cross-chain \\ninteroperability, enabling assets and data to move seamlessly between the layer-1 blockchain and the Ola rollup.\\n- Conclusion: No, Ola is not a zkVM. While it leverages ZKPs (in the form of ZK-rollups) to ensure the validity of offchain computations, its primary focus is on scalability and performance \\nrather than privacy or verifiable execution of arbitrary programs. Ola is more accurately described as a ZK-rollup platform aimed at improving transaction throughput and reducing transaction costs on \\nlayer-1 blockchains. \\n\\n## 11. [Miden]\\n- Overview: Miden zkVM [[11](https://0xpolygonmiden.github.io/miden-vm/intro/main.html)] is a zk-STARK-based virtual machine that converts code into Miden VM instructions and proves the execution of these instructions with zero-knowledge privacy.\\n- Main focus: Miden focuses on scalability and privacy for ZK-rollups, offering efficient proof generation for dapps.\\n- Privacy: Miden ensures privacy for transactions and programs via zk-STARK proofs, making it suitable for private dapps.\\n- Performance: Miden is optimized for scalability, with benchmarks showing its ability to handle up to 1,000 transactions per second (TPS).\\n- Integration: Miden integrates well with ZK-rollup solutions, making it ideal for L2 scaling solutions on blockchains like Ethereum.\\n- Conclusion: Yes, Miden qualifies as a zkVM, providing strong privacy and scalability for dapps and ZK-rollups.\\n\\n## 12. [ZkOS]\\n- Overview: ZkOS [[12](https://osblog.stephenmarz.com/index.html)] is a verifiable operating system focused on running zkApps in a decentralized manner. It is built on the RISC-V architecture and aims to create \\na world computer where all untrusted executions can be verified.\\n- Main focus: ZkOS is primarily designed to offer a proof-of-concept operating system where all executions can be verified in a trustless manner. \\nHowever, its focus is more on the infrastructure for verifiable applications rather than being a traditional zkVM.\\n- Privacy: ZkOS does not focus on privacy guarantees such as those found in zkVMs that generate ZKPs.\\n- Performance: ZkOS focuses on the efficient execution of dapps, but performance benchmarks specific to ZKP generation are not provided.\\n- Integration: ZkOS supports the execution of zkApps, but it is more of a verifiable operating system rather than a zkVM, making it distinct in its functionality.\\n- Conclusion: No, zkOS is not a zkVM. It is a verifiable operating system focused on the infrastructure to support zkApps but does not directly generate ZKPs or focus on privacy preservation.\\n\\n## 13. [Triton]\\n- Overview: Triton [[13](https://triton-vm.org/spec/)] is a domain-specific language (DSL) and compiler designed primarily for high-performance GPU kernels, particularly those used in deep learning applications. \\n- Main focus: The primary goal of Triton is to optimize computation for machine learning and GPU workloads. It is focused on enhancing performance and efficiency in processing data \\nrather than on ZKPs or verifiable computation.\\n- Privacy: Triton does not provide ZKPs or privacy features typically associated with zkVMs. Its focus is on high-performance computation rather than cryptographic verifiability.\\n- Performance: Triton is highly optimized for GPU execution, offering significant improvements in performance for computationally intensive tasks such as those found in deep learning.\\n- Integration: Triton is integrated with GPU-based computation environments and is highly specialized for optimizing low-level operations on hardware rather \\nthan being a general-purpose virtual machine.\\n- Conclusion: No, Triton is not a zkVM. It is a specialized tool for optimizing GPU workloads, focusing on performance rather than privacy or ZKPs.\\n\\n## 14. [Cairo]\\n- Overview: Cairo zkVM [[14](https://github.com/lambdaclass/cairo-vm/blob/main/docs/python_vm/README.md)] uses a custom language that compiles to an optimized STARK-based proof system, ensuring verifiable computation. It is primarily used in systems like Starknet.\\n- Main focus: Cairo focuses on scalability and performance, using zk-STARK proofs to ensure the verifiable and secure execution of programs.\\n- Privacy: Cairo provides privacy through zk-STARKs, but it focuses more on scalability and performance than privacy-first use cases.\\n- Performance: Cairo is highly optimized for performance, making it well-suited for scalable applications on Starknet.\\n- Integration: Cairo integrates deeply with systems like Starknet, supporting verifiable computation in a highly scalable and efficient manner.\\n- Conclusion: Yes, Cairo qualifies as a zkVM, focusing on performance and verifiable execution while being ZK-friendly.\\n\\n## 15. [SnarkOS]\\n- Overview: SnarkOS [[15](https://aleo.org/post/aleo-completes-security-audits-of-snarkos-and-snarkvm/)] is a decentralized operating system designed to power Aleo\'s network, enabling secure and private dapps. \\nIt manages transactions and consensus, making it a critical infrastructure component for Aleo\'s zkVM-based ecosystem.\\n- Main focus: SnarkOS primarily focuses on securing Aleo\'s network through consensus mechanisms and privacy-preserving transactions rather than acting as a \\nzkVM that directly proves program execution.\\n- Privacy: SnarkOS supports zero-knowledge privacy through its integration with Aleo\'s zkVM, but the operating system itself does not generate ZKPs for arbitrary computations.\\n- Performance: SnarkOS is optimized for managing dapps on the Aleo network and handling private transactions, but its focus is more on infrastructure \\nand consensus than on proof generation.\\n- Integration: SnarkOS integrates seamlessly with Aleo\'s zkVM to support private dapps and transactions, but its primary role is as a consensus layer.\\n- Conclusion: No, SnarkOS is not a zkVM. It serves as an operating system for Aleo\'s decentralized network, focusing on privacy and consensus rather than on generating ZKPs for computations.\\n\\n## 16. [Lurk]\\n- Overview: Lurk [[16](https://github.com/lurk-lab)] is a Turing-complete programming language designed for recursive zk-SNARKs. It focuses on enabling developers to build complex, \\nrecursive ZKPs efficiently through a custom language tailored for verifiable computation.\\n- Main focus: Lurk is centered around recursive proof generation rather than serving as a traditional virtual machine. Its purpose is to facilitate the creation of complex zk-SNARK-based proofs, \\nmaking it a specialized tool for cryptographic proofs rather than general-purpose computation.\\n- Privacy: Lurk is built for generating zk-SNARKs, which inherently provide privacy. However, Lurk itself is a language and not a zkVM that executes arbitrary programs and generates ZKPs for them.\\n- Performance: Lurk is optimized for recursive zk-SNARK generation, but specific performance metrics are tied to its proof-generation capabilities rather than traditional execution environments.\\n- Integration: Lurk is specialized for zk-SNARKs and may not easily integrate with other general-purpose systems, as it focuses on specific cryptographic tasks.\\n- Conclusion: No, Lurk is not a zkVM. It is a programming language designed for recursive zk-SNARKs and focuses on proof generation rather than program execution in a virtual machine environment.\\n\\n## 17. [Piecrust]\\n- Overview: Piecrust [[17](https://docs.rs/piecrust/latest/piecrust/)] is a WASM-based zkVM designed to run on the Dusk Network. It supports concurrent execution and focuses on providing privacy and scalability for smart contracts.\\n- Main focus: Piecrust is designed to provide private and efficient execution of smart contracts through the use of ZKPs.\\n- Privacy: Piecrust supports ZK-friendly computations and enhances privacy through cryptographic primitives such as Merkle trees.\\n- Performance: Piecrust is designed to be scalable and concurrent, allowing multiple sessions to run simultaneously, which improves overall performance.\\n- Integration: Piecrust integrates with the Dusk Network and supports private smart contracts, making it ideal for dapps.\\n- Conclusion: Yes, Piecrust qualifies as a zkVM, offering scalability, privacy, and support for succinct proof generation.\\n\\n## 18. [Ceno]\\n- Overview: Ceno [[18](https://eprint.iacr.org/2024/387)] is a zkVM that provides a theoretical framework for reducing proving time by grouping common portions of code together. It uses recursive proofs to enhance prover efficiency.\\n- Main focus: Ceno aims to optimize prover performance through recursive proofs, making it a powerful tool for handling complex computations efficiently.\\n- Privacy: Ceno supports zero-knowledge privacy through recursive proofs and is designed to handle large-scale computations securely.\\n- Performance: Ceno\'s recursive proof framework ensures that it can efficiently prove the execution of programs, reducing the time required for proof generation.\\n- Integration: Ceno can be integrated into systems that require high efficiency and privacy, particularly those handling complex, repeated computations.\\n- Conclusion: Yes, Ceno qualifies as a zkVM, providing efficient and private computation through the use of recursive proofs.\\n\\n## 19. [Stellar]\\n- Overview: Stellar [[19](https://stellar.org/blog/developers/zkvm-a-new-design-for-fast-confidential-smart-contracts)] is a decentralized protocol designed to facilitate cross-border transactions between digital and fiat currencies.\\n- Main focus: Stellar\'s primary goal is to improve financial transactions by enabling decentralized, low-cost currency transfers. It does not aim to provide ZKPs or run verifiable computations \\nlike a zkVM.\\n- Privacy: Stellar focuses on confidentiality and security for financial transactions, but it does not employ ZKPs in the way zkVMs do for verifying computation without revealing data.\\n- Performance: Stellar prioritizes the performance of financial transactions, ensuring low latency and high throughput across its decentralized network. \\nHowever, this performance focus is specific to transactions rather than general-purpose program execution.\\n- Integration: Stellar is designed for integration with financial systems, enabling currency conversions and transfers, but it is not built for executing smart contracts or verifiable computations.\\n- Conclusion: No, Stellar is not a zkVM. It is a decentralized financial protocol focused on facilitating cross-border payments rather than verifiable or privacy-preserving computation.\\n\\n## 20. [NovaNet]\\n- Overview: NovaNet [[20](https://www.novanet.xyz/blog)] is an open peer-to-peer network that aims to build upon concepts of non-uniform incremental verifiable computation. \\n- Main focus: NovaNet\'s focus is on peer-to-peer networking and decentralized computing rather than on proving the execution of programs in a zero-knowledge manner.\\n- Privacy: NovaNet does not provide ZKPs or privacy features typically associated with zkVMs. Its focus is on decentralized networking and computation.\\n- Performance: NovaNet prioritizes efficient decentralized computation but does not focus on privacy or performance benchmarks related to ZKPs.\\n- Integration: NovaNet is built for decentralized networks but is not designed to integrate with systems requiring verifiable computation or ZKP generation.\\n- Conclusion: No, NovaNet is not a zkVM. It is a decentralized peer-to-peer network focused on distributed computing rather than zero-knowledge computation.\\n\\n\\n## 21. [ZkLLVM]\\n- Overview: ZkLLVM [[21](https://github.com/NilFoundation/zkLLVM)] is a compiler that transforms C++ or Rust code into circuits for use in zk-SNARK or zk-STARK systems. Its primary purpose is to bridge high-level programming \\nlanguages with ZKP systems by compiling code into arithmetic circuits that can be used to generate and verify proofs.\\n- Main focus: ZkLLVM focuses on making ZKPs accessible to developers by enabling them to write code in familiar languages (C++, Rust) and then compile that code into ZK circuits.\\n- Privacy: ZkLLVM enables the generation of ZKPs by compiling high-level code into ZK-compatible circuits. It plays a crucial role in privacy-preserving applications but does not act \\nas a zkVM itself.\\n- Performance: ZkLLVM allows for the performance of ZKPs to be closely tied to the complexity of the compiled circuits. The performance depends on the underlying \\nzk-SNARK or zk-STARK system used.\\n- Integration: ZkLLVM integrates with zk-SNARK and zk-STARK proof systems, making it useful for a variety of privacy-focused applications, but it does not serve as a zkVM \\nfor general-purpose computation.\\n- Conclusion: No, zkLLVM is not a zkVM. It is a compiler that transforms high-level code into ZK circuits, enabling ZKPs but not acting as a virtual machine for executing and proving programs.\\n\\n\\n## 22. [ZkMove]\\n- Overview: ZkMove [[22](https://www.zkmove.net/2023-06-20-zkMove-0.2.0-Achieving-Full-Bytecode-Compatibility-with-Move/)] is a zkVM designed to execute smart contracts written in the Move language. It utilizes ZKPs to ensure that the execution of these contracts remains verifiable and secure.\\n- Main focus: ZkMove focuses on privacy and verifiable execution for Move-based smart contracts, providing a framework for ZK-friendly computation.\\n- Privacy: ZkMove ensures that smart contract execution remains private through ZKPs, making it suitable for privacy-preserving applications.\\n- Performance: ZkMove is optimized for verifiable execution, ensuring that contracts can be proven correct while preserving privacy.\\n- Integration: ZkMove integrates well with systems that use the Move language, particularly in environments that require private smart contract execution.\\n- Conclusion: Yes, zkMove qualifies as a zkVM, offering ZK-friendly execution and privacy for smart contracts written in the Move language.\\n\\n## 23. [O1VM]\\n- Overview: O1VM [[23](https://github.com/o1-labs/proof-systems/tree/master/o1vm)] is a general-purpose zkVM developed by o1Labs. It is designed to prove the execution of MIPS programs efficiently through a combination of zk-SNARKs \\nand specialized techniques like folding schemes and RAMLookups.\\n- Main focus: O1VM focuses on scalability and verifiable computation for MIPS-based programs, making it a strong contender for executing and proving complex programs efficiently.\\n- Privacy: O1VM ensures privacy through zk-SNARK proofs, keeping the details of the computation private while proving its correctness.\\n- Performance: O1VM is optimized for handling long execution traces and complex computations, making it highly scalable.\\n- Integration: O1VM integrates well with MIPS-based architectures and systems that require privacy-preserving computation.\\n- Conclusion: Yes, o1VM qualifies as a zkVM, providing privacy, scalability, and strong proof generation for MIPS programs.\\n\\n\\n# Summary of findings \\n| Project name | ZkVM status | Zero knowledge | Reasoning/comments |\\n|--------------|-------------|----------------|-------------------|\\n| **SP1**      | Yes         | No             | Proves execution of LLVM-based programs but lacks privacy features. |\\n| **Nexus**    | Yes         | No             | Strong proof generation but lacks zero-knowledge privacy due to Spartan. |\\n| **Risc0**    | Yes         | Yes            | Supports full ZKP generation for Rust programs. |\\n| **Powdr**    | No          | Yes            | Toolkit for creating custom zkVMs, not a zkVM itself. |\\n| **ZkMIPS**   | Yes         | Yes            | Supports MIPS-like architecture with full zero-knowledge and proof generation. |\\n| **Valida**   | Yes         | No             | Performance-focused zkVM, lacks privacy guarantees. |\\n| **Jolt**     | Yes         | No             | Performance-focused zkVM, does not achieve zero-knowledge privacy. |\\n| **ZkWASM**   | Yes         | Yes            | Full zero-knowledge and verifiable execution of WebAssembly code. |\\n| **Aleo**     | Yes         | Yes            | Fully private and scalable dapps. |\\n| **Ola**      | No          | No             | Primarily a ZK-rollup platform, not a zkVM, focusing on scalability and performance rather than privacy. |\\n| **Miden**    | Yes         | Yes            | Zk-STARK-based zkVM with strong privacy and scalability. |\\n| **ZkOS**     | No          | No             | Verifiable operating system focused on zkApps, not a zkVM. |\\n| **Triton**   | No          | No             | Optimizes GPU workloads but not designed for ZKPs. |\\n| **Cairo**    | Yes         | ZK-friendly    | Custom Rust-based language with zk-STARK proof generation. |\\n| **SnarkOS**  | No          | Yes            | Decentralized OS for Aleo\'s network, focuses on consensus rather than verifiable computation. |\\n| **Lurk**     | No          | No             | Programming language for recursive zk-SNARKs, not a zkVM. |\\n| **Piecrust** | Yes         | ZK-friendly    | ZkVM with recursive SNARK capabilities, focused on succinct proof generation. |\\n| **Ceno**     | Yes         | Yes            | Theoretical zkVM improving prover efficiency through recursive proofs. |\\n| **Stellar**  | No          | No             | Focuses on cross-border transactions, not ZK-proof generation or verifiable computation. |\\n| **NovaNet**  | No          | No             | Peer-to-peer network focused on distributed computing, not zero-knowledge computation. |\\n| **ZkLLVM**   | No          | Yes, in some cases | Compiler for generating ZK-circuits, not a zkVM. |\\n| **ZkMove**   | Yes         | ZK-friendly    | ZkVM supporting Move language with ZKP execution. |\\n| **O1VM**     | Yes         | Yes            | MIPS-based zkVM with strong privacy, scalability, and proof generation. |\\n\\n# Insights and conclusions\\n\\nOur analysis reveals that many of the projects labeled as zkVMs do meet the core criteria for zkVMs, offering verifiable computation and proof generation \\nas foundational features. However, a number of these projects fall short of delivering full zero-knowledge privacy. Projects like Risc0, Aleo, and Miden stand out as leading zkVM frameworks \\nthat balance proof generation, privacy, and scalability, offering strong platforms for developers seeking to build privacy-preserving applications.\\n\\nConversely, projects like SP1 and Nexus excel in generating verifiable proofs but currently lack comprehensive zero-knowledge privacy mechanisms. These platforms are excellent for \\nscenarios where proof generation and scalability are paramount, but privacy is not a primary concern.\\n\\nAs zkVM technology continues to evolve, we expect to see more projects integrating enhanced privacy-preserving mechanisms while simultaneously improving performance and scalability. \\nThis ongoing development will likely broaden the application of zkVMs across the blockchain ecosystem, particularly in privacy-sensitive sectors such as finance, data security, \\nand decentralized applications.\\n\\nWhat are your thoughts on our zkVM analysis? Do you agree with our findings, or do you know of other zkVM projects that should be on our radar? We would love to hear your insights, questions, \\nor suggestions! Feel free to join the [discussion](https://forum.vac.dev/t/exploring-zkvms-which-projects-truly-qualify-as-zero-knowledge-virtual-machines/317) on our forum.\\n\\n\\n# References\\n\\n[1] Introducing SP1: A performant, 100% open-source, contributor-friendly zkVM. Retrieved from https://blog.succinct.xyz/introducing-sp1/\\n\\n[2] The Nexus 2.0 zkVM. Retrieved from https://docs.nexus.xyz/\\n\\n[3] The first general purpose zkVM. Retrieved from https://www.risczero.com/zkvm\\n\\n[4] Powdr. Retrieved from https://docs.powdr.org/\\n\\n[5] ZKM Architecture. Retrieved from https://docs.zkm.io/zkm-architecture\\n\\n[6] Valida zkVM Design. Retrieved from https://delendum.xyz/writings/2023-05-10-zkvm-design.html\\n\\n[7] Building Jolt: A fast, easy-to-use zkVM. Retrieved from https://a16zcrypto.com/posts/article/building-jolt/\\n\\n[8] ZK-WASM. Retrieved from https://delphinuslab.com/zk-wasm/\\n\\n[9] Aleo. Retrieved from https://aleo.org/blog/\\n\\n[10] OlaVM Whitepaper V2. Retrieved from https://github.com/Sin7Y/olavm-whitepaper-v2/tree/master\\n\\n[11] Polygon Miden VM. Retrieved from https://0xpolygonmiden.github.io/miden-vm/intro/main.html\\n\\n[12] The Adventures of OS: Making a RISC-V Operating System using Rust. Retrieved from https://osblog.stephenmarz.com/index.html\\n\\n[13] Triton VM. Retrieved from https://triton-vm.org/spec/\\n\\n[14] How does the original Cairo VM work?. Retrieved from https://github.com/lambdaclass/cairo-vm/blob/main/docs/python_vm/README.md\\n\\n[15] Aleo completes security audits of snarkOS & snarkVM. Retrieved from https://aleo.org/post/aleo-completes-security-audits-of-snarkos-and-snarkvm/\\n\\n[16] Lurk zkVM. Retrieved from https://github.com/lurk-lab\\n\\n[17] Piecrust VM. Retrieved from https://docs.rs/piecrust/latest/piecrust/\\n\\n[18] Ceno: Non-uniform, Segment and Parallel Zero-knowledge Virtual Machine. Retrieved from https://eprint.iacr.org/2024/387\\n\\n[19] ZkVM: a new design for fast, confidential smart contracts. Retrieved from https://stellar.org/blog/developers/zkvm-a-new-design-for-fast-confidential-smart-contracts\\n\\n[20] Novanet. Retrieved from https://www.novanet.xyz/blog\\n\\n[21] ZKLLVM. Retrieved from https://github.com/NilFoundation/zkLLVM\\n\\n[22] zkMove 0.2.0 - Achieving Full Bytecode Compatibility with Move. Retrieved from https://www.zkmove.net/2023-06-20-zkMove-0.2.0-Achieving-Full-Bytecode-Compatibility-with-Move/\\n\\n[23] O1VM. Retrieved from https://github.com/o1-labs/proof-systems/tree/master/o1vm"},{"id":"Nescience-state-separation-architecture","metadata":{"permalink":"/rlog/Nescience-state-separation-architecture","source":"@site/rlog/2024-08-23-state-separation.mdx","title":"Nescience: A User-Centric State-Separation Architecture","description":"Nescience: A user-centric state-separation architecture.","date":"2024-08-23T12:00:00.000Z","formattedDate":"August 23, 2024","tags":[],"readingTime":86.535,"hasTruncateMarker":true,"authors":[{"name":"Moudy","github":"moudyellaz","key":"moudy"}],"frontMatter":{"title":"Nescience: A User-Centric State-Separation Architecture","date":"2024-08-23T12:00:00.000Z","authors":"moudy","published":true,"slug":"Nescience-state-separation-architecture","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Exploring zkVMs: Which Projects Truly Qualify as Zero-Knowledge Virtual Machines?","permalink":"/rlog/zkVM-explorations"},"nextItem":{"title":"Vac 101: Membership with Bloom Filters and Cuckoo Filters","permalink":"/rlog/vac101-membership-with-bloom-filters-and-cuckoo-filters"}},"content":"Nescience: A user-centric state-separation architecture.\\n\\n\x3c!--truncate--\x3e\\n\\n_Disclaimer: This content is a work in progress. Some components may be updated, changed, or expanded as new research findings become available._\\n\\n# A. Introduction\\n\\nIn blockchain applications, privacy settings are typically predefined by developers, leaving users with limited control. This traditional, \\none-size-fits-all approach often leads to inefficiencies and potential privacy concerns as it fails to cater to the diverse needs of individual users. \\nThe Nescience state-separation architecture (NSSA) aims to address these issues by shifting privacy control from developers to users. NSSA introduces a flexible, \\nuser-centric approach that allows for customized privacy settings to better meet individual needs. This blog post will delve into the details of NSSA, \\nincluding its different execution types, cryptographic foundations, and unique challenges.\\n\\n## Introducing NSSA: A user-centric approach\\n\\nNSSA gives users control over their privacy settings by introducing _shielded_ (which creates a layer of privacy for the outputs, and only the necessary details are shared) \\nand _deshielded_ (which reveal private details, making them publicly visible) execution types in addition to the traditional public and private modes. This flexibility allows \\nusers to customize their privacy settings to match their unique needs, whether they require high levels of confidentiality or more transparency. In NSSA, the system is divided \\ninto two states: public and private. The public state uses an account-based model while the private state employs a UTXO-based (unspent transaction output) model. Private executions within NSSA utilize \\nUTXO exchanges, ensuring that transaction details remain confidential. The sequencer verifies these exchanges without accessing specific details, enhancing privacy by unlinking \\nsender and receiver identities. Zero-knowledge proofs (ZKPs) allow users to prove transaction validity without revealing data, maintaining the integrity and confidentiality of \\nprivate transactions. UTXOs contain assets such as balances, NFTs, or private storage data, and are stored in plaintext within Sparse Merkle trees (SMTs) in the private state and \\nas hashes in the public state. This dual-storage approach keeps UTXO details confidential while allowing public verification through hashes, achieving a balance between privacy and transparency.\\n\\n\\nImplementing NSSA introduces unique challenges, particularly in cryptographic implementation and maintaining the integrity of private executions. These challenges are addressed \\nthrough various solutions such as ZKPs, which ensure transaction validity without compromising privacy, and the dual-storage approach, which maintains confidentiality while enabling \\npublic verification. By allowing users to customize their privacy settings, NSSA enhances user experience and promotes wider adoption of private execution platforms. As we move towards \\na future where user-empowered privacy control is crucial, NSSA provides a flexible and user-centric solution that meets the diverse needs of blockchain users.\\n\\n## Why NSSA differs from other hybrid execution platforms\\n\\nIn many existing hybrid execution platforms, privacy settings are predefined by developers, often applying a one-size-fits-all approach that does not accommodate the \\ndiverse privacy needs of users. These platforms blend public and private states, but control over privacy remains with the application developers. \\nWhile this approach is straightforward for developers (who bear the responsibility for any potential privacy leaks), it leaves users with no control over their own privacy settings. \\nThis rigidity becomes problematic as user needs evolve over time, or as new regulations necessitate changes to privacy configurations. In such cases, \\nupdates to decentralized applications are required to adjust privacy settings, which can disrupt the user experience and create friction.\\n\\nNSSA addresses these limitations by introducing a groundbreaking concept: **selective privacy**. Unlike traditional platforms where privacy \\nis static and determined by developers, selective privacy empowers users to dynamically choose their own privacy levels based on their unique needs and sensitivity. \\nThis flexibility is critical in a decentralized ecosystem where the diversity of users and use cases demands a more adaptable privacy solution.\\n\\nIn the NSSA model, users have the autonomy to select how they interact with decentralized applications (dapps) by choosing from four types of transaction executions: **public**, \\n**private**, **shielded**, and **deshielded**. This model allows users to tailor their privacy settings on a per-transaction basis, selecting the most appropriate execution type for each \\nspecific interaction. For instance, a user concerned about data confidentiality might opt for a fully private transaction while another user, wary of privacy but seeking transparency, \\nmight choose a public execution.\\n\\nWhile selective privacy may appear complex, especially for users who are not technically inclined, Nescience mitigates this by allowing the community or developers to \\nestablish best practices and recommended approaches. These guidelines provide users with an informed starting point, and over time, users can adjust their privacy \\nsettings as their preferences and trust in the platform evolve. Importantly, selective privacy gives users the right to alter their privacy level at any point in the future, \\nensuring that their privacy settings remain aligned with their needs as they change.\\n\\nThis approach not only empowers users but also facilitates greater adoption of dapps. Users who are skeptical about privacy concerns can initially engage with transparent \\ntransactions and gradually shift towards more private executions as they gain confidence in the system and vice versa for users who start with privacy but later find transparency \\nbeneficial for certain transactions. In this way, selective privacy bridges the gap between privacy and transparency, allowing for an optimal balance to emerge from the community\u2019s \\ncollective preferences.\\n\\nTo liken this to open-source projects: in traditional systems, developers fix privacy rules much like immutable code\u2014users must comply with these fixed settings. \\nIn contrast, with selective privacy, the rules are malleable and shaped by the users\u2019 preferences, enabling the community to find the ideal balance between privacy and efficiency over time.\\n\\nNSSA is distinct from traditional zero-knowledge (ZK) rollups in several key ways. One of the unique features of NSSA is its **public execution type**, which does not \\nrequire ZKPs or a zero-knowledge virtual machine (zkVM). This provides a significant advantage in terms of scalability and efficiency as users can choose public executions for \\ntransactions that do not require enhanced privacy, avoiding the overhead associated with ZKP generation and verification.\\n\\nMoreover, NSSA introduces two additional execution types\u2014**shielded and deshielded**\u2014which further distinguish it from traditional privacy-preserving rollups. \\nThese execution types allow for more nuanced control over privacy, giving users the ability to shield certain aspects of a transaction while deshielding others. \\nThis flexibility sets NSSA apart as a more adaptable and user-centric platform, catering to a wide range of privacy needs without imposing a one-size-fits-all solution.\\n\\nBy combining selective privacy with a flexible execution model, NSSA offers a more robust and adaptable framework for decentralized applications, \\nensuring that users maintain control over their privacy while benefiting from the security and efficiency of blockchain technology.\\n\\n## How Nescience state-separation architecture can be used\\n\\nNSSA offers a flexible, privacy-preserving add-on that can be applied to existing dapps. \\nOne of the emerging trends in the blockchain space is that each dapp is expected to have its own rollup for efficiency, and it is estimated that Ethereum could see \\nthe deployment of different rollups in the near future. A key question arises: how many of these rollups will incorporate privacy? For dapp developers who want to offer flexible, \\nuser-centric privacy features, NSSA provides a solution through selective privacy.\\n\\n### Use case: Adding privacy to existing dapps\\n\\nConsider a dapp running on a transparent network that offers no inherent privacy to its users. Converting this dapp to a privacy-preserving architecture from scratch would \\nrequire significant effort, restructuring, and a deep understanding of cryptographic frameworks. However, with NSSA, the dapp does not need to undergo extensive changes. \\nInstead, the **Nescience state-separation model** can be deployed as an **add-on**, offering selective privacy as an option for the dapp\u2019s users.\\n\\nThis allows the dapp to retain its existing functionality while providing users with a choice between the traditional, transparent version and a new version with selective privacy features. \\nWith NSSA, the privacy settings are flexible, meaning users can tailor their level of privacy according to their individual needs while the dapp operates on its current infrastructure. \\nThis contrasts sharply with the typical approach, where dapps are either entirely transparent or fully private, with no flexibility for users to select their own privacy preferences.\\n\\n### Key advantage: Decoupling from the host chain\\n\\nA key feature of NSSA is that it operates independently of the privacy characteristics of the host blockchain. Whether the host chain is fully transparent or fully private, \\nthe Nescience state-separation architecture can be deployed on top of it, offering users the ability to choose their own privacy settings. \\nThis decoupling from the host chain\u2019s inherent privacy model is critical as it allows users to benefit from selective privacy even in environments that were not originally designed to offer it.\\n\\nIn **fully private chains**, NSSA allows users to selectively reveal transaction details when compliance with regulations or other requirements is necessary. \\nIn **fully transparent chains**, NSSA allows users to maintain privacy for specific transactions, offering flexibility that would not otherwise be possible.\\n\\n### Conclusion\\n\\nNSSA provides a powerful tool for dapp developers who want to offer **selective privacy** to their users without the need for a complete overhaul of their existing systems. \\nBy deploying NSSA as an add-on, dapps can give users the ability to choose their own privacy settings whether they are operating on \\ntransparent or private blockchains. This flexibility makes NSSA a valuable option for any dapp looking to provide enhanced privacy options while maintaining efficiency and ease of use.\\n\\n# B. Design\\nIn this section, we will delve into the core design components of the Nescience state-separation architecture, covering its key structural elements and the mechanisms \\nthat drive its functionality. We will explore the following topics:\\n\\n1. **Architecture\'s components**: An in-depth look at the foundational building blocks of NSSA, including the public and private states, UTXO structures, zkVM, and smart contracts. \\nThese components work together to facilitate secure, flexible, and scalable transactions within the architecture.\\n\\n2. **General execution overview**: We will outline the overall flow of transaction execution within NSSA, describing how users interact with the system and how the architecture \\nsupports various types of executions\u2014public, private, shielded, and deshielded\u2014while preserving privacy and efficiency.\\n\\n3. **Execution processes and UTXO management**: This section will focus on the lifecycle of UTXOs within the architecture, from their generation to consumption. \\nWe will also cover the processes involved in managing UTXOs, including proof generation, state transitions, and ensuring transaction validity.\\n\\nThese topics will provide a comprehensive understanding of how NSSA enables flexible and secure interactions within dapps.\\n\\n## 1. Architecture\'s components\\n---\\nNSSA introduces an advanced prototype execution framework designed to enhance privacy and security in blockchain applications. \\nThis framework integrates several essential components: the public state, private state, zkVM, various execution types, Nescience users, and smart contracts.\\n\\n### a) Public state\\n---\\nThe public state in the NSSA is a fundamental component designed to hold all publicly accessible information within \\nthe blockchain network. This state is organized as a single Merkle tree structure, a sophisticated data structure that ensures efficient and secure data verification. \\nThe public state includes critical information such as user balances and the public storage data of smart contracts.\\n\\nIn an account-based model, the public state operates by storing each account or smart contract\'s public data as individual leaf nodes within the Merkle tree. \\nWhen transactions occur, they directly modify the state by updating these leaf nodes. This direct modification ensures that the most current state of the network \\nis always reflected accurately.\\n\\nThe Merkle tree structure is essential for maintaining data integrity. Each leaf node contains a hash of a data block, and each non-leaf node contains the \\nhash of its child nodes. This hierarchical arrangement means that any change in the data will result in a change in the corresponding hash, making it easy to detect \\nany tampering. The root hash, or Merkle root, is stored on the blockchain, providing a cryptographic guarantee of the data\'s integrity. This root hash serves as a single, \\nconcise representation of the entire state, enabling quick and reliable verification by any network participant.\\n\\nTransparency is a key feature of the public state. All data stored within this state is openly accessible and verifiable by any participant in the network. \\nThis openness ensures that all transactions and state changes are visible and auditable, fostering trust and accountability. For example, user balances are\\npublicly viewable, which helps ensure transparency and trust in the system. Similarly, public smart contract storage can be accessed and verified by anyone, \\nmaking it suitable for applications that require public scrutiny and auditability, such as public record updates and some financial transactions.\\n\\nThe workflow of managing the public state involves several steps to ensure data integrity and transparency. When a user initiates a transaction involving public data, \\nthe relevant changes are proposed and applied to the public state tree. The transaction details, such as transferring funds between accounts or updating smart contract storage, \\nupdate the corresponding leaf nodes in the Merkle tree. Following this, the hashes of the affected nodes are recalculated up to the root, ensuring that the entire tree \\naccurately reflects the new state of the network. The updated Merkle root is then recorded on the blockchain, allowing all network participants to verify the integrity \\nof the public state. Any discrepancy in the data will result in a mismatched root hash, signaling potential tampering or errors.\\n\\nIn summary, the public state in NSSA leverages the robustness of the Merkle tree structure to provide a secure, transparent, and verifiable environment for publicly \\naccessible information. By operating on an account-based model and maintaining rigorous data integrity checks, the public state ensures that all transactions are \\ntransparent and trustworthy, laying a strong foundation for a reliable blockchain network.\\n\\n\\n### b) Private state\\n---\\nThe private state in the NSSA is a sophisticated system designed to maintain user privacy while ensuring transaction integrity. \\nEach user has their own individual Merkle tree, which holds their private information such as balances and storage data. This structure is distinct from the public state, \\nwhich uses an account-based model. Instead, the private state employs a UTXO-based model. In this model, each transaction output is a discrete \\nunit that can be independently spent in future transactions. This design provides users with granular control over their transaction outputs.\\n\\nA key aspect of maintaining privacy within the private state is the use of ZKPs. ZKPs allow transactions to be validated without revealing any \\nunderlying private data. This means that while the system can verify that a transaction is legitimate, the details of the transaction remain confidential. Only parties \\nwith the appropriate viewing key can access and reconstruct the user\u2019s list of UTXOs, ensuring that sensitive information is protected.\\n\\nThe private state also employs a dual-storage approach to balance privacy and transparency. UTXOs are stored in plaintext within SMTs in the private state, \\nproviding detailed and accessible records for the user. In contrast, the public state only holds hashes of these UTXOs. This method ensures that while the public can verify \\nthe existence and integrity of private transactions through these hashes, they cannot access the specific details.\\n\\nThe workflow for a transaction in the private state begins with the user initiating a transaction involving their private data, such as transferring a private balance or \\nupdating private smart contract storage. The transaction involves spending existing UTXOs, represented as leaves in the Merkle tree, and creating new UTXOs, \\nwhich are then appended to the user\u2019s private list. The zkVM generates a ZKP to validate the transaction without revealing \\nany private data, ensuring the transaction adheres to the system\'s rules.\\n\\nOnce the proof is generated, it is submitted to the sequencer, which verifies the transaction\u2019s validity. Upon successful verification, the nullifier is added to the nullifier set, \\npreventing double spending of the same UTXO. The use of ZKPs and nullifiers ensures that the private state maintains both security and privacy.\\n\\nIn summary, the private state in NSSA is meticulously designed to provide users with control over their private information while ensuring the security and integrity of transactions. \\nBy utilizing a UTXO-based model, individual Merkle trees, ZKPs, and a dual-storage system, NSSA achieves a balance between confidentiality and verifiability, \\nmaking it a robust solution for managing private blockchain transactions.\\n\\n### c) ZkVM (zero-knowledge virtual machine)\\n---\\nThe zkVM is a pivotal component in NSSA, designed to uphold the highest standards \\nof privacy and security in blockchain transactions. Its primary function is to generate and aggregate ZKPs, enabling users to validate the \\ncorrectness of their transactions without disclosing any underlying details. This capability is crucial for maintaining the confidentiality and integrity of sensitive \\ndata within the blockchain network.\\n\\nZKPs are sophisticated cryptographic protocols that allow one party, the prover, to convince another party, the verifier, that a certain statement is true, \\nwithout revealing any information beyond the validity of the statement itself. In the context of the zkVM, this means users can prove their transactions are valid without \\nexposing transaction specifics, such as amounts or parties involved. This process is essential for transactions within the private state, where maintaining confidentiality is paramount.\\n\\nThe generation of ZKPs involves intricate cryptographic computations. When a user initiates a transaction, the zkVM processes the transaction inputs and produces a proof \\nthat the transaction adheres to the protocol\'s rules. This proof must be robust enough to convince the verifier of the transaction\'s validity while preserving the privacy \\nof the transaction details.\\n\\nPerformance optimization is another critical function of the zkVM. In a typical blockchain scenario, verifying multiple individual proofs can be computationally intensive \\nand time consuming, potentially leading to network congestion and delays. To address this, the zkVM can aggregate multiple ZKPs into a single, consolidated proof. \\nThis aggregation significantly reduces the verification overhead as the verifier needs to check only one comprehensive proof rather than multiple individual ones. \\nThis efficiency is vital for maintaining high throughput and low latency in the blockchain network, ensuring that the system can handle a large volume of transactions swiftly and securely.\\n\\nFurthermore, the zkVM\'s role extends beyond mere proof generation and aggregation. It also ensures that all transactions meet the required privacy and security standards \\nbefore they are recorded on the blockchain. By interacting seamlessly with other components such as the public and private states, the zkVM ensures that any transaction, \\nwhether it involves public data, private data, or a mix of both, is thoroughly validated and secured.\\n\\nIn summary, the zkVM is essential for the NSSA, providing the cryptographic backbone necessary to support secure and private transactions. Its ability to generate and \\naggregate ZKPs not only preserves the confidentiality of user data but also enhances the overall efficiency and scalability of the blockchain network. \\nBy ensuring that all transactions are validated without revealing sensitive information, the zkVM upholds the integrity and trustworthiness of the Nescience blockchain system.\\n\\n\\n### d) Execution types in NSSA\\n---\\nNSSA incorporates multiple execution types to cater to varying levels of privacy and security requirements. \\nThese execution types\u2014public, private, shielded, and deshielded\u2014are designed to provide users with flexible options for managing their transactions based on their specific privacy needs.\\n\\n#### 1. Public executions\\n\\nPublic executions are straightforward transactions that involve reading from and writing to the public state. In this model, data is openly accessible and verifiable \\nby all participants in the network. Public executions do not require ZKPs since transparency is the primary goal. This execution type is ideal \\nfor non-sensitive transactions where public visibility is beneficial, such as updating public records, performing open financial transactions, or interacting with public smart contracts.\\n\\nThe workflow for a public execution starts with a user initiating a transaction that modifies public data. The transaction details are then used to update the relevant \\nleaf nodes in the Merkle tree. As changes are made, the hashes of affected nodes are recalculated up to the root, ensuring that the entire tree reflects the most recent state. \\nFinally, the updated Merkle root is recorded on the blockchain, making the new state publicly verifiable.\\n\\n#### 2. Private executions\\n\\nPrivate executions are designed for confidential transactions, reading from and writing to the private state. These transactions require ZKPs to ensure that while the \\ntransaction details are validated, the actual data remains private. This execution type is suitable for scenarios where privacy is crucial, such as private financial \\ntransactions or sensitive data management within smart contracts.\\n\\nIn a private execution, the user initiates a transaction involving private data. The transaction spends existing UTXOs and creates new ones, all of which are represented as \\nleaves in the Merkle tree. The zkVM generates a ZKP to validate the transaction without revealing private data. This proof is submitted to the sequencer, \\nwhich verifies the proof to ensure the transaction\'s validity. Upon successful verification, the nullifier is added to the nullifier set, and the private state is updated \\nwith the new Merkle root.\\n\\n#### 3. Shielded executions\\n\\nShielded executions create a layer of privacy for the outputs by allowing interactions between the public and private states. When a transaction occurs in a shielded execution, \\ndetails of the transaction are processed within the private state, ensuring that sensitive information remains confidential. Only the necessary details are shared with the public state, \\noften in a masked or encrypted form. This approach allows for the validation of the transaction without revealing critical data, thus preserving the privacy of the involved parties.\\n\\nThe workflow for shielded executions begins with the user initiating a transaction that reads from the public state and prepares to write to the private state. Public data is accessed, \\nand the private state is prepared to receive new data. The zkVM generates a ZKP to hide the receiver\u2019s identity. This proof is submitted to the sequencer, which verifies \\nthe proof to ensure the transaction\'s validity. If valid, the private state is updated with the new data while the public state reflects the change without revealing private details. \\nThis type of execution is particularly useful for scenarios where the receiver\u2019s identity needs to be hidden, such as in anonymous donation systems or confidential data storage.\\n\\n#### 4. Deshielded executions\\n\\nDeshielded executions operate in the opposite manner of shielded executions, where data is read from the private state and written to the public state. This execution type is useful \\nin situations where the sender\'s identity needs to be kept confidential while making the transaction results publicly visible.\\n\\nIn a deshielded execution, the user initiates a transaction that reads from the private state and prepares to write to the public state. Private data is accessed, \\nand the transaction details are prepared. The zkVM generates a ZKP to hide the sender\u2019s identity. This proof is then submitted to the sequencer, \\nwhich verifies the proof to ensure the transaction\'s validity. Once verified, the public state is updated with the new data, reflecting the change while keeping the sender\u2019s \\ndetails confidential. This can be useful when transparency is needed, such as when auditing or proving certain aspects of a transaction to a wider audience. \\nBy selectively deshielding certain transactions, users can control what information is shared publicly, thus maintaining a balance between privacy and transparency \\nas required by their specific use case.\\n\\n\\n#### Table of execution types\\n\\n| Type       | Read from    | Write to    | ZKP required | Use case                                                                | Description                                                                                                                                                                                                                                                                                                                                                                        |\\n|------------|--------------|-------------|--------------|------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Public     | Public state | Public state| No           | Non-sensitive transactions requiring transparency.                      | Ideal for transactions that do not require privacy, ensuring full transparency.                                                                                                                                              |\\n| Private    | Private state| Private state| Yes         | Confidential transactions needing privacy.                              | Suitable for transactions that require confidentiality. Ensures that transaction details remain private through the use of ZKPs.                                                                                                    |\\n| Shielded   | Public state | Private state| Yes         | Transactions where the receiver\u2019s identity needs to be hidden.          | Hides the identity of the receiver while keeping the transaction details private. Suitable for anonymous donations or confidential data storage.                                                                                                                                                                                               |\\n| Deshielded | Private state| Public state| Yes         | Transactions where the sender\u2019s identity needs to be hidden.            | Ensures the sender\u2019s identity remains confidential while making the transaction results public. Suitable for confidential disbursements or anonymized data publication.  |\\n---\\n\\n\\nBy supporting a range of execution types, NSSA provides a flexible and robust framework for managing privacy and security in blockchain transactions. \\nWhether the need is for complete transparency, total privacy, or a balanced approach, NSSA\'s execution types allow users to select the level of confidentiality \\nthat best fits their requirements. This flexibility enhances the overall utility of the blockchain, making it suitable for a wide array of applications and use cases.\\n\\n### e) Nescience users\\n---\\nNescience users are integral to the architecture, managing balances and assets within the blockchain network and invoking smart contracts with various privacy options. \\nThey can choose the appropriate execution type\u2014public, private, shielded, or deshielded\u2014based on their specific privacy and security needs.\\n\\nUsers handle both public and private balances. Public balances are visible to all network participants and suitable for non-sensitive transactions, \\nwhile private balances are confidential and used for transactions requiring privacy. Digital wallets provide a user-friendly interface for managing \\nthese balances, assets, and transactions, allowing users to select the desired execution type seamlessly.\\n\\nSecurity is ensured through the use of cryptographic keys, which authenticate and verify transactions. ZKPs maintain privacy \\nby validating transaction correctness without revealing underlying data, ensuring sensitive information remains confidential even during verification.\\n\\nThe workflow for users involves initiating a transaction, preparing inputs, interacting with smart contracts, generating proofs if needed, \\nand submitting the transaction to the sequencer for verification and state update. This flexible approach supports various use cases, \\nfrom financial transactions and decentralized applications to data privacy management, allowing users to maintain control over their privacy settings.\\n\\nBy offering this high degree of flexibility and security, Nescience enables users to tailor their privacy settings to their specific needs, \\nensuring sensitive transactions remain confidential while non-sensitive ones are transparent. This integration of cryptographic keys and ZKPs \\nprovides a robust framework for a wide range of blockchain applications, enhancing both utility and trust within the network.\\n\\n\\n### f) Smart contracts in NSSA\\n---\\nSmart contracts are a core feature of NSSA, providing a way to automate and execute predefined actions based on coded rules. \\nOnce deployed on the blockchain, these contracts become immutable, meaning their behavior cannot be altered. This ensures that they perform exactly as \\nintended without the risk of tampering. Because the state and data of the contract are stored permanently on the blockchain, all interactions are fully \\ntransparent and auditable, creating a reliable and trustworthy environment.\\n\\nOne of the key strengths of smart contracts is their ability to automate processes. They are designed to automatically execute when specific conditions are met, \\nreducing the need for manual oversight or intermediaries. For example, a smart contract might transfer funds when a certain deadline is reached or update a record \\nonce a task is completed. This self-executing nature makes them efficient and minimizes human error.\\n\\nSmart contracts operate deterministically, meaning they will always produce the same result given the same inputs. This predictability is crucial for ensuring reliability, \\nespecially in complex systems. Additionally, they run in isolated environments on the blockchain, which enhances security by preventing unintended interactions with other processes.\\n\\nSecurity is another critical feature of smart contracts. They leverage the underlying cryptographic protections of the blockchain, ensuring that every interaction \\nis secure and authenticated. Before deployment, the contract code can be audited and verified to ensure it functions correctly. Once on the blockchain, \\nthe immutable nature of the code prevents unauthorized modifications, further ensuring the integrity of the system.\\n\\nRunning smart contracts requires computational resources, which are compensated through gas fees. These fees vary depending on the complexity of the operations within the contract. \\nMore resource-intensive contracts incur higher fees, which helps manage the computational load on the blockchain network.\\n\\nThe workflow of a smart contract begins with its development, where developers code the contract using languages like Rust. Once the code is compiled and deployed to the blockchain, \\nit becomes a permanent part of the network. Users can then interact with the contract by sending transactions that invoke specific functions. The contract checks whether the \\nrequired conditions are met, and if so, it automatically executes the specified actions, such as transferring tokens or updating data on the blockchain.\\n\\nThe benefits of smart contracts are numerous. They eliminate the need for intermediaries by providing a system where trust is built into the code itself. \\nThis not only reduces costs but also increases efficiency by automating repetitive processes. The inherent security of smart contracts, combined with their \\ntransparency\u2014where every action is recorded and visible on the blockchain\u2014makes them a powerful tool for ensuring accountability and trust in decentralized systems. \\nThey can be ideal for managing decentralized autonomous organizations (DAOs), where governance decisions are automated through coded rules.\\n\\nBy integrating smart contracts, NSSA offers a highly versatile, secure, and transparent framework that can support a wide range of applications \\nacross various industries, from finance to governance, supply chains, and more.\\n\\n## 2. General execution overview\\n---\\nThis section explains the execution process within NSSA, providing an overview of how it works from start to finish. \\nIt outlines the steps involved in each execution type, guiding the reader through the entire process from user interaction to completion.\\n\\nThe process begins when a user initiates a transaction by invoking a smart contract. This invocation involves selecting at least one of \\nthe four execution types: public, private, shielded, or deshielded. The choice of execution type determines how data will be read from and written to the blockchain, \\naffecting the transaction\'s privacy and security levels. Each execution type caters to different privacy needs, allowing the user to tailor the transaction according \\nto their specific requirements, whether it be full transparency or complete confidentiality.\\n\\n![general](/img/general.png)\\n\\n\\n### User actions\\n---\\n**Step 1**: **Smart contract selection and input creation**\\n- **Smart contract selection**: The user selects a smart contract they wish to invoke.\\n- **Input creation**: The user creates a set of inputs required for the invocation by reading the necessary data from both the public and private states. This includes:\\n  - Public data such as current account balances, public keys, and smart contract states.\\n  - Private data such as private account balances and UTXOs.\\n\\n**Step 2**: **Choosing execution type**\\n- **Execution type selection**: The user selects the type of execution based on their privacy needs. The options include:\\n  - **Public execution**: Suitable for transactions where transparency is desired.\\n  - **Private execution**: Used when transaction details need to be confidential.\\n  - **Shielded execution**: Hides the receiver\'s identity.\\n  - **Deshielded execution**: Hides the sender\'s identity.\\n- **ZkVM requirement**: If the execution involves private, shielded, or deshielded types, the user must call the zkVM to handle these confidential transactions. \\nFor purely public executions, the zkVM is not needed, and the user can directly transmit the transaction code to the sequencer.\\n\\n**Step 3**: **Calling zkVM for proof generation**\\n- **ZkVM compilation**: The user calls the zkVM to compile the smart contract with both public and private inputs.\\n  - **Kernel circuit proofs**: The zkVM generates individual proofs for each execution type through kernel circuits.\\n  - **Proof aggregation**: The zkVM aggregates these individual proofs into a single comprehensive proof, combining both private and public inputs.\\n\\n**Step 4**: **Transmitting public inputs and retaining private inputs**\\n- **Retaining private inputs**: The user keeps the private inputs secure and does not transmit them.\\n- **Revealing public inputs**: The user transmits the following public inputs to the sequencer:\\n  - Public inputs of the recursive proof\\n  - Hashes of UTXOs\\n  - Updates to the public state\\n  - Transaction signature\\n  - Nullifiers (to prevent double spending)\\n\\nAfter completing these steps, the user\'s part of the execution is done, and the sequencer takes over the process.\\n\\n### Sequencer actions\\n---\\n**Step 5**: **Proof verification**\\n- **Proof and data reception**: The sequencer receives the proof and public inputs from the user.\\n- **Verification process**:\\n  - For private, shielded, and deshielded executions, the sequencer verifies the proof using the provided public data.\\n  - For public executions, the sequencer reruns the smart contract code with the provided inputs to check the results.\\n- **Validation**: If both the zkVM proofs and public execution results are verified successfully, the sequencer collects the proof and public data to proceed. \\nIf verification fails, the process is aborted, and the transaction is rejected.\\n\\n**Step 6**: **Aggregating proofs and finalizing the block**\\n- **Proof aggregation**: The sequencer calls the zkVM again to aggregate all received proofs into one comprehensive proof to finalize the block.\\n- **Finalizing the block**:\\n  - **Public state update**: The sequencer updates the public state with the new transaction data.\\n  - **Nullifier tree update**: Updates the nullifier tree to reflect the new state and prevent double spending.\\n  - **Synchronization mechanism**: Runs synchronization mechanisms to ensure fairness and consistency across the network.\\n  - **UTXO validation**: Validates the exchanged UTXOs to complete the transaction process.\\n\\nThis comprehensive process ensures that transactions are executed securely, with the appropriate level of privacy and state updates synchronized across the network.\\n\\nBelow, we outline the execution process of the four different execution types within NSSA:\\n- **Public execution**:\\n\\n![public](/img/public.png)\\n\\n- **Private execution**:\\n\\n![private](/img/private.png)\\n\\n- **Shielded execution**:\\n\\n![shielded](/img/se.png)\\n\\n- **Deshielded execution**:\\n\\n![deshielded](/img/de.png)\\n\\n\\n## 3. Execution processes and UTXO management\\n---\\nIn Nescience state-separation architecture, UTXOs are key components for managing private data and assets. They serve as private entities that hold both storage and assets, \\nfacilitating secure and confidential transactions. UTXOs are utilized in three of the four execution types within NSSA: private, shielded, \\nand deshielded executions. This section explores the lifecycle of UTXOs, detailing their generation, transfer, encryption, and eventual consumption within the private execution framework.\\n\\n### a) Components of a Nescience UTXO\\n---\\nA Nescience UTXO is a critical and versatile component of the private state in the Nescience state-separation architecture. \\nIt carries essential information that ensures its proper functionality within private execution, such as the owner, value, private storage slot, non-fungibles, \\nand other cryptographic components. Below is a detailed breakdown of each component and its role in maintaining the integrity, security, and privacy of the system:\\n\\n* **Owner:**\\nThe owner component represents the public key of the entity that controls the UTXO. Only the owner can spend this UTXO, ensuring its security and privacy through public key cryptography. \\nThis means that the UTXO remains secure as only the rightful owner, using their private key, can generate valid signatures to authorize the transaction. For example, \\nif Alice owns a UTXO linked to her public key, she must sign any transaction to spend it using her private key. This cryptographic protection ensures that only Alice can authorize \\nspending the UTXO and transfer it to someone else, such as Bob.\\n\\n* **Value:**\\nThe value in a UTXO represents the balance or asset contained within it. This could be cryptocurrency, tokens, or other digital assets. The value ensures accurate accounting, \\npreventing double spending and maintaining the overall integrity of the system. For instance, if Alice\'s UTXO has a value of 10 tokens, this represents her ownership of that amount \\nwithin the network, and when spent, this value will be deducted from her UTXO and transferred accordingly.\\n\\n* **Private storage slot:**\\nThe private storage slot is an arbitrary and flexible storage space within the UTXO for Nescience applications. It allows users and smart contracts to store additional private data \\nthat is only accessible by the owner. This could be used to hold metadata, smart contract states, or user-specific information. For example, if a smart contract is holding private user data, \\nthis information is securely stored in the private storage slot and can only be accessed or modified by the owner, ensuring privacy and security.\\n\\n* **Non-fungibles:**\\nNon-fungibles within the UTXO represent unique assets, such as NFTs (Non-Fungible Tokens). Each non-fungible asset is assigned a unique serial number or identifier within the UTXO, \\nensuring its distinctiveness and traceability. For example, if Alice owns a digital artwork represented as an NFT, the non-fungible component of the UTXO will store the unique identifier \\nfor this NFT, preventing duplication or forgery of the digital asset.\\n\\n* **Random commitment key:**\\nThe random commitment key (RCK) is a randomly generated number used to create a cryptographic commitment to the contents of the UTXO. This commitment ensures the integrity of the data \\nwithout revealing any private information. By generating a random key for the commitment, the system ensures that even if someone observes the commitment, they cannot infer any details \\nabout the underlying UTXO. For example, RCK helps maintain confidentiality in the system while still allowing the verification of transactions.\\n\\n* **Nullifier key:**\\nThe Nullifier key is another randomly generated number, used to ensure that a UTXO is only spent once. When a UTXO is spent, its nullifier key is recorded in a nullifier set to prevent \\ndouble spending. This key guarantees that once a UTXO is spent, it cannot be reused in another transaction, effectively nullifying it from future use. This mechanism is crucial for \\nmaintaining the security and integrity of the system, as it ensures that no UTXO can be spent more than once.\\n\\n\\n### b) UTXO lifecycle: From generation to consumption\\n---\\nUTXOs in NSSA are created when a transaction outputs a specific value, asset, or data intended for future use. Once generated, these UTXOs become private entities \\nowned by specific users, containing sensitive information such as balances, private data, or unique assets like NFTs.\\n\\nTo maintain the required level of confidentiality, UTXOs are encrypted and transferred anonymously across the network. This encryption process ensures that the data within each UTXO \\nremains hidden from network participants, including the sequencer, while still allowing for verification and validation through ZKPs. These proofs enable the network \\nto ensure that UTXOs are valid, prevent double spending, and maintain security, all without revealing any sensitive information.\\n\\nWhen a user wishes to spend or transfer a UTXO, the lifecycle progresses towards its consumption. The user must prove ownership and validity of the UTXO through a ZKP, \\nwhich is then verified by the sequencer. This process occurs in private, shielded, and deshielded executions, where confidentiality is a priority. Once the proof is validated, \\nthe UTXO is consumed, meaning it is marked as spent and cannot be reused, ensuring the integrity of the transaction and preventing double spending. \\n\\nUTXOs are central to the private, shielded, and deshielded execution types in Nescience. In private executions, UTXOs are transferred securely between parties without revealing any \\ndetails to the public state. In shielded executions, UTXOs are used to receive assets from the public state while keeping the recipient\'s identity confidential. Finally, \\nin deshielded executions, UTXOs are used to send assets from the private state to the public state, while preserving the sender\'s anonymity.\\n\\nSince UTXOs are not exchanged in public executions, this lifecycle analysis is focused solely on private, shielded, and deshielded executions, where privacy and confidentiality are essential. \\nIn these contexts, the careful management and transfer of UTXOs ensure that the users\' private data and assets remain secure, while still allowing for seamless and confidential transactions \\nwithin the network.\\n\\nAt this point, it\'s crucial to introduce two key components that will play a significant role in the next section: the ephemeral key and the nillifier.\\n\\n* **Ephemeral key:** The ephemeral key is embedded in the transaction message and plays a crucial role in maintaining privacy. It is used by the sender, alongside the receiver\'s public key, \\nin a key agreement protocol to derive a shared secret. This shared secret is then employed to encrypt the transaction details, ensuring that only those with the receiver\'s viewing key can \\ndecrypt the transaction. By using the ephemeral key, the receiver can regenerate the shared secret, granting access to the transaction\'s contents. The sender generates the ephemeral key \\nusing their spending key and the UTXO\'s nullifier, reinforcing the security of the transaction. (more details in [key management and addresses section](#key))\\n\\n* **Nullifier:** A nullifier is a unique value tied to a specific UTXO, ensuring that it has not been spent before. Its uniqueness is essential, as a nullifier must never correspond to more \\nthan one UTXO\u2014otherwise, even if both UTXOs are valid, only one could be spent. This would undermine the integrity of the system. To spend a UTXO, a proof must be provided showing that \\nthe nullifier does not already exist in the Nullifier Tree. Once the transaction is confirmed and included in the blockchain, the nullifier is added to the Nullifier Tree, preventing any \\nfuture reuse of the same UTXO. A UTXO\'s nullifier is generated by combining the receiver\'s nullifier key with the transaction note\'s commitment, further ensuring its distinctiveness \\nand security. (More details in [nullifier tree section](#nul).)\\n\\n####  <a id=\\"pe\\"></a> I) UTXOs in private executions\\n---\\nIn private executions within NSSA, transactions are handled ensuring maximum privacy by concealing all transaction details from the public state. \\nThis approach is particularly useful for confidential payments, where the identities of the sender and receiver, as well as the transaction amounts, must remain hidden. \\nThe process is powered by ZKPs, ensuring that only the involved parties have access to the transaction details while maintaining the integrity of the network.\\n\\n* **Stages of private execution**: Private executions operate in two key stages: UTXO consumption and UTXO creation. In the first stage, UTXOs from the private state are used \\nas inputs for the transaction. In the second stage, new UTXOs are generated as outputs and stored back in the private state. Throughout this process, the details of the \\ntransaction are kept confidential and only shared between the sender and receiver.\\n\\n* **Private transaction workflow (transaction initialization)**: The user initiates a private transaction by selecting the input UTXOs that will be spent and determining the \\noutput UTXOs to be created. This involves specifying the amounts to be transferred and the recipient\u2019s private address (a divestified address that hides the recipient\'s public \\naddress from the network). The nullifier key and random number for commitments (RCK) are also generated at this stage to define how these UTXOs can be spent or nullified in the \\nfuture by the receiver.\\n\\n* **Proof generation and verification**: Next, the zkVM generates a ZKP to validate the transaction. This proof includes both a membership proof for the input UTXOs, \\nconfirming their presence in the hashed UTXO tree, and a non-membership proof to ensure that the input UTXOs have not already been spent (i.e., they are not in the nullifier tree). \\nThe proof also confirms that the total input value matches the total output value, ensuring no discrepancies. The user then submits the proof, along with the necessary metadata, to the sequencer.\\n\\n* **Shared secret and encryption**: To maintain confidentiality, the sender uses the receiver\u2019s divestified address to generate an ephemeral public key. \\nThis allows the creation of a shared secret between the sender and receiver. Using a key derivation function, a symmetric encryption key is generated from the shared secret. \\nThe input and output UTXOs are then encrypted using this symmetric key, ensuring that only the intended recipient can decrypt the data.\\n\\n* **Broadcasting the transaction**: The user broadcasts the encrypted UTXOs to the network, along with a commitment to the output UTXOs using Pedersen hashes. \\nThese committed UTXOs are sent to the sequencer, which updates the hashed UTXO tree without knowing the transaction details.\\n\\n* **Decryption by the receiver**: After the broadcast, the receiver attempts to decrypt the broadcast UTXOs using their symmetric key, derived from the ephemeral public key. \\nIf the receiver successfully decrypts a UTXO, it confirms ownership of that UTXO. The receiver then computes the nullifier for the UTXO and verifies its presence in the hashed \\nUTXO tree and its absence from the nullifier tree, ensuring it has not been spent. Finally, the new UTXO is added to the receiver\u2019s locally stored UTXO tree for future transactions.\\n\\nThroughout the private execution process, the identities of both the sender and receiver, as well as all transaction details, remain hidden from the public. \\nThe use of ZKPs ensures that the integrity of the transaction is verified without revealing any sensitive information. At the end of the process, \\nthe network guarantees that no participant, aside from the sender and receiver, can deduce any details about the transaction or the involved parties.\\n\\n#### II) UTXOs in shielded executions\\n---\\nIn shielded executions, the interaction between public and private states provides a hybrid privacy model that balances transparency and confidentiality. \\nThis model is suitable for scenarios where the initial step, such as a public transaction, requires visibility, while subsequent actions, such as private asset management, \\nneed to remain confidential. One common use case is asset conversion\u2014where a public token is converted into a private token. The conversion is visible on the public ledger, \\nbut subsequent transactions remain private.\\n\\n##### a) How shielded executions work\\nShielded executions operate in two distinct stages: first, there is a modification of the public state, and then new UTXOs are created and stored in the private state. \\nImportantly, shielded executions do not consume UTXOs but instead mint them, as new UTXOs are created to reflect the changes in the private state. This structure demands \\nZKPs to ensure that the newly minted UTXOs are consistent with the modifications in the public state. Here\u2019s a step-by-step breakdown of how the shielded \\nexecution process unfolds:\\n\\n1. **Transaction initiation:** The user initiates a transaction that modifies the public state, such as converting a public token to a private token.\\n The transaction alters the public state (e.g., balances or smart contract storage) while simultaneously preparing to mint new UTXOs in the private state.\\n\\n2. **Generating UTXOs:** After modifying the public state, the system mints new UTXOs in the private state. These UTXOs must be securely created, ensuring their integrity \\nand consistency with the initial public state modification. A ZKP is generated by the user to prove that these new UTXOs align with the changes made in the public state.\\n\\n3. **Key setup for privacy**: The sender retrieves the receiver\'s address and uses it to create a shared secret through an ephemeral public key. This shared secret is then used \\nto derive a symmetric key, which encrypts the output UTXOs. This encryption ensures that only the intended receiver can decrypt and access the UTXOs.\\n\\n4. **Broadcasting and verifying UTXOs**: After encrypting the UTXOs, the sender broadcasts them to the network. The new hashed UTXOs are sent to the sequencer, \\nwhich verifies the validity of the UTXOs and attaches them to the hashed UTXO tree within the private state. The public inputs for the ZKP circuits consist of the \\nPedersen-hashed UTXOs and the modifications in the public state.\\n\\n5. **Receiver\'s role**: Once the UTXOs are broadcast, the receiver attempts to decrypt each UTXO using the symmetric key derived from the shared secret. If the decryption is successful, \\nthe UTXO belongs to the receiver. The receiver then verifies the UTXO\u2019s validity by checking its inclusion in the hashed UTXO tree and ensuring that its nullifier has not yet been used.\\n\\n6. **Nullifier check and integration**: To prevent double spending, the receiver computes the nullifier for the received UTXO and verifies that it is not already present in the nullifier tree. \\nOnce verified, the receiver adds the UTXO to their locally stored UTXO tree for future use in private transactions.\\n\\n\\nWhile shielded executions offer privacy, certain information is still exposed to the public state, such as the sender\'s identity. To further enhance privacy, \\nthe sender can create empty UTXOs\u2014UTXOs that don\u2019t belong to anyone but are included in the transaction to obfuscate the true details of the transaction. \\nThough this approach increases the size of the data, it adds a layer of privacy by complicating the identification of meaningful transactions.\\n\\n##### b) Summary of shielded execution flow\\n* **Stage 1 (public modification):** The user modifies public state data, such as converting tokens from public to private. This stage is visible to the public.\\n* **Stage 2 (UTXO minting and privacy):** New UTXOs are minted in the private state, encrypted, and broadcast to the network. The transaction remains private from this point forward, \\nsecured by ZKPs and cryptographic keys.\\n* **Receiver\u2019s role:** The receiver decrypts the UTXOs and verifies their validity, ensuring the UTXOs are not double spent and are ready for future transactions.\\n\\nIn summary, shielded executions enable a hybrid privacy model in Nescience, balancing public transparency and private confidentiality. They are well-suited for \\ntransactions requiring initial public visibility, such as asset conversions, while ensuring that subsequent actions remain secure and private within the network.\\n\\n#### III) UTXOs in deshielded executions\\n---\\nIn NSSA, deshielded executions offer a unique way to move data and assets from the private state to the public state, revealing previously private \\ninformation in a controlled and verifiable manner. This type of execution allows for selective disclosure, ensuring transparency when needed while still maintaining \\nthe security and privacy of critical details through cryptographic techniques like  ZKPs. Deshielded executions are particularly valuable for use cases \\nsuch as regulatory compliance reporting, where specific transaction details must be revealed to meet legal requirements, while other sensitive transactions remain private.\\n\\n##### a) Stages of deshielded executions\\n\\n* **Stage 1 (UTXO consumption):** The process begins in the private state, where UTXOs are consumed as inputs for the transaction. This involves gathering all necessary \\nUTXOs that contain the assets or balances to be made public, as well as any associated private data stored in memory slots.\\n\\n* **Stage 2 (public state modification):** After the UTXOs are consumed, the transaction details are made public by modifying the public state. This update includes changes \\nto the public balances, storage data, and any necessary public records. While the public state is updated, the sender\u2019s identity and other sensitive information remain hidden, \\nthanks to the privacy-preserving properties of ZKPs.\\n\\nThis model ensures that private data can be selectively revealed when needed, offering both flexibility and transparency. It is particularly useful for scenarios requiring \\nauditing or compliance reporting, where specific details must be made publicly verifiable without exposing the entire history or contents of private transactions.\\n\\n##### b) How deshielded executions work\\nThe deshielded execution process starts when a user initiates a transaction using private UTXOs. The Nescience zkVM is called to generate a ZKP, \\nwhich validates the transaction without revealing sensitive details such as the sender\'s identity or the specifics of the Nescience application being executed.\\n\\nDuring the transaction, the UTXOs from the private state are consumed, meaning they are used up as inputs and will no longer be available for future transactions. \\nInstead of generating new UTXOs, the transaction modifies the public state, updating the necessary balances or memory slots related to the transaction. \\nHere\u2019s a step-by-step breakdown of how the deshielded execution process unfolds:\\n\\n1. **Get receiver\'s public address:** The sender first identifies the public address of the receiver, to which the information or assets will be made public.\\n\\n2. **Determine input UTXOs and public state modifications:** The sender gathers all the input UTXOs needed for the transaction and determines the public state modifications \\nnecessary for the Nescience applications and token transfers involved.\\n\\n3. **Calculate nullifiers:** Nullifiers are generated for each input UTXO, ensuring that these UTXOs cannot be reused or double spent. The nullifiers are derived from the\\n corresponding UTXO commitments.\\n\\n4. **Call zkVM with deshielded circuits:** The sender invokes the zkVM with deshielded kernel circuits, which generates the proof. The proof ensures that all input UTXOs \\nare valid by verifying their membership in the UTXO tree and their non-membership in the nullifier tree, ensuring they haven\u2019t been spent.\\n\\n5. **Generate and submit proof:** The zkVM generates a ZKP that verifies the correctness of the transaction without revealing private details. \\nThe proof includes the nullifiers and the planned modifications to the public state.\\n\\n6. **Send proof to sequencer:** The sender then sends the proof and any relevant public information to the sequencer. The sequencer is responsible for verifying the proof, \\nupdating the public state accordingly, and adding the nullifiers to the nullifier tree.\\n\\nOnce the proof and public information have been broadcast to the network, the receiver does not need to take any further action. \\nThe sequencer manages the public state updates and ensures that the transaction is properly executed. By the end of the deshielded execution, \\nspecific transaction details become publicly visible, such as the identity of the receiver and the outcome of the transaction. \\nThis allows participants in the public state to extract information about the transaction, including the receiver\'s identity and some details about the execution. \\nWhile the receiver\'s identity is revealed, the sender\'s identity and sensitive transaction details remain hidden, thanks to the use of ZKPs. \\nThis makes deshielded executions ideal for cases where transparency is needed, but complete privacy is still a priority for certain elements of the transaction.\\n\\n### Summary of UTXO consumption in NSSA\\n---\\nIn NSSA, consuming UTXOs is a critical step in maintaining the security and integrity of the blockchain by preventing double spending. \\nWhen a UTXO is consumed, it is used as an input in a transaction, effectively marking it as spent. This ensures that the UTXO cannot be reused, preserving the integrity of the blockchain.\\n\\n1. **The process of consuming UTXOs:** The process of consuming a UTXO begins when a user selects a UTXO from their private state. The user verifies the UTXO\u2019s existence and \\nownership using their viewing key, ensuring that they are the legitimate owner of the UTXO. Once verified, the user generates two key cryptographic proofs:\\n    * **Membership proof:** This proof confirms that the UTXO exists within the hashed UTXO tree, ensuring its validity within the system.\\n    * **Non-membership proof:** This proof ensures that the UTXO has not been previously consumed by checking its absence in the nullifier tree, which tracks spent UTXOs.\\n\\nTo mark the UTXO as spent, a **nullifier** is generated. This nullifier is a unique cryptographic hash derived from the UTXO, which is then added to the nullifier tree in the public state. \\nAdding the nullifier to the tree prevents the UTXO from being reused in future transactions, thus preventing double spending.\\n\\nAfter generating the membership and non-membership proofs, the user compiles the transaction using the zkVM. The zkVM is responsible for generating the necessary ZKPs,\\n which validate the transaction without revealing sensitive details. The compiled transaction, along with the proofs, is then submitted to the sequencer for verification.\\n\\n2. **The role of the sequencer:** Once the transaction is submitted, the sequencer verifies the ZKPs to confirm that the transaction is valid. If the proofs are verified \\nsuccessfully, the sequencer updates both the private and public states to reflect the transaction. This includes updating the nullifier tree with the newly generated nullifier, \\nensuring that the UTXO is marked as spent and cannot be reused.\\n\\n#### Example: Alice sending tokens to Bob\\n---\\nConsider an example where Alice wants to send 5 Nescience tokens to Bob using a private execution. Alice selects a UTXO from her private state that contains 5 Nescience tokens. \\nShe generates the necessary membership and non-membership proofs, ensuring that her UTXO exists in the system and has not been previously spent. Alice then creates a nullifier by \\nhashing the UTXO and compiles the transaction with the zkVM.\\n\\nOnce Alice submits the transaction, the sequencer verifies the proofs and updates the blockchain by adding the nullifier to the nullifier tree and recording the transaction details. \\nThis ensures that Alice\u2019s UTXO is marked as spent and cannot be used again, while Bob receives the 5 tokens.\\n\\n#### The importance of nullifiers\\n---\\nNullifiers are a key mechanism in preventing double spending. By marking consumed UTXOs as spent and tracking them in the nullifier tree, NSSA ensures that \\nonce a UTXO is used in a transaction, it cannot be reused in any future transactions. This process is fundamental to maintaining the integrity and security of the blockchain, \\nas it guarantees that assets are only spent once and prevents potential attacks on the system.\\n\\nIn conclusion, the process of consuming UTXOs in NSSA combines cryptographic proofs, nullifiers, and ZKPs to ensure that transactions \\nare secure, confidential, and free from the risks of double spending.\\n\\n\\n# C. Cryptographic primitives in NSSA\\n\\nIn the NSSA, cryptographic primitives are the foundational elements that ensure the security, privacy, and efficiency of the state separation model.\\n These cryptographic tools enable private transactions, secure data management, and robust verification processes across both public and private states. \\n The architecture leverages a wide range of cryptographic mechanisms, including advanced hash functions, key management systems, tree structures, and ZKPs, \\n to safeguard user data and maintain the integrity of transactions.\\n\\nCryptographic hash functions play a pivotal role in concealing UTXO details, generating nullifiers, and constructing sparse Merkle trees, which organize and verify \\ndata efficiently within the network. Key management and address generation further enhance the security of user assets and identity, ensuring that only authorized \\nusers can access and control their holdings.\\n\\nThe architecture also relies on specialized tree structures for organizing data, verifying the existence of UTXOs, and tracking nullifiers, which prevent double spending. \\nAdditionally, Nescience features a privacy-preserving zero-knowledge virtual machine (zk-zkVM), which allows users to prove the correctness of an execution without \\ndisclosing sensitive information. This enables private transactions and maintains confidentiality across the network.\\n\\nAs Nescience evolves, optional cryptographic mechanisms such as multi-party computation (MPC) may be integrated to enhance synchronization across privacy levels. \\nThis MPC-based synchronization mechanism is still under development and under review for potential inclusion in the system. Together, these cryptographic primitives \\nform the backbone of Nescience\u2019s security architecture, ensuring that users can transact and interact privately, securely, and efficiently.\\n\\nIn the following sections, we will explore each of these cryptographic components in detail, beginning with the role of hash functions.\\n\\n## a) Hash functions in Nescience\\n---\\nHash functions are a foundational element of Nescience\u2019s cryptographic framework, serving multiple critical roles that ensure the security, privacy, and efficiency of the system. \\nOne of the primary uses of hash functions in Nescience is to conceal sensitive details of UTXOs by converting them into fixed-size hashes. This process allows UTXO details \\nto remain private, ensuring that sensitive information is not directly exposed on the blockchain, while still enabling their existence and integrity to be verified. Hashing \\nthe UTXO details allows the actual data to remain confidential, with the hashes stored in a global tree structure for efficient management and retrieval.\\n\\nAdditionally, hash functions are essential for generating **nullifiers**, which play a crucial role in preventing double spending. Nullifiers are created by hashing UTXOs \\nand are used to mark them as spent, ensuring that they cannot be reused in subsequent transactions. These nullifiers are stored in a nullifier tree, and each transaction \\nmust prove that its UTXO\u2019s nullifier is not already present in the tree before it can be processed. This ensures that the UTXO has not been spent before, maintaining the \\nintegrity of the transaction process.\\n\\nHash functions are also vital in the construction of **sparse Merkle trees**, which provide an efficient and secure method for verifying data within the blockchain. \\nSparse Merkle trees enable quick and reliable proofs of membership and non-membership, making them essential for verifying both UTXOs and nullifiers. By using hash functions \\nto build these trees, Nescience can ensure the integrity of the data, as any tampering with the data would result in a change in the hash, making the manipulation detectable.\\n\\nAnother critical consideration in Nescience is the compatibility of hash functions with **ZKPs**. ZK-friendly hash functions are optimized for efficient \\ncomputation within the constraints of ZK circuits, ensuring that they do not become a bottleneck in the proof generation or verification process. These hash functions \\nmaintain strong cryptographic security properties while enabling efficient computations in ZKP systems, which is essential for maintaining privacy and \\nintegrity within the ZK framework.\\n\\nThe primary advantage of using hash functions in Nescience is their ability to ensure that transaction details remain private while still allowing for verification \\nof their validity. Furthermore, by integrating hash functions into Merkle trees, the blockchain data becomes tamper-proof, enabling quick and efficient verification \\nprocesses that uphold the system\u2019s security and privacy standards.\\n\\n### Use case: How to use the Pedersen hash to create the UTXO commitment\\n---\\nAs mentioned in the [UTXOs in private executions section](#pe), the user broadcasts the encrypted UTXOs to the network, along with a commitment to the output UTXOs \\nusing **Pedersen hashes**. The Pedersen hash is used to create the UTXO commitment. The Pedersen hash is a homomorphic commitment scheme that allows secure commitments\\nwhile maintaining privacy and enabling proofs of correctness in transactions. The commitment formula is as follows:\\n\\n$$Commitment = C(UTXO,RCK) =g^{UTXO}\u22c5h^{RCK}$$\\n\\n\\nIn this formula, $g$ and $h$ are two generators of a cryptographic group where no known relationship exists between them. This ensures that the commitment is secure \\nand computationally infeasible to reverse or manipulate without knowing the original UTXO components. The random number $RCK$ adds an additional layer of security \\nby blinding the UTXO\'s contents, ensuring that the commitment doesn\'t leak any information about the underlying data.\\n\\n**Importance of homomorphic commitments**\\n\\nIt is essential to use a homomorphic commitment like the Pedersen commitment for UTXOs because it allows for the verification of important properties in transactions, \\nsuch as ensuring that the total input value of a transaction equals the total output value. This balance is crucial for preventing the unauthorized creation of funds or d\\ndiscrepancies in transactions. A homomorphic commitment enables these proofs because of its additive properties. Specifically, the exponents in the commitment formula are additive, \\nmeaning that commitments can be combined and verified without revealing the individual components. For instance, if you have two UTXOs with commitments $C(UTXO_1,RCK_1)$ \\nand $C(UTXO_2,RCK_2)$, you can combine them and verify that the resulting commitment is valid without exposing the actual amounts.\\n\\nThis capability is leveraged through a modified version of the Schnorr protocol, which is used in conjunction with the Pedersen hash to verify the correctness of transactions. \\nThe Schnorr protocol allows users to prove, without revealing the actual values, that the sum of inputs equals the sum of outputs, ensuring that no funds are created or lost in the transaction.\\n\\n**Limitations of standard cryptographic hashes**\\n\\nStandard cryptographic hash functions, such as SHA-256, are not suitable for this purpose because they lack the algebraic structure needed for homomorphic properties. \\nIn particular, while SHA-256 provides strong security for general hashing purposes, it does not allow the additive properties that are required to perform the type of \\nZKPs used in Nescience for UTXO commitments. This is why the Pedersen hash is preferred, as it enables the secure and private execution of transactions\\nwhile allowing for balance verification and other critical proofs.\\n\\n**Conclusion**\\n\\nBy using homomorphic commitments like the Pedersen hash, NSSA ensures that UTXOs can be securely committed and validated without exposing sensitive information. \\nThe random component (RCK) adds an additional layer of security, and the additive properties of the Pedersen commitment enable powerful ZKPs that maintain the \\nintegrity of the system.\\n\\n\\n## <a id=\\"key\\"></a> b) Key management and addresses in Nescience\\n---\\nNSSA utilizes different cryptographic schemes, such as public key encryption and digital signatures, to ensure secure private executions through \\nthe exchange of UTXOs. These schemes rely on a structured set of cryptographic keys, each serving a specific purpose in maintaining privacy, security, and control over assets. \\nHere\'s a breakdown of the keys used in Nescience:\\n\\n### I. Spending key\\nThe spending key is the fundamental secret key in NSSA, acting as the primary control mechanism for a user\u2019s UTXOs and other digital assets. \\nIt plays a critical role in the cryptographic security of the system, ensuring that only the rightful owner can authorize and spend their assets.\\n\\n- **Role of the spending key**: The spending key is responsible for generating the user\u2019s private keys, which are used in various cryptographic operations such as \\nsigning transactions and creating commitments. This hierarchical relationship means that the spending key sits at the root of a user\u2019s key structure, safeguarding \\naccess to all associated private keys and, consequently, to the user\u2019s assets. In Nescience\u2019s privacy-focused model, the spending key is never exposed or shared outside \\nthe user\u2019s control. Unlike other keys, it does not interact with the public state, kernel circuits, or even the ZKP system. This isolation ensures that \\nthe spending key remains completely private and inaccessible to external entities. By keeping the spending key separate from the operational aspects of the network, \\nNescience minimizes the risk of key leakage or compromise.\\n\\n- **Generation and security of the spending key**: The spending key is generated randomly from the scalar field, a large mathematical space that ensures uniqueness \\nand cryptographic strength. This randomness is crucial because it prevents attackers from predicting or replicating the key, thereby safeguarding the user\u2019s assets \\nfrom unauthorized access: it is computationally infeasible for an attacker to guess or brute-force the key. Once the spending key is generated, it is securely stored \\nby the user, typically in a hardware wallet or another secure storage mechanism that prevents unauthorized access.\\n\\n- **Spending UTXOs with the spending key**: The spending key\u2019s primary function is to authorize the spending of UTXOs in private transactions. When a user initiates \\na transaction, the spending key is used to generate the necessary cryptographic proofs and signatures, ensuring that the transaction is valid and originates from \\nthe rightful owner. However, even though the spending key generates these proofs, it is never directly exposed during the transaction process. Instead, derived \\nprivate keys handle the operational aspects while the spending key remains secure in the background. For example, when Alice decides to spend a UTXO in a \\nprivate execution, her spending key generates the required private keys that will sign the transaction and ensure its validity. However, the spending key itself \\nnever appears in any public state or transaction data, preserving its confidentiality.\\n\\n- **Ensuring security through isolation**: One of the key security principles of the spending key is its isolation from the network. Since it never interacts with \\npublic-facing elements, such as the public state or kernel circuits, the risk of exposure is significantly reduced. This isolation ensures that even if other parts \\nof the cryptographic infrastructure are compromised, the spending key remains protected, preventing unauthorized spending of UTXOs.\\n\\nIn summary, the spending key in Nescience is a powerful and carefully guarded element of the cryptographic system. It is the root key from which other private keys\\nare derived, allowing users to spend their UTXOs securely and privately. Its isolation from the public state and its random generation from a secure scalar field ensures \\nthat the spending key remains protected, making it a cornerstone of security in NSSA.\\n\\n### II. Private keys\\nIn Nescience, the private key is an essential cryptographic element responsible for facilitating various secure operations, such as generating commitments and signing \\ntransactions. While the spending key plays a foundational role in safeguarding access to UTXOs and assets, the private keys handle the operational aspects of transactions \\nand cryptographic proofs. The private key consists of three critical components: ${private}_{key}.rsd$, ${private}_{key}.rcm$, and ${private}_{key}.sig$, each serving a \\ndistinct purpose within the Nescience cryptographic framework.\\n\\n1. **${private}_{key}.rsd$ (random seed)**: The random seed (${private}_{key}.rsd$) is the first and foundational component of the private key. It is a value randomly chosen from the scalar field, which ensures \\nits cryptographic security and unpredictability. This seed is generated using a random number generator, making it virtually impossible to predict or replicate. \\nThe random seed is essential because it is used to derive the other two components of the private key. By leveraging a secure random seed, Nescience ensures that \\nthe entire private key structure is rooted in randomness, preventing external entities from guessing or deriving the key through brute-force attacks. \\nThe strength of the random seed ensures the overall security of the private key and, consequently, the integrity of the user\'s transactions and commitments.\\n\\n2. **${private}_{key}.rcm$ (random commitment)**: The random commitment component (${private}_{key}.rcm$) is a crucial part of the private key used specifically in the commitment scheme. It acts as a blinding factor, \\nadding a layer of security to commitments made by the user. The ${private}_{key}.rcm$ value is also drawn from the scalar field and is used to ensure that the commitment \\nto any UTXO or other sensitive data remains confidential. The commitment scheme in Nescience requires the use of ${private}_{key}.rcm$ to create cryptographic commitments \\nthat bind the user to specific data (such as UTXO details) without revealing the actual data. The role of ${private}_{key}.rcm$ is to ensure that these commitments are \\nnon-malleable and secure, preventing anyone from modifying the committed data without detection. For instance, when Alice commits to a UTXO, ${private}_{key}.rcm$ is used \\nto generate a Pedersen commitment that ensures the UTXO details are hidden but can still be verified cryptographically. This means that even though the actual UTXO details \\nare concealed, their existence and integrity can be proven.\\n\\n3. **${private}_{key}.sig$ (signing key for transactions)**: The signing key (${private}_{key}.sig$) is the third and final component of the private key, used primarily for signing transactions. One possible approach is that \\nNescience employs Schnorr signatures, a cryptographic protocol known for its efficiency and security. In this case, the ${private}_{key}.sig$ component would generate \\nSchnorr signatures that are used to authenticate transactions, ensuring that only the rightful owner of the private key can authorize the spending of UTXOs. Schnorr \\nsignatures are important as they provide a secure and non-repudiable method of verifying that a transaction was initiated by the legitimate owner of the assets. \\nWhen Alice signs a transaction using her ${private}_{key}.sig$, the corresponding public key allows others to verify that the transaction was indeed signed by Alice, \\nwithout revealing her private key. This verification process ensures that all transactions are legitimate and prevents unauthorized entities from forging transactions \\nor spending assets they do not control. Even if an attacker gains access to the signed transaction, they cannot reverse engineer the ${private}_{key}.sig$, ensuring \\nthe security of Alice\'s future transactions.\\n\\n**Robustness of private keys in Nescience**\\n\\nDespite the critical role of the private key in the operation of NSSA, the system is designed to maintain security even in the event that the \\nprivate key is compromised. This resilience is achieved through the integrity of the spending key, which is never exposed in the process of signing or committing. \\nThe spending key acts as the ultimate safeguard, ensuring that even if a private key component is compromised, the attacker cannot access or spend the user\'s assets \\nwithout control over the spending key.\\n\\nThe architecture\u2019s design, where private keys handle operational tasks but rely on the spending key for ultimate control, ensures a layered approach to security. \\nThis way, the system can mitigate the damage of a compromised private key by maintaining the inviolability of the user\'s assets.\\n\\n**Conclusion**\\n\\nIn summary, the private key in Nescience consists of three interrelated components that together ensure secure transaction signing, commitment creation, and the \\nprotection of user data. The ${private}_{key}.rsd$ serves as the root from which the other key components are derived, ensuring randomness and security. \\nThe ${private}_{key}.rcm$ plays a crucial role in generating commitments, while ${private}_{key}.sig$ provides the signing capability needed for transaction authentication. \\nTogether, these components enable users to engage in private, secure transactions while preserving the integrity of their assets, even in the face of potential key compromise.\\n\\n### III. Public keys\\nPublic keys in Nescience serve as the user\'s interface with the network, allowing for secure interaction and verification without exposing the user\'s private keys. \\nDerived directly from the user\'s private keys, public keys play a crucial role in enabling cryptographic operations such as transaction verification, commitment schemes, \\nand deterministic computations. The public key components correspond to their private key counterparts and ensure that transactions and commitments are securely processed \\nand validated across the network.\\n\\n1. **${public}_{key}.sig$ (verifying Schnorr signatures)**:\\n\\nThe ${public}_{key}.sig$ is derived from the signing component of the private key (${private}_{key}.sig$) and is used for verifying **Schnorr signatures**. \\nSchnorr signatures are used to authenticate transactions, ensuring that they have been signed by the legitimate owner of the private key. This public key is \\nessentially a verification key, allowing others in the network to confirm that a specific transaction was indeed authorized by the user. When a transaction is \\nbroadcast to the network, ${public}_{key}.sig$ enables any participant to verify that the transaction\u2019s signature matches the user\u2019s private key without \\nneeding access to the private key itself. This mechanism prevents forgeries as only the legitimate owner with access to the private key can generate a valid Schnorr signature.\\nFor example, if Alice sends a transaction, she signs it with her private key (${private}_{key}.sig$). Bob, or any other network participant, can use Alice\u2019s ${public}_{key}.sig$ \\nto verify the signature. If the signature is valid, Bob can be confident that the transaction was authorized by Alice and not by an imposter.\\n\\n2. **${public}_{key}.rcm$ (commitment schemes)**\\n\\nThe ${public}_{key}.rcm$ is derived from the commitment component of the private key (${private}_{key}.rcm$). It is used in the **commitment schemes** \\nthat underpin Nescience\u2019s privacy-preserving architecture. Commitments are a crucial cryptographic technique that allows users to commit to a piece of data (such as a UTXO)\\n without revealing the actual data, while still enabling proof of its integrity and existence. In Nescience, the ${public}_{key}.rcm$ is used as part of the Pedersen commitment scheme, \\n where it functions as a public commitment to certain transaction details. Even though the actual values are hidden (thanks to the private key component), the commitment can \\n still be verified by other network participants using ${public}_{key}.rcm$. This enables secure and private transactions while maintaining the ability to verify that commitments \\n are consistent with the original data. For instance, when Alice commits to a UTXO, she uses her private key to generate the commitment, and the ${public}_{key}.rcm$ is available \\n to others to verify the commitment\u2019s validity without revealing the underlying details.\\n\\n3. **${public}_{key}.sk(prf)$ (pseudorandom function)**\\n\\nThe ${public}_{key}.sk(prf)$ is derived from a random field element within the private key and is used to generate the **pseudorandom function (PRF)** associated with the user\'s account. \\nThis PRF is essential for producing deterministic outputs based on the user\u2019s keys and transaction data while ensuring that these outputs are unique to the user and cannot be \\npredicted or replicated by others. The PRF is crucial in scenarios where the user needs to derive unique identifiers or values that are tied to their specific account, \\nensuring that these values remain consistent across different transactions or interactions without revealing sensitive information. For example, ${public}_{key}.sk(prf)$ may be \\nused in generating deterministic yet secure addresses or transaction references, which can be linked to the user\u2019s activity in a controlled manner. By using ${public}_{key}.sk(prf)$, \\nNescience ensures that certain operations, like generating addresses or computing deterministic transaction outcomes, remain both private and cryptographically secure. The public key\u2019s \\nrole in this process is to maintain consistency in these outputs while preventing unauthorized parties from reverse engineering the associated private keys or transaction data.\\n\\n**Summary**\\n\\nPublic keys in Nescience are essential for secure interactions within the network. \\"${public}_{key}.sig$\\" allows others to verify that transactions were signed by the legitimate owner, \\nensuring the authenticity of every operation. \\"${public}_{key}.rcm$\\" enables secure and private commitment schemes, allowing participants to commit to transaction details without \\nrevealing sensitive information. Finally, \\"${public}_{key}.sk(prf)$\\" powers deterministic outputs through a pseudorandom function, ensuring that user-specific data remains consistent \\nand secure throughout various transactions. Together, these public key components facilitate privacy, security, and trust within NSSA, enabling seamless interactions while safeguarding user data.\\n\\n### IV. Viewing key\\nThe **viewing key** in NSSA is a specialized cryptographic key that allows a user to decrypt both incoming and outgoing transactions associated with their account. \\nThis key is designed to offer a degree of transparency to the user, enabling them to view the details of their transactions without compromising the security of their assets or granting \\ncontrol over those assets.\\n\\n- **Role of the viewing key**: The primary function of the viewing key is to provide visibility into transaction details while maintaining the integrity of private, shielded, \\nor deshielded transactions. It enables the user to see the specifics of the transactions they are involved in\u2014such as amounts transferred, asset types, and metadata\u2014without \\nexposing the sensitive transaction data to the broader network. For instance, if Alice has executed a private transaction with Bob, her viewing key allows her to decrypt and \\nreview the details of the transaction, ensuring that everything was processed correctly. This ability to audit her own transactions helps Alice maintain confidence in the integrity \\nof her private interactions on the blockchain.\\n\\n- **Security considerations**: Despite its utility, the viewing key must be handled with care as its exposure could potentially compromise the user\u2019s privacy. \\nAlthough possessing the viewing key does **not** provide the ability to spend or sign transactions (that authority remains strictly with the spending key and private keys), \\nit does allow anyone with access to the viewing key to decrypt the details of the user\u2019s private transactions. This means that if the viewing key is leaked or stolen, \\nthe privacy guarantees of Nescience\u2019s private, shielded, and deshielded executions could be undermined. Specifically, the viewing key could be used to link various transactions, \\nbreaking the unlinkability of private transactions. For example, an attacker with access to the viewing key could decrypt past and future transactions, exposing the relationships \\nbetween different parties and transaction flows. To mitigate this risk, Nescience recommends that users treat their viewing key with the same level of protection as their private keys. \\nIt should be stored securely in encrypted hardware wallets or other secure storage solutions to prevent unauthorized access. \\n\\n- **Balancing privacy and transparency**: The viewing key provides an essential balance between privacy and transparency in NSSA. While it ensures that users \\ncan monitor their transaction history and verify the details of their private transactions, it does so without compromising the control of their funds. This allows users to maintain \\na transparent view of their interactions while keeping their assets secure. For example, if Alice is using shielded execution to transfer assets, her viewing key enables her to \\naudit the transaction without allowing anyone else, including Bob or external observers, to see the specific details unless they also have access to the viewing key. Moreover, \\nsince the viewing key does not grant signing or spending authority, even if it were exposed, an attacker would still not be able to manipulate the user\u2019s assets. However, \\nto maintain the unlinkability and confidentiality of private transactions, the viewing key must be kept secure at all times.\\n\\n- **Protecting transaction unlinkability**: In private transactions, unlinkability is one of the core privacy guarantees. This property ensures that individual \\ntransactions cannot be correlated with each other or linked to the same user unless that user chooses to reveal the connection. The viewing key must be carefully \\nprotected to preserve this unlinkability, as its compromise could allow someone to map out a user\u2019s private transaction history. For instance, in deshielded transactions, \\nthe viewing key allows the user to see which private UTXOs were consumed and how the public state was modified. If the viewing key is compromised, an attacker could potentially \\nlink private UTXOs across multiple transactions, unraveling the user\u2019s privacy.\\n\\n**Conclusion**\\n\\nThe viewing key in Nescience is a powerful tool for providing insight into both incoming and outgoing transactions without granting control over assets. It allows users \\nto decrypt and verify their transaction details, maintaining transparency in their interactions. However, due to its potential to compromise privacy if exposed, the viewing \\nkey must be handled with great care. Proper security measures are necessary to protect the viewing key, ensuring that the unlinkability of private, shielded, and deshielded \\ntransactions remains intact. In this way, the viewing key offers a crucial balance between privacy and transparency within the Nescience ecosystem.\\n\\n### <a id=\\"key\\"></a> V. Ephemeral key\\nThe ephemeral key is generated using a combination of the sender\u2019s spending key and the UTXO\'s nullifier, ensuring that the key is unique to each transaction. \\nThe process can be informally described as follows:\\n\\n1. **Ephemeral key generation**  \\n   Let $\\\\rho$ denote the nullifier of the UTXO being consumed in the transaction. The sender uses the receiver\u2019s public key component ${public}_{key}.sk(prf)$, \\n   which is derived from the receiver\u2019s private key, to compute an **ephemeral secret key ($esk$)**. The computation is based on the nullifier $\\\\rho$ and a base value:\\n  \\n  $$esk = {public}_{key}.sk(prf((0,0,0,0) || \\\\rho)$$\\n   This formula binds the secret key to the specific transaction, leveraging the receiver\u2019s cryptographic identity and the unique properties of the UTXO being spent.\\n\\n2. **Deriving the ephemeral public key**  \\n   After computing the ephemeral secret key ($esk$), the next step is to derive the corresponding **ephemeral public key (epk)**. This is done using the Key Agreement \\n   Protocol\'s **DerivePublic algorithm**, which generates the public key associated with the shared secret key. The ephemeral public key is computed as:\\n  \\n  $$epk = KA.DerivePublic(esk, gd)$$\\n  \\nHere, ($gd$) is the **diversifier address** associated with the receiver\u2019s account. The diversifier address is computed from the receiver\u2019s \\naccount using the **DiversifierHash** function:\\n  \\n  $$gd = receiver.DiversifierHash(d)$$\\n\\nThe diversifier ($d$) is a random value selected by the sender to add randomness to the process. This diversifier ensures that even if a single receiver is involved \\nin multiple transactions, the derived keys remain distinct for each transaction. The value ($d$) is included in the transaction note for transparency and reproducibility.\\n\\n3. **Establishing the shared secret**  \\nThe shared secret, used to encrypt the transaction details, is derived from the key agreement between the sender\u2019s ephemeral key and the receiver\u2019s viewing key. \\nAny party possessing the receiver\u2019s viewing key can use it in conjunction with the ephemeral key to compute the shared secret, which is then used to decrypt the transaction. \\nThis ensures that only the intended recipient (or anyone with their viewing key) can access the transaction details.\\n\\n**Key components and protocol**\\n\\nThe formal protocol for generating ephemeral keys closely follows this informal description but involves additional intermediate steps for converting values to \\nbinary sequences to fit implementation requirements. These steps are essential for ensuring compatibility with cryptographic algorithms used in NSSA. \\nThe protocol uses the following key components:\\n- **Nullifier ($\\\\rho$):** Ensures that the ephemeral key is tied to the specific UTXO being consumed, preventing reuse of the key in future transactions.\\n- **Receiver\u2019s public key (${public}_{key}.sk(prf)$:** Establishes the receiver\'s identity in the key generation process, ensuring that the shared secret can \\nonly be derived by the intended party.\\n- **Diversifier ($d$):** Adds randomness to the transaction, ensuring that keys remain unique across different transactions involving the same receiver.\\n\\nThe end result is an ephemeral key system that provides strong cryptographic guarantees for transaction privacy, leveraging key agreement protocols and secure \\ncryptographic primitives to prevent unauthorized access to sensitive transaction data.\\n\\n**Conclusion**\\n\\nThe ephemeral key in Nescience is a critical element for maintaining transaction confidentiality. It facilitates a secure key agreement between the sender and the receiver, \\nallowing for the encryption of transaction details with a shared secret that can only be derived by the intended recipient. By incorporating the nullifier, receiver\'s public key, \\nand diversifier address, the ephemeral key ensures that transaction privacy is preserved while preventing unauthorized access to transaction information, even in a complex, \\nmulti-party blockchain environment.\\n\\n### VI. Nescience addresses\\nNescience\u2019s dual address system is a core component of its privacy-focused architecture, designed to balance transparency and confidentiality across different types of transactions. \\nThe architecture provides each user or smart contract with both public addresses and private addresses, allowing them to participate in both open and confidential activities on the blockchain.\\n\\n#### a) Public addresses\\nPublic addresses in Nescience are visible to all participants on the network and reside within the public state. These addresses are essential for engaging in \\ntransparent and verifiable interactions, such as sending tokens or invoking smart contracts that are meant to be publicly auditable. Public addresses serve as \\nthe interface for users who need to engage with the transparent elements of the system, including public transactions or smart contracts that require public access.\\n\\nThey are analogous to traditional blockchain addresses seen in systems like Ethereum or Bitcoin, where every participant can see the address and the transactions associated with it. \\nFor example, when Alice wants to receive tokens from Bob in a public transaction, she can provide her public address, allowing Bob to send the tokens transparently. \\nAnyone on the network can verify the transaction, providing accountability and trust in the public state.\\n\\nBecause public addresses are visible and auditable, they are typically used for interactions where privacy is not a concern or where transparency is desirable. \\nThis could include simple token transfers, public contract calls, or interactions with dapps that require public accountability, \\nsuch as voting or governance systems.\\n\\n#### b) Private addresses\\nIn contrast, private addresses are designed for confidentiality and are not visible onchain. These addresses are used exclusively for private transactions and executions, \\nensuring that sensitive details\u2014such as the sender, receiver, or amount transferred\u2014remain hidden from the public state. Private addresses are a key feature of \\nNescience\u2019s private, shielded, and deshielded execution models, where preserving the confidentiality of participants is crucial.\\n\\nUsers can generate an unlimited number of private addresses using their private keys. This flexibility allows users to compartmentalize their interactions, \\ngiving them the ability to provide different private addresses to different parties. For instance, Alice could create a unique private address for each entity \\nshe interacts with, thereby ensuring that her transactions remain isolated and difficult to trace. This feature enhances privacy by preventing any direct linkage \\nbetween different transactions or activities associated with a single user.\\n\\nPrivate addresses are not tied to the public state and are only accessible through the user\u2019s private key infrastructure. Transactions involving private addresses \\nare conducted within the confines of the private state and are only decrypted by the intended participants. For example, when Alice sends tokens to Bob using \\na private address, the details of that transaction remain confidential, accessible only to Alice and Bob, unless they choose to reveal it.\\n\\n**Role of the viewing key in private addresses**: A key feature of Nescience\u2019s private address system is the viewing key, which allows users to decrypt any transaction \\ninvolving their private addresses. This capability provides oversight and transparency into the user\u2019s private transactions, ensuring that they can monitor their own \\nactivity without exposing the details to the public. The viewing key does not compromise the security of the user\'s assets as it does not grant spending or signing authority. \\nHowever, it does allow the user to audit and verify the accuracy of their private transactions, ensuring that everything proceeds as expected. For instance, Alice can use her \\nviewing key to review the details of a private transaction she conducted with Bob, ensuring that the correct amount was transferred and that the transaction was properly processed. \\nThis functionality is critical for users who want to maintain control over their private interactions while still benefiting from transparency into their transaction history. \\nThe ability to generate multiple private addresses and decrypt them with the viewing key ensures that users can maintain compartmentalized privacy without sacrificing oversight.\\n\\n**Summary**\\n\\nNescience\u2019s dual address system\u2014comprising public and private addresses\u2014provides users with the flexibility to engage in both transparent and confidential transactions. \\nPublic addresses are visible onchain and are used for open, public interactions that require accountability and auditability. In contrast, private addresses are \\ninvisible onchain and are used for confidential transactions, enhancing privacy and security.\\n\\nBy allowing users to generate multiple private addresses, Nescience gives individuals control over the visibility of their transactions. Combined with the viewing \\nkey\u2019s ability to decrypt transactions involving private addresses, the system ensures that users can maintain transparency over their private transactions without \\nexposing sensitive information to the public state. This dual-address approach enables users to seamlessly switch between public and private interactions depending on their needs, \\nproviding a robust framework for both privacy and transparency in NSSA.\\n\\n### VII. Conclusion\\nKey management in NSSA is a carefully designed system that strikes an optimal balance between security, privacy, and flexibility. \\nThe architecture\u2019s hierarchical structure, with distinct roles for the spending key, private keys, and public keys, ensures that users retain full control \\nover their assets while maintaining the integrity of their transactions. The spending key, as the root of security, provides unassailable control over the \\nuser\'s UTXOs and assets, ensuring that only the rightful owner can authorize spending. Private keys, derived from the spending key, enable users to engage \\nin cryptographic operations such as signing transactions and generating commitments without exposing sensitive information to the network.\\n\\nThe viewing key adds another layer of transparency, allowing users to decrypt and review their transactions without compromising their authority over their assets. \\nWhile it provides a window into transaction history, the viewing key does not grant spending power, preserving the critical separation between visibility and control.\\n\\nThe dual system of public and private addresses gives users the flexibility to navigate between open, transparent transactions and confidential, privacy-protected activities. \\nPublic addresses allow users to engage in verifiable, public interactions while private addresses enable compartmentalized, secure transactions that remain hidden \\nfrom the public eye. This dual-address framework ensures that users can seamlessly adapt to different privacy requirements, whether they are participating in public \\ndapps or conducting sensitive financial operations.\\n\\nOverall, Nescience\u2019s cryptographic infrastructure is designed to empower users to engage confidently in both transparent and confidential activities. \\nBy providing flexible, secure key management and address systems, Nescience ensures that users can fully participate in the blockchain ecosystem without \\ncompromising their privacy or control. The architecture supports the nuanced needs of modern blockchain users, who require both the transparency of public \\ninteractions and the security of private transactions, all while maintaining the integrity and confidentiality of their assets.\\n\\n\\n## c) Trees in NSSA\\nTrees in NSSA serve as verifiable databases, essential for maintaining privacy and security. Different types of trees are used for various purposes:\\n\\n1. **Global state tree:** The global state tree is a single, public tree that holds all public assets and storage information. It acts as a central repository for all \\npublicly accessible data on the blockchain. By organizing this data in a Merkle tree structure, the Global State Tree allows for efficient and secure verification of public information.\\n\\n2. **Hashed UTXO tree:** The hashed UTXO tree is a public tree that contains hashes of all created UTXOs. When users wish to consume a UTXO, they provide a membership \\nproof to demonstrate that the UTXO exists within this tree. This process ensures that only valid and existing UTXOs can be spent, maintaining the integrity of transactions. \\nIn fact, users generate membership proofs that verify the presence of specific UTXOs in the tree without revealing their actual data. The benefit here is that the Merkle \\ntree structure allows for quick and efficient verification of UTXO existence.\\n\\n3. **UTXO trees (private states):** Each user or smart contract has its private state stored in UTXO trees. These trees are kept as plaintext on the client\u2019s \\nlocal system (off-chain), ensuring privacy as sensitive information remains confidential. The private state includes all UTXOs owned by the user or the smart contract, and these \\nare not directly exposed to the public blockchain. For instance, users have full control over their private state, which is not visible to other participants in the network.\\n\\nIn conclusion, the tree structures enable efficient verification of transaction validity without compromising privacy. By using Merkle trees, \\nNescience ensures that any tampering with the data can be easily detected. The efficient structure of these trees supports the scalability of the architecture, \\nallowing it to handle a large number of transactions and data entries. By leveraging different types of trees, Nescience ensures efficient and secure management \\nof both public and private states.\\n\\n##<a id=\\"nul\\"></a>  d) Nullifier tree in Nescience\\n\\nThe **nullifier tree** is a fundamental component of NSSA, designed to prevent double spending by securely tracking all consumed UTXOs. \\nThis tree acts as a public ledger of spent UTXOs, ensuring that once a UTXO is consumed in a transaction, it cannot be reused in future transactions. \\n\\nThe primary function of the nullifier Tree is to store the **nullifiers** of all consumed UTXOs. By recording the nullifiers in a public tree, \\nthe system ensures that each UTXO is spent only once, thereby safeguarding the integrity of the entire network.\\n\\n- **Ensuring non-membership and preventing double spending**\\nBefore a user can consume a UTXO in a transaction, they must provide a **non-membership proof**. This proof demonstrates that the UTXO\u2019s nullifier \\ndoes not already exist in the Nullifier Tree, proving that the UTXO has not been spent before. If the UTXO\u2019s nullifier is found in the tree, \\nthe system will reject the transaction, preventing double spending. The non-membership proof ensures that users cannot attempt to spend the \\nsame UTXO in multiple transactions. This mechanism is critical for maintaining the security and reliability of NSSA. \\nThe tree structure, which is typically built using a cryptographic tree like a Merkle tree, allows for efficient verification of nullifiers. \\nVerifiers can quickly check whether a nullifier is present or absent in the tree, ensuring that each UTXO is only spent once.\\n\\n- **Nullifier tree structure and operation**\\nThe nullifier tree is likely structured as a **Merkle tree**, which is a cryptographic binary tree where each node represents the hash of its child nodes. \\nThis structure allows for efficient storage and verification of large sets of nullifiers as only the root hash of the tree needs to be stored on the blockchain. \\nWhen a new nullifier is added to the tree, the tree is recalculated, and the root hash is updated. This process ensures that all consumed UTXOs are securely recorded. \\nEach time a transaction consumes a UTXO, the nullifier is added to the Nullifier Tree, and the tree is updated to reflect this new entry. To verify that a \\nUTXO has not been double spent, verifiers can use the tree\u2019s root hash and a proof of inclusion or exclusion (membership or non-membership proof) to check whether the \\nnullifier is present in the tree. For example, if Alice wants to spend a UTXO, she must prove that the nullifier associated with that UTXO is not already in the Nullifier Tree. \\nShe generates a non-membership proof that shows her nullifier is not recorded in the tree, and the transaction is allowed to proceed. Once the transaction is completed, \\nthe nullifier is added to the tree, ensuring that the UTXO cannot be used again.\\n\\n**Conclusion**\\nThe Nullifier Tree is a crucial element of Nescience\'s security. By recording all consumed UTXOs and ensuring that nullifiers are unique, the tree prevents double spending \\nand maintains the integrity of the blockchain. The non-membership proof mechanism guarantees that every transaction is validated against the tree. This structure supports \\nthe scalability and security of NSSA, providing a reliable method for verifying the validity of transactions while preventing malicious behavior.\\n\\n## e) Recursive-friendly privacy-preserving zk-zkVM\\nThe development of the zk-zkVM in Nescience is a work in progress, as the architecture continues to evolve to support privacy-preserving transactions \\nand efficient ZKP generation. The goal of the zk-zkVM is to seamlessly integrate with the Nescience state-separation architecture, \\nensuring that private transactions remain confidential while allowing the network to verify their validity without compromising privacy.\\n\\nCurrently, we are exploring and testing several existing zkVMs to identify the most suitable platform for our needs. Our focus is on finding a zkVM \\nthat not only supports the core features of Nescience, such as state separation and privacy, but also provides the efficiency and scalability required \\nfor a decentralized system. Once a suitable zkVM is chosen, we will begin implementing advanced privacy features on top of it, including support for \\nconfidential transactions, selective disclosure, and recursive proof aggregation.\\n\\nThe integration of these privacy-preserving features with an existing zkVM will enable Nescience to fully employ its state-separation architecture, \\nensuring that users can conduct private transactions with robust security and scalability. This approach will allow us to leverage the strengths of \\nproven zkVM technologies while enhancing them with the unique privacy and state-separation capabilities that Nescience requires.\\n\\n\\n- **Privacy-preserving features**: At its core, the zk-zkVM is designed with privacy in mind. One of the zk-zkVM\u2019s standout privacy features is **selective disclosure**, \\nwhich allows users to reveal only specific details of a transaction as needed. For example, a user could disclose the transaction amount while concealing the identities \\nof the participants. The zk-zkVM employs advanced encryption techniques to protect this sensitive data. All transaction data is encrypted before being stored on the blockchain, \\nso even if the data is intercepted, it cannot be deciphered without the appropriate decryption keys. Another of the crucial privacy-preserving features is the support \\nfor **confidential transactions**. Only the parties involved in the transaction can access the encrypted data. Furthermore, the zk-zkVM supports **verifiable encryption**, \\na powerful capability that allows encrypted data to be included in ZKPs without needing to decrypt it. This ensures that transaction details remain private \\nwhile their correctness can still be proven.\\n\\n\\n- **Lightweight design for accessibility**: The zk-zkVM is being designed to be lightweight and efficient, enabling it to run on standard consumer-grade hardware. \\nThis makes it accessible to a wide range of users without requiring specialized equipment or significant computational resources.\\n\\n- **Faster proving time**: To maintain a seamless user experience, especially during high transaction volumes, the zk-zkVM is being optimized for **fast proving times**. \\nFast proof generation is particularly important for ensuring that the system remains usable during periods of peak activity, preventing bottlenecks and maintaining the fluidity of the network.\\n\\n- **Recursive-friendly operations**: One of the most advanced features of the zk-zkVM will be its support for **recursive operations**. Recursion enables the aggregation \\nof multiple proofs into a single proof, improving efficiency on both the client and sequencer sides of the network.\\n\\n- **Client-side recursion (batch processing):** When a single transaction involves multiple executions, each requiring its own ZKP, these individual \\nproofs can be recursively aggregated before being sent to the sequencer. This reduces the overall data transmitted, enhancing the efficiency of the transaction process \\nby compressing multiple proofs into a single package.\\n  \\n- **Sequencer-side recursion (reduced redundancy):** The sequencer, which is responsible for processing transactions and creating verifiable blocks, collects transactions \\ncontaining aggregated proofs. These proofs are further merged into a single comprehensive proof, ensuring that all transactions within a block are validated collectively. \\nThis process reduces redundancy and optimizes the blockchain\u2019s efficiency by minimizing the size and complexity of the proofs required for verification.\\n\\n- **Developer-friendly language**: To foster widespread adoption and innovation within the Nescience ecosystem, the zk-zkVM would include a **developer-friendly language**. \\nThis high-level language simplifies the process of building applications that leverage state separation and privacy-preserving transactions. The language should offer extensive \\nsupport for modular design, APIs, and SDKs, enabling developers to integrate their applications with the zk-zkVM more easily. By lowering the barrier to entry, Nescience encourages \\ninnovation and helps expand the range of privacy-preserving applications that can be built on its platform.\\n\\n**Conclusion**\\n\\nThe zk-zkVM in Nescience is a powerful and versatile virtual machine that embodies the principles of privacy, efficiency, and scalability. By supporting ZKPs \\nand integrating with advanced privacy technologies like homomorphic encryption. Its lightweight design allows it to run efficiently on standard hardware, promoting decentralization, \\nand its recursive operations further enhance the system\'s scalability. With its developer-friendly language and fast proving times, the zk-zkVM is positioned as a key component in \\nfostering the growth and adoption of privacy-preserving blockchain applications.\\n\\n## f) MPC-based synchronization mechanism (under review)\\n\\nNescience is developing an **MPC-based** synchronization mechanism to balance privacy and fairness between public and private execution types. \\nThis mechanism extracts common information from encrypted UTXOs without revealing private details, ensuring privacy and preventing UTXO linkage to users or specific transactions. \\nIt guarantees that public and private executions remain equitable, with the total input equaling the public output.\\n\\nThe mechanism employs **MPC protocols** to perform computations privately, **ZKPs** to verify correctness, and **cryptographic protocols** \\nto secure data during synchronization. This ensures a consistent and fair environment for all users, regardless of their chosen privacy level. Currently, \\nthis feature is under development and review for potential inclusion depending on the research output and compatibility.\\n\\n\\n\\n\\n\\n# D. Future plans for Nescience\\n\\nNescience is committed to continuously evolving its architecture to ensure scalability, privacy, and security in a growing blockchain landscape. \\nOne of the primary goals is to integrate the **zk-zkVM** and the **Nescience state-separation architecture** into a fully functioning node, \\nenabling efficient private transactions while maintaining network integrity.\\n\\n- **Addressing scalability challenges**: A key challenge facing Nescience is the increasing size of nullifier and hashed UTXO trees, which could impact \\nnetwork performance and scalability over time. To mitigate this, Nescience plans to adopt state-of-the-art scalable privacy techniques such as:\\n  - **Mutator sets:** Dynamically adjusting data structures to manage the growth of the nullifier set efficiently.\\n  - **SNARK-based accumulators:** Compressing data in a verifiable way to ensure that only relevant information is stored while maintaining cryptographic security.\\n  - **Pruning techniques:** Periodically trimming unnecessary data from trees to maintain optimal size and performance, ensuring that the network scales logarithmically \\n  rather than exponentially as more transactions occur.\\n\\nBy implementing these approaches, Nescience aims to keep the size of its data structures manageable, ensuring that scalability does not come at the cost of performance or privacy.\\n\\n- **Enhanced key management**: Another critical focus for Nescience is improving key management to streamline operations and enhance security. \\nThe plan is to integrate the different keys used for signatures, addresses, UTXO encryption, and SNARK verification into a unified system. \\nThis integration will simplify key management for users while reducing the risk of security breaches caused by complex, disparate key systems. \\nNescience also plans to implement **Hierarchical Deterministic (HD) keys**, which allow users to derive multiple keys from a single seed, \\nenhancing both security and usability. This approach reduces the complexity of managing multiple keys across various functions and provides an additional \\nlayer of protection for private transactions. Additionally, **multi-signature schemes** will be introduced, requiring multiple parties to authorize transactions. \\nThis feature increases security by reducing the likelihood of unauthorized access, ensuring that a single compromised key cannot lead to malicious transactions.\\n\\n- **Integrating advanced cryptographic techniques**: Nescience will integrate advanced cryptographic techniques, enhancing both privacy and scalability. Among these are:\\n  - **Homomorphic encryption:** Allowing computations to be performed on encrypted data without the need to decrypt it, preserving privacy while enabling secure, complex data processing.\\n  - **Zero-knowledge rollups:** Bundling multiple transactions into a single proof to reduce the amount of data processed and stored on the blockchain, \\n  significantly improving scalability without sacrificing security.\\n\\nThese cryptographic enhancements will ensure that Nescience can support a growing network while continuing to protect user privacy and maintaining high transaction throughput.\\n\\n- **Long-term vision**\\n\\nThe ultimate goal for Nescience is to deploy a fully operational **node powered by zk-zkVM** and the **Nescience state-separation architecture**. \\nThis node will handle complex, private transactions at scale while integrating all of the advanced cryptographic techniques outlined in the roadmap. \\nNescience aims to provide users with an infrastructure that balances privacy, security, and efficiency, ensuring the network remains resilient and capable of handling future demands.\\n\\nBy pursuing these future plans, Nescience is poised to not only address current challenges around scalability and key management but also lead the way in \\napplying advanced cryptography to decentralized systems. This vision will help secure the long-term integrity and performance of the Nescience state-separation \\nmodel as the blockchain grows and evolves.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n# References\\n\\n[1] Nakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. Retrieved from https://bitcoin.org/bitcoin.pdf\\n\\n[2] Sanchez, F. (2021). Cardano\u2019s Extended UTXO accounting model. Retrieved from https://iohk.io/en/blog/posts/2021/03/11/cardanos-extended-utxo-accounting-model/\\n\\n[3] Morgan, D. (2020). HD Wallets Explained: From High Level to Nuts and Bolts. Retrieved from https://medium.com/mycrypto/the-journey-from-mnemonic-phrase-to-address-6c5e86e11e14\\n\\n[4] Wuille, P. (2012). Bitcoin Improvement Proposal (BIP) 32. Retrieved from https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\\n\\n[5] Sin7y Tech Review (29): Design Principles of Private Transactions in Aleo & Zcash. Retrieved from https://hackmd.io/@sin7y/rkxFXLkgs\\n\\n[6] Sin7y Tech Review (33): Principles of private transactions and regulatory compliance issues. Retrieved from https://hackmd.io/@sin7y/S16RyFzZn\\n\\n[7] Zcash Protocol Specification. Retrieved from https://zips.z.cash/protocol/protocol.pdf\\n\\n[8] Anatomy of a Zcash Transaction. Retrieved from https://electriccoin.co/blog/anatomy-of-zcash\\n\\n[9] The Penumbra Protocol: Notes, Nullifiers, and Trees. Retrieved from https://protocol.penumbra.zone/main/concepts/notes_nullifiers_trees.html\\n\\n[10] Zero-knowledge Virtual Machine (ZKVM). Retrieved from https://medium.com/@abhilashkrish/zero-knowledge-virtual-machine-zkvm-95adc2082cfd\\n\\n[11] What\'s a Sparse Merkle tree?. Retrieved from https://medium.com/@kelvinfichter/whats-a-sparse-merkle-tree-acda70aeb837\\n\\n[12] Lecture 10: Accounts Model and Merkle Trees. Retrieved from https://web.stanford.edu/class/ee374/lec_notes/lec10.pdf\\n\\n[13] The UTXO vs Account Model. Retrieved from https://www.horizen.io/academy/utxo-vs-account-model/\\n\\n[14] Addresses and Value Pools in Zcash. Retrieved from https://zcash.readthedocs.io/en/latest/rtd_pages/addresses.html"},{"id":"vac101-membership-with-bloom-filters-and-cuckoo-filters","metadata":{"permalink":"/rlog/vac101-membership-with-bloom-filters-and-cuckoo-filters","source":"@site/rlog/2024-07-19-vac101-bloomfilter.mdx","title":"Vac 101: Membership with Bloom Filters and Cuckoo Filters","description":"We examine two data structures: Bloom filters and Cuckoo filters.","date":"2024-07-19T12:00:00.000Z","formattedDate":"July 19, 2024","tags":[],"readingTime":12.62,"hasTruncateMarker":true,"authors":[{"name":"Marvin","github":"jonesmarvin8","key":"marvin"}],"frontMatter":{"title":"Vac 101: Membership with Bloom Filters and Cuckoo Filters","date":"2024-07-19T12:00:00.000Z","authors":"marvin","published":false,"slug":"vac101-membership-with-bloom-filters-and-cuckoo-filters","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":4},"prevItem":{"title":"Nescience: A User-Centric State-Separation Architecture","permalink":"/rlog/Nescience-state-separation-architecture"},"nextItem":{"title":"RLN-v3: Towards a Flexible and Cost-Efficient Implementation","permalink":"/rlog/rln-v3"}},"content":"We examine two data structures: Bloom filters and Cuckoo filters.\\n\\n\x3c!--truncate--\x3e\\n\\n## Membership with Bloom Filters and Cuckoo Filters\\n\\nThe ability to efficiently query the membership of an element in a given data set is crucial.\\nIn certain applications, it is more important to output a result quickly than to have a \'perfect\' result.\\nIn particular, false positives may be an acceptable tradeoff for speed.\\nIn this blog, we examine [Bloom](https://dl.acm.org/doi/10.1145/362686.362692) and [Cuckoo](https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf) data filters.\\nBoth of these filters are data structures that can be used for membership proofs.\\n\\nEveryone is familiar with the process of creating a new account for various websites, whether it is an e-mail account or a social media account.\\nConsider when you enter your desired username.\\nMany sites provide real-time feedback, as you type, on the availability of a given string.\\nIn this scenario, it is necessary that the result is seemingly instant, regardless of the number of existing accounts.\\nHowever, it is not important that the usernames that are flagged as unavailable are, in fact, in use.\\nThat is, it is sufficient to have a probabilistic check for membership.\\n\\n**Bloom filters** and **Cuckoo filters** are data structures that can be used to accumulate data with a fixed amount of space.\\nThe associated filter $F$ for a digest of data $D$ can be queried to determine whether an element is (possibly) a member of $D$:\\n\\n- **0:** The queried element is definitely not a member of digest $D$.\\n- **1:** The entry is possibly a member of the digest $D$.\\n\\nThe algorithms associated with Bloom filters and Cuckoo filters, which we will discuss shortly, are deterministic.\\nThe possibility of false positives arises from the query algorithm.\\n\\n\\n## Bloom filters\\nA **Bloom filter** is a data structure that can be used to accumulate an arbitrary amount of data with a fixed amount of space.\\nBloom filters have been a popular data structure for proof of non-membership due to their small storage size.\\nSpecifically, a Bloom filter consists of a binary string ${\\\\bf{v}} \\\\in \\\\{0,1\\\\}^n$ and $k$ hash functions $\\\\{h_i: \\\\{0,1\\\\}^* \\\\rightarrow \\\\{0,\\\\dots,n-1\\\\}\\\\}_{i=0}^{k-1}$.\\nWe note that each hash function $h_i$ is used to determine an index of our binary string ${\\\\bf{v}}$ to flip the associated bit to 1.\\nThe binary string ${\\\\bf{v}}$ is initialized with every entry as 0.\\nThe hash functions do not need to be cryptographic hash functions.\\n\\n\\n- **Append:** Suppose that we wish to add the element $x$ to the Bloom filter.\\n    - Define the vector ${\\\\bf{b}} \\\\in \\\\{0,\\\\dots,n-1\\\\}^k$ so that ${\\\\bf{b}}[i] := h_i(x)$ for each $i \\\\in \\\\{0,\\\\dots,k-1\\\\}$.\\n    - Update the binary string ${\\\\bf{v}}[{\\\\bf{b}}[i]] \\\\leftarrow 1$ for each $i \\\\in \\\\{0,\\\\dots,k-1\\\\}$.\\n   \\n- **Query:** Suppose that we wish to query the Bloom filter for element $y$.\\n    - Return 1 provided ${\\\\bf{v}}[h_i(y)] = 1$ for every $i \\\\in \\\\{0,\\\\dots,k-1\\\\}$. Otherwise, return 0.\\n    \\nThe algorithm **Query** will output 1 for every element $y$ that has been added to the Bloom filter.\\nThis is a consequence of the **Append** algorithm.\\nHowever, due to potential collisions over a set of hash functions, it is possible for false positives to occur.\\nMoreover, the possibility of collisions makes it impossible to remove elements from the Bloom filter.\\n\\n### Complexity\\nThe storage of a Bloom filter requires constant space.\\nSpecifically, the Bloom filter uses $n$ bits regardless of the size of the digest.\\nSo, regardless of the number of elements that we append, the Bloom filter will use $n$ bits.\\nFurther, if we assume that each of the $k$ hash functions runs in constant time, then we can append/query an entry in $O(k)$.\\n\\n### Example\\nSuppose that $k = 3$ and $n = 10$. \\nOur Bloom filter is initialized as $\\\\bf{v} = \\\\begin{pmatrix}0&0&0&0&0&0&0&0&0&0\\\\end{pmatrix}.$\\nNow, we will append the words $add$, $sum$, and $equal$.\\nSuppose that\\n\\n$\\\\begin{matrix}\\nh_0(add) = 1 & h_1(add) = 4 & h_2(add) = 7\\\\\\\\\\nh_0(sum) = 9 & h_1(sum) = 2 & h_2(sum) = 1\\\\\\\\\\nh_0(equal) = 5 & h_1(equal) = 8 & h_2(equal) = 0.\\n\\\\end{matrix}$\\n\\nAfter appending these words, the Bloom filter is $\\\\bf{v} = \\\\begin{pmatrix}1&1&1&0&1&1&0&1&1&1\\\\end{pmatrix}.$\\n\\nNow, suppose that we query the words $subtract$ and $multiple$ so that\\n\\n$\\\\begin{matrix} h_0(subtract) = 3 & h_1(subtract) = 5 & h_2(subtract) = 1\\\\\\\\ h_0(multiple) = 7 & h_1(multiple) = 1 & h_2(multiple) = 4\\\\\\\\\\n\\\\end{matrix}$.\\n\\nThe query for $subtract$ returns 0 since ${\\\\bf{v}}[3]=0$.\\nOn the other hand, the query for $multiple$ returns 1 since ${\\\\bf{v}}[1]=1, {\\\\bf{v}}[4] = 1$, and ${\\\\bf{v}}[7]=1$.\\nEven though $multiple$ was not used to generate the Bloom filter ${\\\\bf{v}}$, our query returns the false positive.\\n\\n\\n### Probability of false positives\\nFor our analysis, we will assume that the probabilities that arise in our analysis are independent. \\nHowever, this assumption can be removed to gain the same approximation.\\n\\nWe note that for a single hash function, the probability that a specific bit is flipped to 1 is $1/n$.\\nSo, the probability that the specific bit is not flipped by the hash function is $1-1/n$.\\nApplying our assumption that the $k$ hash functions are \'independent,\'\\nthe probability that the specific bit is not flipped by any of the hash functions is\\n$(1-1/n)^k$.\\n\\nRecall the calculus fact $\\\\lim_{\\\\infty} (1-1/n)^n = e^{-1}$.\\nThat is, as we increase the number of bits that our Bloom filter uses, the approximate probability that a given bit is not flipped by any of the $k$ hash functions is $e^{-k/n}$.\\n\\nSuppose that $\\\\ell$ entries have been added to the Bloom filter.\\nThe probability that a specific bit is still 0 after the $\\\\ell$ entries have been added is approximately $e^{-\\\\ell k/n}$.\\nThe probability that a queried element is erroneously claimed as a member of the digest is approximately\\n$(1-e^{-\\\\ell k/n})^k$.\\n\\nThe following table provides concrete values for these approximations.\\n\\n| $n$ | $k$ | $\\\\ell$ | $(1-e^{-\\\\ell k/n})^k$|\\n| -------- | -------- | -------- | --- |\\n| 32     |   3   |  3    | 0.01474 |\\n| 32     |   3   |  7    | 0.11143 |\\n| 32     |   3   |  12    |0.30802 |\\n| 32     |   3   |  17    |0.50595 |\\n| 32     |   3   |  28    |0.79804 |\\n\\nNotice that the probability of false positives increases as the number of elements ($\\\\ell$) that have been added to the digest increases.\\n\\n### Sliding-Window Bloom filter\\nOur toy example and table illustrated an issue concerning Bloom filters.\\nThe number of entries that can be added to a Bloom filter is restricted by our choice of $k$ and $n$.\\nNot only does the probability that false positives will occur increase,\\nbut it is possible that our vector ${\\\\bf{v}}$ can be a string of all 1s.\\n[Szepieniec and V\xe6rge](https://eprint.iacr.org/2023/1208.pdf) proposed a modification to Bloom filters to handle this.\\n\\nInstead of having a fixed number of bits for our Bloom filter, we dynamically allot memory based on the number of entries that have been added to the filter.\\nGiven a predetermined threshold ($b$) for the number of entries, we shift our \'window\' of flipping bits by $s$ bits.\\nNote that this means that it is necessary to keep track of when a given entry is added to the digest.\\nThis means that querying the Sliding-Window Bloom filter will yield different results when different timestamps are used.\\n\\nThis can be done with $k$ hash functions as we used earlier.\\nAlternatively, Szepieniec and V\xe6rge proposed using the same hash function but to produce $k$ entries in the current window.\\nSpecifically, we obtain the bits we wish to flip to 1s by computing $h(X || i)$ for each $i \\\\in \\\\{0,\\\\dots, k-1\\\\}$ and $X$ as we will define next.\\nFor Sliding-Window Bloom filters, $X$ is more than just the element we wish to append to the filter.\\nInstead, $X$ consists of the element $x$ and a timestamp $t$. \\nThe timestamp $t$ is used to locate the correct window for bits, as we see below:\\n\\n- **Append:** Suppose that we wish to add the element $x$ with timestamp $t$ to the Sliding-Window Bloom filter.\\n    - Define the vector ${\\\\bf{b}} \\\\in \\\\{0,\\\\dots,n-1\\\\}^k$ so that ${\\\\bf{b}}[i] := h(x||t||i)$ for each $i \\\\in \\\\{0,\\\\dots,k-1\\\\}$.\\n    - Update the binary string ${\\\\bf{v}}[{\\\\bf{b}}[i]+\\\\lfloor t/b \\\\rfloor s] \\\\leftarrow 1$ for each $i \\\\in \\\\{0,\\\\dots,k-1\\\\}$.\\n   \\n- **Query:** Suppose that we wish to query the Bloom filter for element $y$ with timestamp $t$.\\n    - Return 1 provided ${\\\\bf{v}}[h(y||t||i) + \\\\lfloor t/b \\\\rfloor s] = 1$ for every $i \\\\in \\\\{0,\\\\dots,k-1\\\\}$. Otherwise, return 0.\\n    \\n\\nBy incorporating a shifting window, we maintain efficient querying and appending at the cost of constant space.\\nHowever, by losing constant space, we gain \'infinite\' scalability.\\n\\n## Cuckoo filters\\nA Cuckoo filter is a data structure for probabilistic membership proofs based on Cuckoo hash tables.\\nThe specific design goal for Cuckoo filters is to address the inability to remove elements from a Bloom Filter.\\nThis is done by replacing a list of bits with a list of \'fingerprints.\'\\nA fingerprint can be thought of as the hash value for an entry in the digest.\\nA Cuckoo filter is a fixed-length list of \'fingerprints.\'\\nIf the maximum number of entries that a Cuckoo filter can hold is $n$ and a fingerprint occupies $f$ bits,\\nthen the Cuckoo filter occupies $nf$ bits.\\n\\nNow, we describe the algorithms associated with the Cuckoo filter $C$ with hash function $hash(X)$ and fingerprint function $fingerprint(X)$.\\n\\n- **Append:** Suppose that we wish to add the element $x$ to the Cuckoo filter.\\n    - If either position $i_x := hash(x)$ or $j_x := i \\\\otimes hash(fingerprint(x))$ of $C$ is empty,\\n    then $fingerprint(x)$ is inserted into an empty position.\\n    - If both $i_x$ and $j_x$ are occupied with a fingerprint that is distinct from $fingerprint(x)$,\\n    then we select either $i_x$ or $j_x$ to insert $fingerprint(x)$.\\n    The fingerprint that had previously occupied this position cannot be discarried.\\n    Instead, we insert this fingerprint into its alternate location.\\n    This reshuffling process either ends with fingerprints all having their own bucket or one that cannot be inserted.\\n    In the case that we have a fingerprint that cannot be inserted, then the Cuckoo filter is overfilled.\\n\\n- **Query:** Suppose that we wish to query the Cuckoo filter for element $y$.\\n    - Return 1 provided $fingerprint(y)$ is either in position $i_y$ or $j_y$.\\n\\n- **Delete:** Suppose that we wish to delete the element $y$ from the Cuckoo filter.\\n    - If $y$ has been added to the Cuckoo filter, then $fingerprint(y)$ is either in position $i_y$ or $j_y$.\\n      We remove $fingerprint(y)$ from the appropriate position.\\n\\nWe note that false positives in Cuckoo filters only occur when an element shares a fingerprint and hash with a value that has already been added to the Cuckoo filter.\\n\\n### Example\\nIn this example, we will append the words $add$, $sum$, and $equal$ to a Cuckoo filter with 8 slots.\\n\\nFor each word $x$, we compute two indices:\\n$i_x := hash(x) \\\\text{ and } j_x := hash(x) \\\\otimes hash(fingerprint(x)).$\\nSuppose that we have the following values for\\nour words:\\n\\n| word | $i_x$ | $j_x$|\\n|---|---|---|\\n|$add$| $(0,1,0)$ | $(1,0,0)$ |\\n|$sum$| $(1,0,1)$ | $(1,1,0)$ |\\n|$equal$| $(0,1,0)$ | $(1,0,1)$ |\\n\\nFor clarity of the example, we append the words directly to the buckets instead of fingerprints of our data.\\n\\n| |0 | 1 | 2 | 3 | 4| 5| 6| 7|\\n|---|---|---|---|---|---|---|---|---|\\n|append $add$| ||$add$||||| |\\n|append $sum$| ||$add$|||$sum$|| |\\n\\nNotice that both of the buckets (2 and 5) that $equal$ can map to are occupied.\\nSo, we select one of these buckets (say 2) to insert $equal$ into.\\nThen, we have to insert $add$ to its possible bucket (1).\\nThis leaves us with the Cuckoo filter:\\n\\n|0 | 1 | 2 | 3 | 4| 5| 6| 7|\\n|---|---|---|---|---|---|---|---|\\n| |$add$|$equal$|||$sum$|| |\\n\\n### Complexity\\nNotice that deletions and queries to Cuckoo filters are done in constant time.\\nSpecifically, only two locations need to be checked for any data $x$.\\nAppends may require shuffling previously added elements to their alternate locations.\\nAs such, the append does not run in constant time.\\n\\n## Bloom filters vs Cuckoo filters\\nThe design of Bloom filters is focused on space efficiency and quick query time.\\nEven though they occupy constant space,\\nCuckoo filters require significantly more space for $n$ items than Bloom filters.\\nThe worst-case append in a Cuckoo filter is slower than the append in a Bloom filter.\\nHowever, an append that does not require any shuffling in a Cuckoo filter can be quicker than appends in Bloom filters.\\nCuckoo filters make up for these disadvantages with quicker query time and the ability to delete entries.\\nFurther, the probability of false positives in Cuckoo filters is lower than the probability of false positives in Bloom filters.\\n\\n\\n## Combining Filters with RLN\\nIn a series of posts ([1](https://vac.dev/rlog/rln-anonymous-dos-prevention),[2](https://vac.dev/rlog/rln-v3/),[3](https://vac.dev/rlog/rln-light-verifiers)),\\nvarious versons of rate limiting nullifiers (RLN) that are used by Waku has been discussed.\\nRLN uses a sparse Merkle tree for the membership set.\\nThe computational power required to construct the Merkle tree prevent light clients from participating in verifying membership proofs.\\nIn [Verifying RLN Proofs in Light Clients with Subtrees](https://vac.dev/rlog/rln-light-verifiers),\\nit was proposed to move the membership set on-chain so that it would not be necessary for a light client to construct the entire Merkle tree locally.\\nUnfortunately, the naive approach is not practical as the gas limit for a single call is too restrictive for an appropriately sized tree.\\nInstead, it was proposed to make utilize of subtrees.\\nIn this section, we provide a discussion of an alternate solution for light clients by using filters for the membership set.\\nThe two [parts of RLN](https://rate-limiting-nullifier.github.io/rln-docs/rln_in_details.html) that we will focus on are user registration and deletion.\\n\\nBoth Bloom and Cuckoo filters support user registration as this is can be done as an append.\\nThe fixed size of these filters would restrict the total number of users that can register.\\nThis can be migitated by using Sliding-Window Bloom filter as this supports system growth.\\nThe Sliding-Window can be adapted to Cuckoo filters as well.\\nIn the case of a Sliding-Window filter, an user would maintain the epoch of when they registered.\\nThe registration of new users to Bloom filters can be done in constant time which is a significant improvement over appending to subtrees.\\nUnfortunately, the complexity of registration to Cuckoo filters cannot be as easily computed.\\n\\nA user could be slashed from the RLN by sending too many messages in a given epoch.\\nUnfortunately, Bloom filters do not support the deletion of members.\\nLuckily, Cuckoo filters allow for deletions that can performed in constant time.\\n\\nCuckoo filter that use Sliding-Window could be used so that light clients are able to verify proofs of membership in the RLN.\\nThese proofs are not a substitute to the usual proofs that a heavy client can verify due to the allowance of false positives.\\nHowever, with the allowance of false positives, a light client can participate in verification RLN proofs in an efficient manner.\\n\\n\\n### References\\n- [Space/Time Trade-offs in Hash Coding with Allowable Errors](https://dl.acm.org/doi/10.1145/362686.362692)\\n- [David Wagner\'s Lecture Notes on Bloom filters](https://people.eecs.berkeley.edu/~daw/teaching/cs170-s03/Notes/lecture10.pdf)\\n- [Mutator Sets and their Application to Scalable Privacy](https://eprint.iacr.org/2023/1208)\\n- [Cuckoo Filter: Practically Better than Bloom](https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf)\\n- [Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku](https://vac.dev/rlog/rln-anonymous-dos-prevention)\\n- [RLN-v3: Towards a Flexible and Cost-Efficient Implementation](https://vac.dev/rlog/rln-v3/)\\n- [Verifying RLN Proofs in Light Clients with Subtrees](https://vac.dev/rlog/rln-light-verifiers)\\n- [RLN in details](https://rate-limiting-nullifier.github.io/rln-docs/rln_in_details.html)"},{"id":"rln-v3","metadata":{"permalink":"/rlog/rln-v3","source":"@site/rlog/2024-05-13-rln-v3.mdx","title":"RLN-v3: Towards a Flexible and Cost-Efficient Implementation","description":"Improving on the previous version of RLN by allowing dynamic epoch sizes.","date":"2024-05-13T12:00:00.000Z","formattedDate":"May 13, 2024","tags":[],"readingTime":6.31,"hasTruncateMarker":true,"authors":[{"name":"Aaryamann","twitter":"p1ge0nh8er","github":"rymnc","key":"p1ge0nh8er"}],"frontMatter":{"title":"RLN-v3: Towards a Flexible and Cost-Efficient Implementation","date":"2024-05-13T12:00:00.000Z","authors":"p1ge0nh8er","published":true,"slug":"rln-v3","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":4},"prevItem":{"title":"Vac 101: Membership with Bloom Filters and Cuckoo Filters","permalink":"/rlog/vac101-membership-with-bloom-filters-and-cuckoo-filters"},"nextItem":{"title":"Verifying RLN Proofs in Light Clients with Subtrees","permalink":"/rlog/rln-light-verifiers"}},"content":"Improving on the previous version of RLN by allowing dynamic epoch sizes.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nRecommended previous reading: [Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku](https://vac.dev/rlog/rln-anonymous-dos-prevention).\\n\\nThe premise of RLN-v3 is to have a variable message rate per variable epoch,\\nwhich can be explained in the following way:\\n\\n- **RLN-v1:** \u201cAlice can send 1 message per global epoch\u201d\\n\\n  Practically, this is `1 msg/second`\\n\\n- **RLN-v2:** \u201cAlice can send `x` messages per global epoch\u201d\\n\\n  Practically, this is `x msg/second`\\n\\n- **RLN-v3:** \u201cAlice can send `x` messages within a time interval `y` chosen by herself.\\n  The funds she has to pay are affected by both the number of messages and the chosen time interval.\\n  Other participants can choose different time intervals fitting their specific needs.\\n\\n  Practically, this is `x msg/y seconds`\\n\\nRLN-v3 allows higher flexibility and ease of payment/stake for users who have more predictable usage patterns and therefore,\\nmore predictable bandwidth usage on a p2p network (Waku, etc.).\\n\\nFor example:\\n\\n- An AMM that broadcasts bids, asks, and fills over Waku may require a lot of throughput in the smallest epoch possible and hence may register an RLN-v3 membership of `10000 msg/1 second`.\\n  They could do this with RLN-v2, too.\\n- Alice, a casual user of a messaging app built on Waku, who messages maybe 3-4 people infrequently during the day, may register an RLN-v3 membership of `100 msg/hour`,\\n  which would not be possible in RLN-v2 considering the `global epoch` was set to `1 second`.\\n  With RLN-v2, Alice would have to register with a membership of `1 msg/sec`,\\n  which would translate to `3600 msg/hour`. This is much higher than her usage and would\\n  result in her overpaying to stake into the membership set.\\n- A sync service built over Waku,\\n  whose spec defines that it MUST broadcast a set of public keys every hour,\\n  may register an RLN-v3 membership of `1 msg/hour`,\\n  cutting down the costs to enter the membership set earlier.\\n\\n## Theory\\n\\n### Modification to leaves set in the membership Merkle tree\\n\\nTo ensure that a user\u2019s epoch size (`user_epoch_limit`) is included within their membership we must modify the user\u2019s commitment/leaf in the tree to contain it.\\nA user\u2019s commitment/leaf in the tree is referred to as a `rate_commitment`,\\nwhich was previously derived from their public key (`identity_commitment`)\\nand their variable message rate (`user_message_limit`).\\n\\nIn **RLN-v2:**\\n\\n$$\\nrate\\\\_commitment = poseidon([identity\\\\_commitment, user\\\\_message\\\\_limit])\\n$$\\n\\nIn **RLN-v3:**\\n\\n$$\\nrate\\\\_commitment = poseidon([identity\\\\_commitment, user\\\\_message\\\\_limit, user\\\\_epoch\\\\_limit])\\n$$\\n\\n### Modification to circuit inputs\\n\\nTo detect double signaling,\\nwe make use of a circuit output `nullifier`,\\nwhich remains the same if a user generates a proof with the same `message_id` and `external_nullifier`,\\nwhere the `external_nullifier` and `nullifier` are defined as:\\n\\n$$\\nexternal\\\\_nullifier = poseidon([epoch, rln\\\\_identifier]) \\\\\\\\\\nnullifier = poseidon([identity\\\\_secret, external\\\\_nullifier, message\\\\_id])\\n$$\\n\\nWhere:\\n\\n- `epoch` is defined as the Unix epoch timestamp with seconds precision.\\n- `rln_identifier` uniquely identifies an application for which a user submits a proof.\\n- `identity_secret` is the private key of the user.\\n- `message_id` is the sequence number of the user\u2019s message within `user_message_limit` in an epoch.\\n\\nIn RLN-v2, the global epoch was 1 second,\\nhence we did not need to perform any assertions to the epoch\u2019s value inside the circuit,\\nand the validation of the epoch was handled off-circuit (i.e., too old, too large, bad values, etc.).\\n\\nIn RLN-v3, we propose that the `epoch` that is passed into the circuit\\nmust be a valid multiple of `user_epoch_limit`\\nsince the user may pass in values of the `epoch` which do not directly correlate with the `user_epoch_limit`.\\n\\nFor example:\\n\\n- A user with `user_epoch_limit` of 120\\n  passes in an epoch of `237`\\n  generates `user_message_limit` proofs with it,\\n  can increment the epoch by `1`,\\n  and generate `user_message_limit` proofs with it,\\n  thereby allowing them to bypass the message per epoch restriction.\\n\\nOne could say that we could perform this validation outside of the circuit,\\nbut we maintain the `user_epoch_limit` as a private input to the circuit so that the user is not deanonymized by the anonymity set connected to that `user_epoch_limit`.\\nSince `user_epoch_limit` is kept private,\\nthe verifier does not have access to that value and cannot perform validation on it.\\n\\nIf we ensure that the `epoch` is a multiple of `user_epoch_limit`,\\nwe have the following scenarios:\\n\\n- A user with `user_epoch_limit` of 120\\n  passes in an epoch of `237`.\\n  Proof generation fails since the epoch is not a multiple of `user_epoch_limit`.\\n- A user with `user_epoch_limit` of 120\\n  passes in an epoch of `240` and\\n  can generate `user_message_limit` proofs without being slashed.\\n\\nSince we perform operations on the `epoch`, we must include it as a circuit input (previously, it was removed from the circuit inputs to RLN-v2).\\n\\nTherefore, the new circuit inputs are as follows:\\n\\n```c\\n// unchanged\\nprivate identity_secret\\nprivate user_message_limit\\nprivate message_id\\nprivate pathElements[]\\nprivate pathIndices[]\\npublic x // messageHash\\n\\n// new/changed\\nprivate user_epoch_limit\\nprivate user_epoch_quotient // epoch/user_epoch_limit to assert within circuit\\npublic epoch\\npublic rln_identifier\\n```\\n\\nThe circuit outputs remain the same.\\n\\n### Additional circuit constraints\\n\\n1. Since we accept the `epoch`, `user_epoch_quotient`, and `user_epoch_limit`,\\n   we must ensure that the relation between these 3 values is preserved. I.e.:\\n\\n   $$\\n   epoch == user\\\\_epoch\\\\_limit * user\\\\_epoch\\\\_quotient\\n   $$\\n\\n2. To ensure no overflows/underflows occur in the above multiplication,\\n   we must constrain the inputs of `epoch`, `user_epoch_quotient`, and `user_epoch_limit`.\\n   We have assumed `3600` to be the maximum valid size of the `user_epoch_quotient`.\\n\\n$$\\nsize(epoch) \\\\leq 64\\\\ bits \\\\\\\\\\nsize(user\\\\_epoch\\\\_limit) \\\\leq 12\\\\ bits \\\\\\\\\\nuser\\\\_epoch\\\\_limit \\\\leq 3600 \\\\\\\\\\nuser\\\\_epoch\\\\_limit \\\\leq epoch \\\\\\\\\\nuser\\\\_epoch\\\\_quotient < user\\\\_epoch\\\\_limit\\n$$\\n\\n### Modifications to external epoch validation (Waku, etc.)\\n\\nFor receivers of an RLN-v3 proof\\nto detect if a message is too old, we must use the higher bound of the `user_epoch_limit`, which has been set to `3600`.\\nThe **trade-off** here is that we allow hour-old messages to propagate within the network.\\n\\n### Modifications to double signaling detection scheme (Waku, etc.)\\n\\nFor verifiers of RLN-v1/v2 proofs,\\na log of nullifiers seen in the last epoch is maintained,\\nand if there is a match with a pre-existing nullifier,\\ndouble signaling has been detected and the verifier MAY proceed to slash the spamming user.\\n\\nWith the RLN-v3 scheme,\\nwe need to increase the size of the nullifier log used,\\nwhich previously cleared itself every second to the higher bound of the `user_epoch_limit`, which is `3600`.\\nNow, the RLN proof verifier must clear the nullifier log every `3600` seconds to satisfactorily detect double signaling.\\n\\n## The implementation\\n\\nAn implementation of the RLN-v3 scheme in [gnark](https://docs.gnark.consensys.io/) can be found [here](https://github.com/vacp2p/gnark-rln/blob/9b05eddc89901a06d8f41b093ce8ce12fd0bb4e0/rln/rln.go).\\n\\n## Comments on performance\\n\\n- Hardware: Macbook Air M2, 16GB RAM\\n- Circuit: [RLN-v3](https://github.com/vacp2p/gnark-rln/blob/9b05eddc89901a06d8f41b093ce8ce12fd0bb4e0/rln/rln.go)\\n- Proving system: [`Groth16`](https://eprint.iacr.org/2016/260.pdf)\\n- Framework: [`gnark`](https://docs.gnark.consensys.io/)\\n- Elliptic curve: [`bn254`](https://eprint.iacr.org/2013/879.pdf) (aka bn128) (not to be confused with the 254-bit Weierstrass curve)\\n- Finite field: Prime-order subgroup of the group of points on the `bn254` curve\\n- Default Merkle tree height: `20`\\n- Hashing algorithm: [`Poseidon`](https://eprint.iacr.org/2019/458.pdf)\\n- Merkle tree: [`Sparse Indexed Merkle Tree`](https://github.com/rate-limiting-nullifier/pmtree)\\n\\n### Proving\\n\\nThe proving time for the RLN-v3 circuit is `90ms` for a single proof.\\n\\n### Verification\\n\\nThe verification time for the RLN-v3 circuit is `1.7ms` for a single proof.\\n\\n## Conclusion\\n\\nThe RLN-v3 scheme introduces a new epoch-based message rate-limiting scheme to the RLN protocol.\\nIt enhances the user\'s flexibility in setting their message limits and cost-optimizes their stake.\\n\\n## Future work\\n\\n- Implementing the RLN-v3 scheme in [Zerokit](https://github.com/vacp2p/zerokit)\\n- Implementing the RLN-v3 scheme in [Waku](https://github.com/waku-org/nwaku)\\n- Formal security analysis of the RLN-v3 scheme\\n\\n## References\\n\\n- [Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku](https://vac.dev/rlog/rln-anonymous-dos-prevention)\\n- [RLN Circuits](https://github.com/rate-limiting-nullifier/circom-rln)\\n- [Groth16](https://eprint.iacr.org/2016/260.pdf)\\n- [Gnark](https://docs.gnark.consensys.io/)\\n- [Poseidon Hash](https://eprint.iacr.org/2019/458.pdf)\\n- [Zerokit](https://github.com/vacp2p/zerokit)\\n- [RLN-v1 RFC](https://rfc.vac.dev/vac/32/rln-v1)\\n- [RLN-v2 RFC](https://rfc.vac.dev/vac/raw/rln-v2)\\n- [Waku](https://waku.org)"},{"id":"rln-light-verifiers","metadata":{"permalink":"/rlog/rln-light-verifiers","source":"@site/rlog/2024-05-03-rln-light-verifiers.mdx","title":"Verifying RLN Proofs in Light Clients with Subtrees","description":"How resource-restricted devices can verify RLN proofs fast and efficiently.","date":"2024-05-03T12:00:00.000Z","formattedDate":"May 3, 2024","tags":[],"readingTime":4.81,"hasTruncateMarker":true,"authors":[{"name":"Aaryamann","twitter":"p1ge0nh8er","github":"rymnc","key":"p1ge0nh8er"}],"frontMatter":{"title":"Verifying RLN Proofs in Light Clients with Subtrees","date":"2024-05-03T12:00:00.000Z","authors":"p1ge0nh8er","published":true,"slug":"rln-light-verifiers","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":4},"prevItem":{"title":"RLN-v3: Towards a Flexible and Cost-Efficient Implementation","permalink":"/rlog/rln-v3"},"nextItem":{"title":"Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku","permalink":"/rlog/rln-anonymous-dos-prevention"}},"content":"How resource-restricted devices can verify RLN proofs fast and efficiently.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nRecommended previous reading: [Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku](https://vac.dev/rlog/rln-anonymous-dos-prevention).\\n\\nThis post expands upon ideas described in the previous post,\\nfocusing on how resource-restricted devices can verify RLN proofs fast and efficiently.\\n\\nPreviously, it was required to fetch all the memberships from the smart contract,\\nconstruct the merkle tree locally,\\nand derive the merkle root,\\nwhich is subsequently used to verify RLN proofs.\\n\\nThis process is not feasible for resource-restricted devices since it involves a lot of RPC calls, computation and fault tolerance.\\nOne cannot expect a mobile phone to fetch all the memberships from the smart contract and construct the merkle tree locally.\\n\\n## Constraints and requirements\\n\\nAn alternative solution to the one proposed in this post is to construct the merkle tree on-chain,\\nand have the root accessible with a single RPC call.\\nHowever, this approach increases gas costs for inserting new memberships and _may_ not be feasible until it is optimized further with batching mechanisms, etc.\\n\\nThe other methods have been explored in more depth [here](https://hackmd.io/@rymnc/rln-tree-storages).\\n\\nFollowing are the requirements and constraints for the solution proposed in this post:\\n\\n1. Cheap membership insertions.\\n2. As few RPC calls as possible to reduce startup time.\\n3. Merkle root of the tree is available on-chain.\\n4. No centralized services to sequence membership insertions.\\n5. Map inserted commitments to the block in which they were inserted.\\n\\n## Metrics on sync time for a tree with 2,653 leaves\\n\\nThe following metrics are based on the current implementation of RLN in the Waku gen0 network.\\n\\n### Test bench\\n\\n- Hardware: Macbook Air M2, 16GB RAM\\n- Network: 120 Megabits/sec\\n- Nwaku commit: [e61e4ff](https://github.com/waku-org/nwaku/tree/e61e4ff90a235657a7dc4248f5be41b6e031e98c)\\n- RLN membership set contract: [0xF471d71E9b1455bBF4b85d475afb9BB0954A29c4](https://sepolia.etherscan.io/address/0xF471d71E9b1455bBF4b85d475afb9BB0954A29c4#code)\\n- Deployed block number: 4,230,716\\n- RLN Membership set depth: 20\\n- Hash function: PoseidonT3 (which is a gas guzzler)\\n- Max size of the membership set: 2^20 = 1,048,576 leaves\\n\\n### Metrics\\n\\n- Time to sync the whole tree: 4 minutes\\n- RPC calls: 702\\n- Number of leaves: 2,653\\n\\nOne can argue that the time to sync the tree at the current state is not _that_ bad.\\nHowever, the number of RPC calls is a concern,\\nwhich scales linearly with the number of blocks since the contract was deployed\\nThis is because the implementation fetches all events from the contract,\\nchunking 2,000 blocks at a time.\\nThis is done to avoid hitting the block limit of 10,000 events per call,\\nwhich is a limitation of popular RPC providers.\\n\\n## Proposed solution\\n\\nFrom a theoretical perspective,\\none could construct the merkle tree on-chain,\\nin a view call, in-memory.\\nHowever, this is not feasible due to the gas costs associated with it.\\n\\nTo compute the root of a Merkle tree with $2^{20}$ leaves it costs approximately 2 billion gas.\\nWith Infura and Alchemy capping the gas limit to 350M and 550M gas respectively,\\nit is not possible to compute the root of the tree in a single call.\\n\\nAcknowledging that [Polygon Miden](https://polygon.technology/blog/polygon-miden-state-model) and [Penumbra](https://penumbra.zone/blog/tiered-commitment-tree/) both make use of a tiered commitment tree,\\nwe propose a similar approach for RLN.\\n\\nA tiered commitment tree is a tree which is sharded into multiple smaller subtrees,\\neach of which is a tree in itself.\\nThis allows scaling in terms of the number of leaves,\\nas well as reducing state bloat by just storing the root of a subtree when it is full instead of all its leaves.\\n\\nHere, the question arises:\\nWhat is the maximum number of leaves in a subtree with which the root can be computed in a single call?\\n\\nIt costs approximately 217M gas to compute the root of a Merkle tree with $2^{10}$ leaves.\\n\\nThis is a feasible number for a single call,\\nand hence we propose a tiered commitment tree with a maximum of $2^{10}$ leaves in a subtree and the number of subtrees is $2^{10}$.\\nTherefore, the maximum number of leaves in the tree is $2^{20}$ (the same as the current implementation).\\n\\n![img](/img/light-rln-verifiers.png)\\n\\n### Insertion\\n\\nWhen a commitment is inserted into the tree it is first inserted into the first subtree.\\nWhen the first subtree is full the next insertions go into the second subtree and so on.\\n\\n### Syncing\\n\\nWhen syncing the tree,\\none only needs to fetch the roots of the subtrees.\\nThe root of the full tree can be computed in-memory or on-chain.\\n\\nThis allows us to derive the following relation:\\n\\n$$\\nnumber\\\\_of\\\\_rpc\\\\_calls = number\\\\_of\\\\_filled\\\\_subtrees + 1\\n$$\\n\\nThis is a significant improvement over the current implementation,\\nwhich requires fetching all the memberships from the smart contract.\\n\\n### Gas costs\\n\\nThe gas costs for inserting a commitment into the tree are the same as the current implementation except it consists of an extra SSTORE operation to store the `shardIndex` of the commitment.\\n\\n### Events\\n\\nThe events emitted by the contract are the same as the current implementation,\\nappending the `shardIndex` of the commitment.\\n\\n### Proof of concept\\n\\nA proof of concept implementation of the tiered commitment tree is available [here](https://github.com/vacp2p/rln-contract/pull/37),\\nand is deployed on Sepolia at [0xE7987c70B54Ff32f0D5CBbAA8c8Fc1cAf632b9A5](https://sepolia.etherscan.io/address/0xE7987c70B54Ff32f0D5CBbAA8c8Fc1cAf632b9A5).\\n\\nIt is compatible with the current implementation of the RLN verifier.\\n\\n## Future work\\n\\n1. Optimize the gas costs of the tiered commitment tree.\\n2. Explore using different number of leaves under a given node in the tree (currently set to 2).\\n\\n## Conclusion\\n\\nThe tiered commitment tree is a promising approach to reduce the number of RPC calls required to sync the tree and reduce the gas costs associated with computing the root of the tree.\\nConsequently, it allows for a more scalable and efficient RLN verifier.\\n\\n## References\\n\\n- [RLN Circuits](https://github.com/rate-limiting-nullifier/circom-rln)\\n- [Zerokit](https://github.com/vacp2p/zerokit)\\n- [RLN-V1 RFC](https://rfc.vac.dev/vac/32/rln-v1)\\n- [RLN-V2 RFC](https://rfc.vac.dev/vac/raw/rln-v2)\\n- [RLN Implementers guide](https://hackmd.io/7cBCMU5hS5OYv8PTaW2wAQ?view)\\n- [Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku](https://vac.dev/rlog/rln-anonymous-dos-prevention)"},{"id":"rln-anonymous-dos-prevention","metadata":{"permalink":"/rlog/rln-anonymous-dos-prevention","source":"@site/rlog/2023-11-07-rln-relay.mdx","title":"Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku","description":"Rate Limiting Nullifiers in practice, applied to an anonymous p2p network, like Waku.","date":"2023-11-07T12:00:00.000Z","formattedDate":"November 7, 2023","tags":[],"readingTime":6.77,"hasTruncateMarker":true,"authors":[{"name":"Aaryamann","twitter":"p1ge0nh8er","github":"rymnc","key":"p1ge0nh8er"}],"frontMatter":{"title":"Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku","date":"2023-11-07T12:00:00.000Z","authors":"p1ge0nh8er","published":true,"slug":"rln-anonymous-dos-prevention","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":4},"prevItem":{"title":"Verifying RLN Proofs in Light Clients with Subtrees","permalink":"/rlog/rln-light-verifiers"},"nextItem":{"title":"GossipSub Improvements: Evolution of Overlay Design and Message Dissemination in Unstructured P2P Networks","permalink":"/rlog/GossipSub Improvements"}},"content":"Rate Limiting Nullifiers in practice, applied to an anonymous p2p network, like Waku.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nRate Limiting Nullifier (RLN) is a zero-knowledge gadget that allows users to prove 2 pieces of information,\\n1. They belong to a permissioned membership set\\n2. Their rate of signaling abides by a fixed number that has been previously declared\\n\\nThe \\"membership set\\" introduced above, is in the form of a sparse, indexed merkle tree.\\nThis membership set can be maintained on-chain, off-chain or as a hybrid depending on the network\'s storage costs.\\nWaku makes use of a hybrid membership set, \\nwhere insertions are tracked in a smart contract.\\nIn addition, each Waku node maintains a local copy of the tree, \\nwhich is updated upon each insertion.\\n\\nUsers register themselves with a hash of a locally generated secret, \\nwhich is then inserted into the tree at the next available index.\\nAfter having registered, users can prove their membership by proving their knowledge of the pre-image of the respective leaf in the tree.\\nThe leaf hashes are also referred to as commitments of the respective users.\\nThe actual proof is done by a [Merkle Inclusion Proof](https://ethereum.org/en/developers/tutorials/merkle-proofs-for-offline-data-integrity/), which is a type of ZK proof.\\n\\nThe circuit ensures that the user\'s secret does indeed hash to a leaf in the tree,\\nand that the provided Merkle proof is valid.\\n\\nAfter a User generates this Merkle proof, \\nthey can transmit it to other users, \\nwho can verify the proof.\\nIncluding a message\'s hash within the proof generation, \\nadditionally guarantees integrity of that message.\\n\\nA malicious user could generate multiple proofs per epoch.\\nthey generate multiple proofs per epoch.\\nHowever, when multiple proofs are generated per epoch, \\nthe malicious user\'s secret is exposed, which strongly disincentivizes this attack.\\nThis mechanism is further described in [malicious User secret interpolation mechanism](#malicious-user-secret-interpolation-mechanism)\\n\\nNote: This blog post describes rln-v1, which excludes the range check in favor of a global rate limit for all users,\\nwhich is once per time window. This version is currently in use in waku-rln-relay.\\n\\n## RLN Protocol parameters\\n\\nGiven below is the set of cryptographic primitives, \\nand constants that are used in the RLN protocol. \\n\\n1. Proving System: [`groth16`](https://eprint.iacr.org/2016/260.pdf)\\n2. Elliptic Curve: [`bn254`](https://eprint.iacr.org/2013/879.pdf) (aka bn128) (not to be confused with the 254 bit Weierstrass curve)\\n3. Finite Field: Prime-order subgroup of the group of points on the `bn254` curve\\n4. Default Merkle Tree Height: `20`\\n5. Hashing algorithm: [`Poseidon`](https://eprint.iacr.org/2019/458.pdf)\\n6. Merkle Tree: [`Sparse Indexed Merkle Tree`](https://github.com/rate-limiting-nullifier/pmtree)\\n7. Messages per epoch: `1`\\n8. Epoch duration: `10 seconds`\\n\\n## Malicious User secret interpolation mechanism\\n\\n> note: all the parameters mentioned below are elements in the finite field mentioned above.\\n\\nThe private inputs to the circuit are as follows: -\\n```\\nidentitySecret: the randomly generated secret of the user\\nidentityPathIndex: the index of the commitment derived from the secret\\npathElements: elements included in the path to the index of the commitment\\n```\\n\\nFollowing are the public inputs to the circuit -\\n```\\nx: hash of the signal to the finite field\\nrlnIdentifier: application-specific identifier which this proof is being generated for\\nepoch: the timestamp which this proof is being generated for\\n```\\n\\nThe outputs of the circuit are as follows: -\\n```\\ny: result of Shamir\'s secret sharing calculation\\nroot: root of the Merkle tree obtained after applying the inclusion proof\\nnullifier: uniquely identifies a message, derived from rlnIdentifier, epoch, and the user\'s secret\\n```\\n\\nWith the above data in mind, following is the circuit pseudocode -\\n\\n```\\nidentityCommitment = Poseidon([identitySecret])\\nroot = MerkleInclusionProof(identityCommitment, identityPathIndex, pathElements)\\nexternalNullifier = Poseidon([epoch, rlnIdentifier])\\na1 = Poseidon([identitySecret, externalNullifier])\\ny = identitySecret + a1 * x\\nnullifier = Poseidon([a1])\\n```\\n\\nTo interpolate the secret of a user who has sent multiple signals during the same epoch to the same rln-based application, we may make use of the following formula -\\n\\n$$a_1 = {(y_1 - y_2) \\\\over (x_1 - x_2)}$$\\n\\nwhere $x_1$, $y_1$ and $x_2$, $y_2$ are shares from different messages\\n\\nsubsequently, we may use one pair of the shares, $x_1$ and $y_1$ to obtain the `identitySecret`\\n\\n$$identitySecret = y_1 - a_1 * x$$\\n\\nThis enables RLN to be used for rate limiting with a *global* limit. For arbitrary limits,\\nplease refer to an article written by @curryrasul, [rln-v2](https://mirror.xyz/privacy-scaling-explorations.eth/iCLmH1JVb7fDqp6Mms2NR001m2_n5OOSHsLF2QrxDnQ).\\n\\n\\n## Waku\'s problem with DoS\\n\\nIn a decentralized, privacy focused messaging system like [Waku](https://waku.org),\\nDenial of Service (DoS) vulnerabilities are very common, and must be addressed to promote network scale and optimal bandwidth utilization.\\n\\n### DoS prevention with user metadata\\n\\nThere are a couple of ways a user can be rate-limited, either -\\n1. IP Logging\\n2. KYC Logging\\n\\nBoth IP and KYC logging prevent systems from being truly anonymous, and hence, cannot be used as a valid DoS prevention mechanism for Waku.\\n\\nRLN can be used as an alternative, which provides the best of both worlds, i.e a permissioned membership set, as well as anonymous signaling.\\nHowever, we are bound by k-anonymity rules of the membership set.\\n\\n[Waku-RLN-Relay](https://rfc.vac.dev/waku/standards/core/17/rln-relay) is a [libp2p](https://libp2p.io) pubsub validator that verifies if a proof attached to a given message is valid.\\nIn case the proof is valid, the message is relayed.\\n\\n## Performance analysis\\n\\n> Test bench specs: AMD EPYC 7502P 32-Core, 4x32GB DDR4 Reg.ECC Memory \\n\\nThis simulation was conducted by @alrevuelta, and is described in more detail [here](https://github.com/waku-org/research/issues/23).\\n\\nThe simulation included 100 waku nodes running in parallel.\\n\\nProof generation times - \\n![img](/img/rln-relay-2023-update//proof_generation_time.png)\\n\\nProof verification times -\\n![img](/img/rln-relay-2023-update/proof_verification_time.png)\\n\\nA spammer node publishes 3000 msg/epoch, which is detected by all connected nodes, and subsequently disconnect to prevent further spam -\\n![img](/img/rln-relay-2023-update/spam_prevention_in_action.png)\\n\\n\\n## Security analysis\\n\\n[Barbulescu and Duquesne](https://doi.org/10.1007/s00145-018-9280-5)\\nconclude that that the `bn254` curve has only 100 bits of security.\\nSince the bn254 curve has a small embedding degree,\\nit is vulnerable to the [MOV attack](https://en.wikipedia.org/wiki/MOV_attack).\\nHowever, the MOV attack is only applicable to pairings,\\nand not to the elliptic curve itself.\\nIt is acceptable to use the bn254 curve for RLN,\\nsince the circuit does not make use of pairings.\\n\\n[An analysis](https://github.com/vacp2p/research/issues/155) on the number of rounds in the Poseidon hash function was done,\\nwhich concluded that the hashing rounds should *not* be reduced, \\n\\nThe [smart contracts](https://github.com/vacp2p/rln-contract) have *not* been audited, and are not recommended for real world deployments *yet*.\\n\\n\\n## Storage analysis\\n\\n$$\\ncommitment\\\\_size = 32\\\\ bytes \\\\\\\\\\ntree\\\\_height =20 \\\\\\\\\\ntotal\\\\_leaves = 2^{20} \\\\\\\\ \\nmax\\\\_tree\\\\_size = total\\\\_leaves * commitment\\\\_size \\\\\\\\\\nmax\\\\_tree\\\\_size = 2^{20} * 32 = 33,554,432 \\\\\\\\\\n\u2234max\\\\_tree\\\\_size = 33.55\\\\ megabytes\\n$$\\nThe storage overhead introduced by RLN is minimal.\\nRLN only requires 34 megabytes of storage, which poses no problem on most end-user hardware, with the exception of IoT/microcontrollers.\\nStill, we are working on further optimizations  allowing proof generation without having to store the full tree.\\n\\n## The bare minimum requirements to run RLN\\n\\nWith proof generation time in sub-second latency, along with low storage overhead for the tree,\\nit is possible for end users to generate and verify RLN proofs on a modern smartphone.\\n\\nFollowing is a demo provided by @rramos that demonstrates\\n[waku-rln-relay used in react native](https://drive.google.com/file/d/1ITLYrDOQrHQX2_3Q6O5EqKPYJN8Ye2gF/view?usp=sharing).\\n\\n> Warning: The react native sdk will be deprecated soon, and the above demo should serve as a PoC for RLN on mobiles\\n\\n## RLN usage guide\\n\\n[Zerokit](https://github.com/vacp2p/zerokit) implements api\'s that allow users to handle operations to the tree, \\nas well as generate/verify RLN proofs.\\n\\nOur main implementation of RLN can be accessed via this Rust [crate](https://crates.io/crates/rln),\\nwhich is documented [here](https://docs.rs/rln/0.4.1/rln/public/struct.RLN.html).\\nIt can used in other langugages via the FFI API, which is documented [here](https://docs.rs/rln/0.4.1/rln/ffi/index.html).\\nThe usage of RLN in Waku is detailed in our [RLN Implementers guide](https://hackmd.io/7cBCMU5hS5OYv8PTaW2wAQ?view),\\nwhich provides step-by-step instructions on how to run Waku-RLN-Relay.\\n\\nFollowing is a diagram that will help understand the dependency tree -\\n\\n![rln-dep-tree](/img/rln-relay-2023-update/rln_dep_tree.jpg)\\n\\n## Future work\\n\\n- Optimizations to zerokit for proof generation time.\\n- Incrementing tree depth from 20 to 32, to allow more memberships.\\n- Optimizations to the smart contract.\\n- An ability to signal validity of a message in different time windows.\\n- Usage of proving systems other than Groth16.\\n\\n## References\\n\\n* [RLN Circuits](https://github.com/rate-limiting-nullifier/circom-rln)\\n* [Zerokit](https://github.com/vacp2p/zerokit)\\n* [RLN-V1 RFC](https://rfc.vac.dev/vac/32/rln-v1)\\n* [RLN-V2 RFC](https://rfc.vac.dev/vac/raw/rln-v2)\\n* [RLN Implementers guide](https://hackmd.io/7cBCMU5hS5OYv8PTaW2wAQ?view)\\n* [groth16](https://eprint.iacr.org/2016/260.pdf)\\n* [bn254](https://eprint.iacr.org/2013/879.pdf)\\n* [Poseidon Hash](https://eprint.iacr.org/2019/458.pdf)\\n* [Sparse Indexed Merkle Tree](https://github.com/rate-limiting-nullifier/pmtree)\\n* [Updating key size estimations for pairings](https://doi.org/10.1007/s00145-018-9280-5)"},{"id":"GossipSub Improvements","metadata":{"permalink":"/rlog/GossipSub Improvements","source":"@site/rlog/2023-09-27-gossipimprovements.mdx","title":"GossipSub Improvements: Evolution of Overlay Design and Message Dissemination in Unstructured P2P Networks","description":"GossipSub Improvements: Evolution of Overlay Design and Message Dissemination in Unstructured P2P Networks","date":"2023-11-06T12:00:00.000Z","formattedDate":"November 6, 2023","tags":[],"readingTime":13.815,"hasTruncateMarker":true,"authors":[{"name":"Umar Farooq","github":"ufarooqstatus","key":"farooq"}],"frontMatter":{"title":"GossipSub Improvements: Evolution of Overlay Design and Message Dissemination in Unstructured P2P Networks","date":"2023-11-06T12:00:00.000Z","authors":"farooq","published":true,"slug":"GossipSub Improvements","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Strengthening Anonymous DoS Prevention with Rate Limiting Nullifiers in Waku","permalink":"/rlog/rln-anonymous-dos-prevention"},"nextItem":{"title":"Nescience - A zkVM leveraging hiding properties","permalink":"/rlog/Nescience-A-zkVM-leveraging-hiding-properties"}},"content":"GossipSub Improvements: Evolution of Overlay Design and Message Dissemination in Unstructured P2P Networks \\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Motivitation\\r\\nWe have been recently working on analyzing and improving the performance of the GossipSub protocol for large messages, \\r\\nas in the case of Ethereum Improvement Proposal [EIP-4844](https://eips.ethereum.org/EIPS/eip-4844).\\r\\nThis work led to a comprehensive study of unstructured P2P networks. \\r\\nThe intention was to identify the best practices that can serve as guidelines for performance improvement and scalability of P2P networks.\\r\\n\\r\\n## Introduction\\r\\n\\r\\nNodes in an unstructured p2p network form self-organizing overlay(s) on top of the IP infrastructure to facilitate different services like information dissemination,\\r\\nquery propagation, file sharing, etc. The overlay(s) can be as optimal as a tree-like structure or as enforcing as a fully connected mesh. \\r\\n\\r\\nDue to peer autonomy and a trustless computing environment, some peers may deviate from the expected operation or even leave the network.\\r\\nAt the same time, the underlying IP layer is unreliable. \\r\\n\\r\\nTherefore, tree-like overlays are not best suited for reliable information propagation.\\r\\nMoreover, tree-based solutions usually result in significantly higher message dissemination latency due to suboptimal branches. \\r\\n\\r\\nFlooding-based solutions, on the other hand, result in maximum resilience against adversaries and achieve minimal message dissemination latency because the message propagates through all (including the optimal) paths. \\r\\nRedundant transmissions help maintain the integrity and security of the network in the presence of adversaries and high node failure but significantly increase network-wide bandwidth utilization, cramming the bottleneck links. \\r\\n\\r\\nAn efficient alternative is to lower the number of redundant transmissions by D-regular broadcasting, where a peer will likely receive (or relay) a message from up to $D$ random peers.\\r\\nPublishing through a D-regular overlay triggers approximately $N \\\\times D$ transmissions.\\r\\nReducing $D$ reduces the redundant transmissions but compromises reachability and latency. \\r\\nSharing metadata through a K-regular overlay (where $K > D$) allows nodes to pull missing messages. \\r\\n\\r\\nGossipSub [[1](https://arxiv.org/pdf/2007.02754.pdf)] benefits from full-message (D-regular) and metadata-only (k-regular) overlays.\\r\\nAlternatively, a metadata-only overlay can be used, requiring a pull-based operation that significantly minimizes bandwidth utilization at the cost of increased latency. \\r\\n\\r\\nStriking the right balance between parameters like $D, K$, pull-based operation, etc., can yield application-specific performance tuning, but scalability remains a problem.\\r\\n\\r\\nAt the same time, many other aspects can significantly contribute to the network\'s performance and scalability.\\r\\nOne option is to realize peers\' suitability and continuously changing capabilities while forming overlays. \\r\\n\\r\\nFor instance, a low-bandwidth link near a publisher can significantly demean the entire network\'s performance.\\r\\nReshuffling of peering links according to the changing network conditions can lead to superior performance. \\r\\n\\r\\nLaying off additional responsibilities to more capable nodes (super nodes) can alleviate peer cramming, but it makes the network susceptible to adversaries/peer churn.\\r\\nGrouping multiple super nodes to form virtual node(s) can solve this problem. \\r\\n\\r\\n\\r\\nSimilarly, flat (single-tier) overlays cannot address the routing needs in large (geographically dispersed) networks.\\r\\n\\r\\nHierarchical (Multi-tier) overlays with different intra/inter-overlay routing solutions can better address these needs.\\r\\nMoreover, using message aggregation schemes for grouping multiple messages can save bandwidth and provide better resilience against adversaries/peer churn.\\r\\n\\r\\n\\r\\nThis article\'s primary objective is to investigate the possible choices that can empower an unstructured P2P network to achieve superior performance for the broadest set of applications. \\r\\nWe look into different constraints imposed by application-specific needs (performance goals) and investigate various choices that can augment the network\'s performance. \\r\\nWe explore overlay designs/freshness, peer selection approaches, message-relaying mechanisms, and resilience against adversaries/peer churn. \\r\\nWe consider GossipSub a baseline protocol to explore various possibilities and decisively commit to the ones demonstrating superior performance.\\r\\nWe also discuss the current state and, where applicable, propose a strategic plan for embedding new features to the GossipSub protocol. \\r\\n\\r\\n\\r\\n## GOAL1: Low Latency Operation\\r\\nDifferent applications, like blockchain, streaming, etc., impose strict time bounds on network-wide message dissemination latency. \\r\\nA message delivered after the imposed time bounds is considered as dropped. \\r\\nAn early message delivery in applications like live streaming can further enhance the viewing quality.\\r\\n\\r\\nThe properties and nature of the overlay network topology significantly impact the performance of services and applications executed on top of them. \\r\\nStudying and devising mechanisms for better overlay design and message dissemination is paramount to achieving superior performance.\\r\\n\\r\\nInterestingly, shortest-path message delivery trees have many limitations: \\r\\n\\r\\n1) Changing network dynamics requires a quicker and continuous readjustment of the multicast tree. \\r\\n2) The presence of resource-constrained (bandwidth/compute, etc.) nodes in the overlay can result in congestion. \\r\\n3) Node failure can result in partitions, making many segments unreachable. \\r\\n4) Assuring a shortest-path tree-like structure requires a detailed view of the underlying (and continuously changing) network topology. \\r\\n\\r\\n\\r\\nSolutions involve creating multiple random trees to add redundancy [[2](https://ieeexplore.ieee.org/abstract/document/6267905)].\\r\\nAlternatives involve building an overlay mesh and forwarding messages through the multicast delivery tree (eager push). \\r\\n\\r\\nMetadata is shared through the overlay links so that the nodes can ask for missing messages (lazy push or pull-based operation) through the overlay links. \\r\\nNew nodes are added from the overlay on node failure, but it requires non-faulty node selection.\\r\\n\\r\\nGossipSub uses eager push (through overlay mesh) and lazy push (through IWANT messages). \\r\\n\\r\\nThe mesh degree $D_{Low} \\\\leq D \\\\leq D_{High}$ is crucial in deciding message dissemination latency. \\r\\nA smaller value for $D$ results in higher latency due to increased rounds, whereas a higher $D$ reduces latency on the cost of increased bandwidth. \\r\\nAt the same time, keeping $D$ independent of the growing network size ($N$) may increase network-wide message dissemination latency.\\r\\nAdjusting $D$ with $N$ maintains similar latency on the cost of increased workload for peers. \\r\\nAuthors in [[3](https://infoscience.epfl.ch/record/83478/files/EugGueKerMas04IEEEComp.pdf)] suggest only a logarithmic increase in $D$ to maintain a manageable workload for peers.\\r\\nIn [[4](https://inria.hal.science/tel-02375909/document)], it is reported that the average mesh degree should not exceed $D_{avg} = \\\\ln(N) + C$ for an optimal operation, \\r\\nwhere $C$ is a small constant.\\r\\n\\r\\nMoreover, quicker shuffling of peers results in better performance in the presence of resource-constrained nodes or node failure [[4](https://inria.hal.science/tel-02375909/document)].\\r\\n\\r\\n## GOAL2: Considering Heterogeneity In Overlay Design\\r\\nRandom peering connections in P2P overlays represent a stochastic process. It is inherently difficult to precisely model the performance of such systems. \\r\\nMost of the research on P2P networks provides simulation results assuming nodes with similar capabilities. \\r\\nThe aspect of dissimilar capabilities and resource-constrained nodes is less explored.\\r\\n\\r\\nIt is discussed in GOAL1 that overlay mesh results in better performance if  $D_{avg}$ does not exceed $\\\\ln(N) + C$. \\r\\nEnforcing all the nodes to have approximately $\\\\ln(N) + C$ peers makes resource-rich nodes under-utilized, while resource-constrained nodes are overloaded. \\r\\nAt the same time, connecting high-bandwidth nodes through a low-bandwidth node undermines the network\'s performance.\\r\\nIdeally, the workload on any node should not exceed its available resources.\\r\\nA better solution involves a two-phased operation:\\r\\n\\r\\n\\r\\n1. Every node computes its available bandwidth and selects a node degree $D$ proportional to its available bandwidth [[4](https://inria.hal.science/tel-02375909/document)].\\r\\n    Different bandwidth estimation approaches are suggested in literature [[5](https://ieeexplore.ieee.org/abstract/document/1224454),[6](https://ieeexplore.ieee.org/abstract/document/1248658)]. \\r\\n    Simple bandwidth estimation approaches like variable packet size probing [[6](https://ieeexplore.ieee.org/abstract/document/1248658)] yield similar results with less complexity.\\r\\n    It is also worth mentioning that many nodes may want to allocate only a capped share of their bandwidth to the network. \\r\\n    Lowering $D$ according to the available bandwidth can still prove helpful. \\r\\n    Additionally, bandwidth preservation at the transport layer through approaches like \xb5TP can be useful.\\r\\n    To further conform to the suggested mesh-degree average $D_{avg}$, every node tries achieving this average within its neighborhood, resulting in an overall similar $D_{avg}$.\\r\\n\\r\\n2. From the available local view, every node tries connecting peers with the lowest latency until $D$ connections are made.\\r\\n    We suggest referring to the peering solution discussed in GOAL5 to avoid network partitioning.\\r\\n\\r\\nThe current GossipSub design considers homogeneous peers, and every node tries maintaining $D_{Low} \\\\leq D \\\\leq D_{High}$ connections. \\r\\n\\r\\n\\r\\n## GOAL3: Bandwidth Optimization\\r\\nRedundant message transmissions are essential for handling adversaries/node failure. However, these transmissions result in traffic bursts, cramming many overlay links. \\r\\nThis not only adds to the network-wide message dissemination latency but a significant share of the network\'s bandwidth is wasted on (usually) unnecessary transmissions. \\r\\nIt is essential to explore solutions that can minimize the number of redundant transmissions while assuring resilience against node failures. \\r\\n\\r\\nMany efforts have been made to minimize the impact of redundant transmissions. \\r\\nThese solutions include multicast delivery trees, metadata sharing to enable pull-based operation, in-network information caching, etc. [[7](https://dl.acm.org/doi/abs/10.1145/945445.945473),[8](https://link.springer.com/chapter/10.1007/11558989_12)]. \\r\\nGossipSub employs a hybrid of eager push (message dissemination through the overlay) and lazy push (a pull-based operation by the nodes requiring information through IWANT messages). \\r\\n\\r\\nA better alternative to simple redundant transmission is to use message aggregation [[9](https://ieeexplore.ieee.org/abstract/document/8737576),[10](https://dl.acm.org/doi/abs/10.1145/1993636.1993676),[11](https://ieeexplore.ieee.org/abstract/document/4276446)] for the GossipSub protocol. \\r\\nAs a result, redundant message transmissions can serve as a critical advantage of the GossipSub protocol. \\r\\nSuppose that we have three equal-length messages $x1, x2, x3$. Assuming an XOR coding function,\\r\\nwe know two trivial properties: $x1 \\\\oplus x2 \\\\oplus x2 = x1$ and $\\\\vert x1 \\\\vert = \\\\vert x1 \\\\oplus x2 \\\\oplus x2 \\\\vert$. \\r\\n\\r\\nThis implies that instead of sending messages individually, we can encode and transmit composite message(s) to the network. \\r\\nThe receiver can reconstruct the original message from encoded segments. \\r\\nAs a result, fewer transmissions are sufficient for sending more messages to the network. \\r\\n\\r\\nHowever, sharing linear combinations of messages requires organizing messages in intervals, \\r\\nand devising techniques to identify all messages belonging to each interval.\\r\\nIn addition, combining messages from different publishers requires more complex arrangements, \\r\\ninvolving embedding publisher/message IDs, delayed forwarding (to accommodate more messages), and mechanisms to ensure the decoding of messages at all peers.\\r\\nCareful application-specific need analysis can help decide the benefits against the added complexity. \\r\\n\\r\\n## GOAL4: Handling Large Messages\\r\\nMany applications require transferring large messages for their successful operation. For instance, database/blockchain transactions [[12](https://eips.ethereum.org/EIPS/eip-4844)]. \\r\\nThis introduces two challenges: \\r\\n\\r\\n1) Redundant large message transmissions result in severe network congestion. \\r\\n2) Message transmissions follow a store/forward process at all peers, which is inefficient in the case of large messages. \\r\\n\\r\\nThe above-mentioned challenges result in a noticeable increase in message dissemination latency and bandwidth wastage. \\r\\nMost of the work done for handling large messages involves curtailing redundant transmissions using multicast delivery trees,\\r\\nreducing the number of fanout nodes, employing in-network message caching, pull-based operation, etc.\\r\\n\\r\\nApproaches like message aggregation also prove helpful in minimizing bandwidth wastage.\\r\\n\\r\\nOur recent work on GossipSub improvements (still a work in progress) suggests the following solutions to deal with large message transmissions: \\r\\n\\r\\n1. Using IDontWant message proposal [[13](https://github.com/libp2p/specs/pull/413)] and staggered sending. \\r\\n\\r\\n    IDontWant message helps curtail redundant transmissions by letting other peers know we have already received the message.\\r\\n    Staggered sending enables relaying the message to a short subset of peers in each round.\\r\\n    We argue that simultaneously relaying a message to all peers hampers the effectiveness of the IDontWant message.\\r\\n    Therefore, using the IDontWant message with staggered sending can yield better results by allowing timely reception and processing of IDontWant messages.\\r\\n\\r\\n2. Message transmissions follow a store/forward process at all peers that is inefficient in the case of large messages.\\r\\n    We can parallelize message transmission by partitioning large messages into smaller fragments, letting intermediate peers relay these fragments as soon as they receive them.\\r\\n\\r\\n\\r\\n## GOAL5: Scalability\\r\\nP2P networks are inherently scalable because every incoming node brings in bandwidth and compute resources.\\r\\nIn other words, we can keep adding nodes to the network as long as every incoming node brings at-least $R \\\\times D$ bandwidth, \\r\\nwhere $R$ is average data arrival rate. \\r\\nIt is worth mentioning that network-wide message dissemination requires at-least $\\\\lceil \\\\log_D (N) \\\\rceil$ hops. \\r\\nTherefore, increasing network size increases message dissemination latency, assuming D is independent of the network size.\\r\\n\\r\\nAdditionally, problems like peer churn, adversaries, heterogeneity, distributed operation, etc., significantly hamper the network\'s performance.\\r\\nMost efforts for bringing scalability to the P2P systems have focused on curtailing redundant transmissions and flat overlay adjustments.\\r\\nHierarchical overlay designs, on the other hand, are less explored.\\r\\n\\r\\nPlacing a logical structure in unstructured P2P systems can help scale P2P networks. \\r\\n\\r\\nOne possible solution is to use a hierarchical overlay inspired by the approaches [[14](https://link.springer.com/article/10.1007/s12083-016-0460-5),[15](https://link.springer.com/chapter/10.1007/978-3-030-19223-5_16),[16](https://ieeexplore.ieee.org/abstract/document/9826458)]. \\r\\nAn abstract operation of such overlay design is provided below:\\r\\n\\r\\n1. Clustering nodes based on locality, assuming that such peers will have relatively lower intra-cluster latency and higher bandwidth. \\r\\n    For this purpose, every node tries connecting peers with the lowest latency until $D$ connections are made or the cluster limit is reached.\\r\\n\\r\\n2. A small subset of nodes having the highest bandwidth and compute resources is selected from each cluster. \\r\\n    These super nodes form a fully connected mesh and jointly act as a virtual node, \\r\\n    mitigating the problem of peer churn among super nodes.\\r\\n    \\r\\n3. Virtual nodes form a fully connected mesh to construct a hierarchical overlay. \\r\\n    Each virtual node is essentially a collection of super nodes; \\r\\n    a link to any of the constituent super nodes represents a link to the virtual node.\\r\\n\\r\\n4. One possible idea is to use GossipSub for intra-cluster message dissemination and FloodSub for inter-cluster message dissemination.\\r\\n\\r\\n## Summary\\r\\nOverlay acts as a virtual backbone for a P2P network. A flat overlay is more straightforward and allows effortless readjustment to application needs. \\r\\nOn the other hand, a hierarchical overlay can bring scalability at the cost of increased complexity. \\r\\nRegardless of the overlay design, a continuous readjustment to appropriate peering links is essential for superior performance. \\r\\nAt the same time, bandwidth preservation (through message aggregation, caching at strategic locations, metadata sharing, pull-based operation, etc.) can help minimize latency.\\r\\nHowever, problems like peer churn and in-network adversaries can be best alleviated through balanced redundant coverage, and frequent reshuffling of the peering links.\\r\\n\\r\\n# References\\r\\n\\r\\n* [1] D. Vyzovitis, Y. Napora, D. McCormick, D. Dias, and Y. Psaras, \u201cGossipsub: Attack-resilient message propagation in the filecoin and eth2. 0 networks,\u201d arXiv preprint arXiv:2007.02754, 2020. Retrieved from https://arxiv.org/pdf/2007.02754.pdf\\r\\n* [2] M. Matos, V. Schiavoni, P. Felber, R. Oliveira, and E. Riviere, \u201cBrisa: Combining efficiency and reliability in epidemic data dissemination,\u201d in 2012 IEEE 26th International Parallel and Distributed Processing Symposium. IEEE, 2012, pp. 983\u2013994. Retrieved from https://ieeexplore.ieee.org/abstract/document/6267905\\r\\n* [3] P. T. Eugster, R. Guerraoui, A. M. Kermarrec, and L. Massouli, \u201cEpidemic information dissemination in distributed systems,\u201d IEEE Computer, vol. 37, no. 5, 2004.  Retrieved from https://infoscience.epfl.ch/record/83478/files/EugGueKerMas04IEEEComp.pdf\\r\\n* [4] D. Frey, \u201cEpidemic protocols: From large scale to big data,\u201d Ph.D. dissertation, Universite De Rennes 1, 2019. Retrieved from https://inria.hal.science/tel-02375909/document\\r\\n* [5] M. Jain and C. Dovrolis, \u201cEnd-to-end available bandwidth: measurement methodology, dynamics, and relation with tcp throughput,\u201d IEEE/ACM Transactions on networking, vol. 11, no. 4, pp. 537\u2013549, 2003. Retrieved from https://ieeexplore.ieee.org/abstract/document/1224454\\r\\n* [6] R. Prasad, C. Dovrolis, M. Murray, and K. Claffy, \u201cBandwidth estimation: metrics, measurement techniques, and tools,\u201d IEEE network, vol. 17, no. 6, pp. 27\u201335, 2003. Retrieved from https://ieeexplore.ieee.org/abstract/document/1248658\\r\\n* [7] D. Kostic, A. Rodriguez, J. Albrecht, and A. Vahdat, \u201cBullet: High bandwidth data dissemination using an overlay mesh,\u201d in Proceedings of the nineteenth ACM symposium on Operating systems principles, 2003, pp. 282\u2013297. Retrieved from https://dl.acm.org/doi/abs/10.1145/945445.945473\\r\\n* [8] V. Pai, K. Kumar, K. Tamilmani, V. Sambamurthy, and A. E. Mohr, \u201cChainsaw: Eliminating trees from overlay multicast,\u201d in Peer-to-Peer Systems IV: 4th International Workshop, IPTPS 2005, Ithaca, NY, USA, February 24-25, 2005. Revised Selected Papers 4. Springer, 2005, pp. 127\u2013140. Retrieved from https://link.springer.com/chapter/10.1007/11558989_12\\r\\n* [9] Y.-D. Bromberg, Q. Dufour, and D. Frey, \u201cMultisource rumor spreading with network coding,\u201d in IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE, 2019, pp. 2359\u20132367. Retrieved from https://ieeexplore.ieee.org/abstract/document/8737576\\r\\n* [10] B. Haeupler, \u201cAnalyzing network coding gossip made easy,\u201d in Proceedings of the forty-third annual ACM symposium on Theory of computing, 2011, pp. 293\u2013302. Retrieved from https://dl.acm.org/doi/abs/10.1145/1993636.1993676\\r\\n* [11] S. Yu and Z. Li, \u201cMassive data delivery in unstructured peer-to-peer networks with network coding,\u201d in 6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007). IEEE, 2007, pp. 592\u2013597. Retrieved from https://ieeexplore.ieee.org/abstract/document/4276446\\r\\n* [12] V. Buterin, D. Feist, D. Loerakker, G. Kadianakis, M. Garnett, M. Taiwo, and A. Dietrichs, \u201cEip-4844: Shard blob transactions scale data-availability of ethereum in a simple, forwards-compatible manner,\u201d 2022. Retrieved from https://eips.ethereum.org/EIPS/eip-4844\\r\\n* [13] A. Manning, \u201cGossipsub extension for epidemic meshes (v1.2.0),\u201d 2022. Retrieved from https://github.com/libp2p/specs/pull/413\\r\\n* [14] Z. Duan, C. Tian, M. Zhou, X. Wang, N. Zhang, H. Du, and L. Wang, \u201cTwo-layer hybrid peer-to-peer networks,\u201d Peer-to-Peer Networking and Applications, vol. 10, pp. 1304\u20131322, 2017. Retrieved from https://link.springer.com/article/10.1007/s12083-016-0460-5\\r\\n* [15] W. Hao, J. Zeng, X. Dai, J. Xiao, Q. Hua, H. Chen, K.-C. Li, and H. Jin, \u201cBlockp2p: Enabling fast blockchain broadcast with scalable peer-to-peer network topology,\u201d in Green, Pervasive, and Cloud Computing: 14th International Conference, GPC 2019, Uberlandia, Brazil, May 26\u201328, 2019, Proceedings 14. Springer, 2019, pp. 223\u2013237. Retrieved from https://link.springer.com/chapter/10.1007/978-3-030-19223-5_16\\r\\n* [16] H. Qiu, T. Ji, S. Zhao, X. Chen, J. Qi, H. Cui, and S. Wang, \u201cA geography-based p2p overlay network for fast and robust blockchain systems,\u201d IEEE Transactions on Services Computing, 2022. Retrieved from https://ieeexplore.ieee.org/abstract/document/9826458"},{"id":"Nescience-A-zkVM-leveraging-hiding-properties","metadata":{"permalink":"/rlog/Nescience-A-zkVM-leveraging-hiding-properties","source":"@site/rlog/2023-08-28-Nescience.mdx","title":"Nescience - A zkVM leveraging hiding properties","description":"Nescience, a privacy-first blockchain zkVM.","date":"2023-08-28T12:00:00.000Z","formattedDate":"August 28, 2023","tags":[],"readingTime":31.34,"hasTruncateMarker":true,"authors":[{"name":"Moudy","github":"moudyellaz","key":"moudy"}],"frontMatter":{"title":"Nescience - A zkVM leveraging hiding properties","date":"2023-08-28T12:00:00.000Z","authors":"moudy","published":true,"slug":"Nescience-A-zkVM-leveraging-hiding-properties","categories":"research","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"GossipSub Improvements: Evolution of Overlay Design and Message Dissemination in Unstructured P2P Networks","permalink":"/rlog/GossipSub Improvements"},"nextItem":{"title":"Device Pairing in Js-waku and Go-waku","permalink":"/rlog/device-pairing-in-js-waku-and-go-waku"}},"content":"Nescience, a privacy-first blockchain zkVM. \\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nNescience is a privacy-first blockchain project that aims to enable private transactions and provide a general-purpose execution environment for classical applications. \\nThe goals include creating a state separation architecture for public/private computation, \\ndesigning a versatile virtual machine based on mainstream instruction sets, \\ncreating proofs for private state updates, implementing a kernel-based architecture for correct execution of private functions, \\nand implementing core DeFi protocols such as AMMs and staking from a privacy perspective. \\n\\nIt intends to create a user experience that is similar to public blockchains, but with additional privacy features that users can leverage at will. \\nTo achieve this goal, Nescience will implement a versatile virtual machine that can be used to implement existing blockchain applications, \\nwhile also enabling the development of privacy-centric protocols such as private staking and private DEXs.\\n\\nTo ensure minimal trust assumptions and prevent information leakage, Nescience proposes a proof system that allows users to create proofs for private state updates, \\nwhile the verification of the proofs and the execution of the public functions inside the virtual machine can be delegated to an external incentivised prover. \\n\\nIt also aims to implement a seamless interaction between public and private state, enabling composability between contracts, and private and public functions. \\nFinally, Nescience intends to implement permissive licensing, which means that the source code will be open-source, \\nand developers will be able to use and modify the code without any restriction.\\n\\nOur primary objective is the construction of the Zero-Knowledge Virtual Machine (zkVM). This document serves as a detailed exploration of the multifaceted challenges, \\npotential solutions, and alternatives that lay ahead. Each step is a testament to our commitment to thoroughness; \\nwe systematically test various possibilities and decisively commit to the one that demonstrates paramount performance and utility. \\nFor instance, as we progress towards achieving Goal 2, we are undertaking a rigorous benchmarking of the Nova proof system against its contemporaries. \\nShould Nova showcase superior performance metrics, we stand ready to integrate it as our proof system of choice. Through such meticulous approaches, \\nwe not only reinforce the foundation of our project but also ensure its scalability and robustness in the ever-evolving landscape of blockchain technology.\\n\\n\\n## Goal 1: Create a State Separation Architecture\\n\\nThe initial goal revolves around crafting a distinctive architecture that segregates public and private computations, \\nemploying an account-based framework for the public state and a UTXO-based structure for the private state. \\n\\nThe UTXO model [[1]( https://bitcoin.org/bitcoin.pdf),[2](https://iohk.io/en/blog/posts/2021/03/11/cardanos-extended-utxo-accounting-model/)], notably utilized in Bitcoin, generates new UTXOs to serve future transactions, \\nwhile the account-based paradigm assigns balances to accounts that transactions can modify. \\nAlthough the UTXO model bolsters privacy by concealing comprehensive balances, \\nthe pursuit of a dual architecture mandates a meticulous synchronization of these state models, \\nensuring that private transactions remain inconspicuous in the wider public network state. \\n\\nThis task is further complicated by the divergent transaction processing methods intrinsic to each model, \\nnecessitating a thoughtful and innovative approach to harmonize their functionality. \\nTo seamlessly bring together the dual architecture, harmonizing the account-based model for public state with the UTXO-based model for private state, \\na comprehensive strategy is essential.\\n\\nThe concept of blending an account-based structure with a UTXO-based model for differentiating between public and private states is intriguing. \\nIt seeks to leverage the strengths of both models: the simplicity and directness of the account-based model with the privacy enhancements of the UTXO model.\\n\\nHere\'s a breakdown and a potential strategy for harmonizing these models:\\n\\n### <ins> Rationale Behind the Dual Architecture: </ins>\\n\\n* **Account-Based Model:** This model is intuitive and easy to work with. Every participant has an account, \\nand transactions directly modify the balances of these accounts. It\'s conducive for smart contracts and a broad range of applications.\\n\\n* **UTXO-Based Model:** This model treats every transaction as a new output, which can then be used as an input for future transactions. \\nBy not explicitly associating transaction outputs with user identities, it offers a degree of privacy.\\n\\n### <ins> Harmonizing the Two Systems: </ins>\\n\\n1. Translation Layer\\n\\n    * Role: Interface between UTXO and account-based states.\\n\\n    * _UTXO-to-Account Adapter:_  When UTXOs are spent, the adapter can translate these into the corresponding account balance modifications. \\n    This could involve creating a temporary \'pseudo-account\' that mirrors the \\n    UTXO\'s attributes.\\n\\n    * _Account-to-UTXO Adapter:_ When an account wishes to make a private transaction, \\n    it would initiate a process converting a part of its balance to a UTXO, facilitating a privacy transaction.\\n\\n2. Unified Identity Management\\n\\n    * Role: Maintain a unified identity (or address) system that works across both state models, \\n    allowing users to easily manage their public and private states without requiring separate identities.\\n\\n    * _Deterministic Wallets:_ Use Hierarchical Deterministic (HD) wallets [[3](https://medium.com/mycrypto/the-journey-from-mnemonic-phrase-to-address-6c5e86e11e14),[4](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki)], enabling users to generate multiple addresses (both UTXO and account-based) from a single seed. \\n     This ensures privacy while keeping management centralized for the user.\\n\\n\\n3. State Commitments\\n\\n    * Role: Use cryptographic commitments to commit to the state of both models. This can help in efficiently validating cross-model transactions.\\n   \\n    * _Verkle Trees:_ Verkle Trees combine Vector Commitment and the KZG polynomial commitment scheme to produce a structure that\'s efficient in terms of both proofs and verification.\\n    Verkle proofs are considerably small in size (less data to store and transmit), where Transaction and state verifications can be faster due to the smaller proof sizes and computational efficiencies.\\n\\n    * _Mimblewimble-style Aggregation_ [[5](https://github.com/mimblewimble/grin/blob/master/doc/intro.md)]: For UTXOs, techniques similar to those used in Mimblewimble can be used to aggregate transactions, keeping the state compact and enhancing privacy.\\n\\n\\n4. Batch Processing & Anonymity Sets\\n\\n    * Role: Group several UTXO-based private transactions into a single public account-based transaction. \\n    This can provide a level of obfuscation and can make synchronization between the two models more efficient.\\n\\n    * _CoinJoin Technique_ [[6](https://en.bitcoin.it/wiki/CoinJoin)]: As seen in Bitcoin, multiple users can combine their UTXO transactions into one, enhancing privacy.\\n\\n    * _Tornado Cash Principle_ [[7](https://github.com/tornadocash/tornado-classic-ui)]: For account-based systems wanting to achieve privacy, methods like those used in Tornado Cash can be implemented, \\n    providing zk-SNARKs-based private transactions.\\n\\n5. Event Hooks & Smart Contracts\\n\\n    * Role: Implement event-driven mechanisms that trigger specific actions in one model based on events in the other. \\n    For instance, a private transaction (UTXO-based) can trigger a corresponding public notification or event in the account-based model.\\n\\n    * _Conditional Execution:_ Smart contracts could be set to execute based on events in the UTXO system. For instance, \\n    a smart contract might release funds (account-based) once a specific UTXO is spent.\\n\\n    * _Privacy Smart Contracts:_ Using zk-SNARKs or zk-STARKs to bring privacy to the smart contract layer, \\n    allowing for private logic execution.\\n\\n\\n### <ins> Challenges and Solutions </ins>\\n\\n1. Synchronization Overhead\\n\\n    * Challenge: Combining two distinct transaction models creates an inherent synchronization challenge.\\n\\n    * State Channels: By allowing transactions to be conducted off-chain between participants, state channels can alleviate synchronization stresses. \\n    Only the final state needs to be settled on-chain, drastically reducing the amount of data and frequency of updates required.\\n\\n    * Sidechains: These act as auxiliary chains to the main blockchain. Transactions can be processed on the sidechain and then periodically synced with the main chain. \\n    This structure helps reduce the immediate load on the primary system.\\n\\n    * Checkpointing: Introduce periodic checkpoints where the two systems\' states are verified and harmonized. \\n    This can ensure consistency without constant synchronization.\\n\\n2. Double Spending\\n\\n    * Challenge: With two models operating in tandem, there\'s an increased risk of double-spending attacks.\\n\\n    * Multi-Signature Transactions: Implementing transactions that require signatures from both systems can prevent unauthorized movements.\\n\\n    * Cross-Verification Mechanisms: Before finalizing a transaction, it undergoes verification in both UTXO and account-based systems. \\n    If discrepancies arise, the transaction can be halted.\\n\\n    * Timestamping: By attaching a timestamp to each transaction, it\'s possible to order them sequentially, making it easier to spot and prevent double spending.\\n\\n3. Complexity in User Experience\\n\\n    * Challenge: The dual model, while powerful, is inherently complex.\\n\\n    * Abstracted User Interfaces: Design UIs that handle the complexity behind the scenes, \\n    allowing users to make transactions without needing to understand the nuances of the dual model.\\n\\n    * Guided Tutorials: Offer onboarding tutorials to acquaint users with the system\'s features, \\n    especially emphasizing when and why they might choose one transaction type over the other.\\n\\n    * Feedback Systems: Implement systems where users can provide feedback on any complexities or challenges they encounter. \\n    This real-time feedback can be invaluable for iterative design improvements.\\n\\n4. Security\\n\\n    * Challenge: Merging two systems can introduce unforeseen vulnerabilities.\\n\\n    * Threat Modeling: Regularly conduct threat modeling exercises to anticipate potential attack vectors, \\n    especially those that might exploit the interaction between the two systems.\\n\\n    * Layered Security Protocols: Beyond regular audits, introduce multiple layers of security checks. \\n    Each layer can act as a fail-safe if a potential threat bypasses another.\\n\\n    * Decentralized Watchtowers: These are third-party services that monitor the network for malicious activities. \\n    If any suspicious activity is detected, they can take corrective measures or raise alerts.\\n\\n5. Gas & Fee Management:\\n\\n    * Challenge: A dual model can lead to convoluted fee structures.\\n\\n    * Dynamic Fee Adjustment: Implement algorithms that adjust fees based on network congestion and transaction type. \\n    This can ensure fairness and prevent network abuse.\\n\\n    * Fee Estimation Tools: Provide tools that can estimate fees before a transaction is initiated. \\n    This helps users understand potential costs upfront.\\n\\n    * Unified Gas Stations: Design platforms where users can purchase or allocate gas for both transaction types simultaneously, \\n    simplifying the gas acquisition process.\\n\\n\\nBy addressing these challenges head-on with a detailed and systematic approach, it\'s possible to unlock the full potential of a dual-architecture system, \\ncombining the strengths of both UTXO and account-based models without their standalone limitations.\\n\\n\\n| Aspect                 \\t| Details                                                                                                                                                                                                                                                                                                                                                                                                        \\t|\\n|------------------------\\t|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\t|\\n| **Harmony**            \\t| - **Advanced VM Development:** Design tailored for private smart contracts. - **Leverage Established Architectures:** Use WASM or RISC-V to harness their versatile and encompassing nature suitable for zero-knowledge applications. - **Support for UTXO & Account-Based Models:** Enhance adaptability across various blockchain structures.                                                                \\t|\\n| **Challenges**         \\t| - **Adaptation Concerns:** WASM and RISC-V weren\'t designed with zero-knowledge proofs as a primary focus, posing integration challenges. - **Complexities with Newer Systems:** Systems like (Super)Nova, STARKs, and Sangria are relatively nascent, adding another layer of intricacy to the integration. - **Optimization Concerns:** Ensuring that these systems are optimized for zero-knowledge proofs. \\t|\\n| **Proposed Solutions** \\t| - **Integration of Nova:** Consider Nova\'s proof system for its potential alignment with project goals. - **Comprehensive Testing:** Rigorously test and benchmark against alternatives like Halo2, Plonky, and Starky to validate choices. - **Poseidon Recursion Technique:** To conduct exhaustive performance tests, providing insights into each system\'s efficiency and scalability.                     \\t|\\n\\n\\n## Goal 2: Virtual Machine Creation\\n\\nThe second goal entails the creation of an advanced virtual machine by leveraging established mainstream instruction sets like WASM or RISC-V. \\nAlternatively, the objective involves pioneering a new, specialized instruction set meticulously optimized for Zero-Knowledge applications.\\n\\nThis initiative seeks to foster a versatile and efficient environment for executing computations within the privacy-focused context of the project. \\nBoth WASM and RISC-V exhibit adaptability to both UTXO and account-based models due to their encompassing nature as general-purpose instruction set architectures.\\n\\n _WASM_, operating as a low-level virtual machine, possesses the capacity to execute code derived from a myriad of high-level programming languages, \\n and boasts seamless integration across diverse blockchain platforms. \\n \\n Meanwhile, _RISC-V_ emerges as a versatile option, accommodating both models, and can be seamlessly integrated with secure enclaves like SGX or TEE, \\n elevating the levels of security and privacy. However, it is crucial to acknowledge that employing WASM or RISC-V might present challenges, \\n given their original design without specific emphasis on optimizing for Zero-Knowledge Proofs (ZKPs). \\n \\n Further complexity arises with the consideration of more potent proof systems like (Super)Nova, STARKs, and Sangria, which, \\n while potentially addressing optimization concerns, necessitate extensive research and testing due to their relatively nascent status within the field. \\n This accentuates the need for a judicious balance between established options and innovative solutions in pursuit of an architecture harmoniously amalgamating privacy, security, and performance.\\n\\nThe ambition to build a powerful virtual machine tailored to zero-knowledge (ZK) applications is both commendable and intricate. \\nThe combination of two renowned instruction sets, WASM and RISC-V, in tandem with ZK, is an innovation that could redefine privacy standards in blockchain. \\nLet\'s dissect the challenges and possibilities inherent in this goal:\\n\\n1. Established Mainstream Instruction Sets - WASM and RISC-V\\n\\n    * Strengths:\\n\\n        * _WASM_: Rooted in its ability to execute diverse high-level language codes, its potential for cross-chain compatibility makes it a formidable contender. \\n        Serving as a low-level virtual machine, its role in the blockchain realm is analogous to that of the Java Virtual Machine in the traditional computing landscape.\\n\\n        * _RISC-V_: This open-standard instruction set architecture has made waves due to its customizable nature. \\n        Its adaptability to both UTXO and account-based structures coupled with its compatibility with trusted execution environments like SGX and TEE augments its appeal, \\n        especially in domains that prioritize security and privacy.\\n\\n    * Challenges: Neither WASM nor RISC-V was primarily designed with ZKPs in mind. While they offer flexibility, \\n    they might lack the necessary optimizations for ZK-centric tasks. Adjustments to these architectures might demand intensive R&D efforts.\\n\\n\\n\\n2. Pioneering a New, Specialized Instruction Set\\n\\n    * Strengths: A bespoke instruction set can be meticulously designed from the ground up with ZK in focus, \\n    potentially offering unmatched performance and optimizations tailored to the project\'s requirements.\\n    \\n    * Challenges: Crafting a new instruction set is a monumental task requiring vast resources, including expertise, time, and capital.\\n     It would also need to garner community trust and support over time.\\n\\n\\n\\n3. Contemporary Proof Systems - (Super)Nova, STARKs, Sangria\\n\\n    * Strengths: These cutting-edge systems, being relatively new, might offer breakthrough cryptographic efficiencies that older systems lack: designed with modern challenges in mind, \\n    they could potentially bridge the gap where WASM and RISC-V might falter in terms of ZKP optimization.\\n\\n    * Challenges: Their nascent nature implies a dearth of exhaustive testing, peer reviews, and potentially limited community support. \\n    The unknowns associated with these systems could introduce unforeseen vulnerabilities or complexities. \\n    While they could offer optimizations that address challenges presented by WASM and RISC-V, their young status demands rigorous vetting and testing.\\n\\n\\n<center>\\n\\n|                    | Mainstream (WASM, RISC-V) | ZK-optimized (New Instruction Set) |\\n|:------------------:|:-------------------------:|:----------------------------------:|\\n|  Existing Tooling  |            YES            |                 NO                 |\\n| Blockchain-focused |             NO            |                 YES                |\\n|     Performant     |          DEPENDS          |                 YES                |\\n\\n</center>\\n\\n### <ins> Optimization Concerns for WASM and RISC-V: </ins>\\n\\n* _Cryptography Libraries_: ZKP applications rely heavily on specific cryptographic primitives. Neither WASM nor RISC-V natively supports all of these primitives. \\nThus, a comprehensive library of cryptographic functions, optimized for these platforms, needs to be developed.\\n\\n* _Parallel Execution_: Given the heavy computational demands of ZKPs, leveraging parallel processing capabilities can optimize the time taken. \\nBoth WASM and RISC-V would need modifications to handle parallel execution of ZKP processes efficiently.\\n\\n* _Memory Management_: ZKP computations can sometimes require significant amounts of memory, especially during the proof generation phase. \\nFine-tuned memory management mechanisms are essential to prevent bottlenecks.\\n\\n\\n### <ins> Emerging ZKP Optimized Systems Considerations: </ins>\\n\\n* _Proof Size_: Different systems generate proofs of varying sizes. A smaller proof size is preferable for blockchain applications to save on storage and bandwidth. \\nThe trade-offs between proof size, computational efficiency, and security need to be balanced.\\n\\n* _Universality_: Some systems can support any computational statement (universal), while others might be tailored to specific tasks. \\nA universal system can be more versatile for diverse applications on the blockchain.\\n\\n* _Setup Requirements_: Certain ZKP systems, like zk-SNARKs, require a trusted setup, which can be a security concern. \\nAlternatives like zk-STARKs don\'t have this requirement but come with other trade-offs.\\n\\n\\n### <ins> Strategies for Integration: </ins>\\n\\n* _Iterative Development_: Given the complexities, an iterative development approach can be beneficial. \\nStart with a basic integration of WASM or RISC-V for general tasks and gradually introduce specialized ZKP functionalities.\\n\\n* _Benchmarking_: Establish benchmark tests specifically for ZKP operations. This will provide continuous feedback on the performance of the system as modifications are made, ensuring optimization.\\n\\n* _External Audits & Research_: Regular checks from cryptographic experts and collaboration with academic researchers can help in staying updated and ensuring secure implementations.\\n\\n\\n## Goal 3: Proofs Creation and Verification\\n\\nThe process of generating proofs for private state updates is vested in the hands of the user, aligning with our commitment to minimizing trust assumptions and enhancing privacy. \\nConcurrently, the responsibility of verifying these proofs and executing public functions within the virtual machine can be effectively delegated to an external prover, \\na role that is incentivized to operate with utmost honesty and integrity. This intricate balance seeks to safeguard against information leakage, \\npreserving the confidentiality of private transactions. Integral to this mechanism is the establishment of a robust incentivization framework.\\n\\nTo ensure the prover\u2019s steadfast commitment to performing tasks with honesty, we should introduce a mechanism that facilitates both rewards for sincere behavior and penalties for any deviation from the expected standards. \\nThis two-pronged approach serves as a compelling deterrent against dishonest behavior and fosters an environment of accountability. \\nIn addition to incentivization, a crucial consideration is the economic aspect of verification and execution. \\nThe verification process has been intentionally designed to be more cost-effective than execution. \\n\\nThis strategic approach prevents potential malicious actors from exploiting the system by flooding it with spurious proofs, a scenario that could arise when the costs align favorably. \\nBy maintaining a cost balance that favors verification, we bolster the system\u2019s resilience against fraudulent activities while ensuring its efficiency. \\nIn sum, our multifaceted approach endeavors to strike an intricate equilibrium between user-initiated proof creation, external verification, and incentivization. \\nThis delicate interplay of mechanisms ensures a level of trustworthiness that hinges on transparency, accountability, and economic viability.\\n\\nAs a result, we are poised to cultivate an ecosystem where users\u2019 privacy is preserved, incentives are aligned, \\nand the overall integrity of the system is fortified against potential adversarial actions. To achieve the goals of user-initiated proof creation, \\nexternal verification, incentivization, and cost-effective verification over execution, several options and mechanisms can be employed:\\n\\n1. **User-Initiated Proof Creation:** Users are entrusted with the generation of proofs for private state updates, thus ensuring greater privacy and reducing trust dependencies.\\n\\n    * Challenges:\\n\\n        * Maintaining the quality and integrity of the proofs generated by users.\\n\\n         * Ensuring that users have the tools and knowledge to produce valid proofs.\\n\\n    * Solutions:\\n\\n        * Offer extensive documentation, tutorials, and user-friendly tools to streamline the proof-generation process.\\n\\n        * Implement checks at the verifier\'s end to ensure the quality of proofs.\\n\\n\\n2. **External Verification by Provers:** An external prover verifies the proofs and executes public functions within the virtual machine.\\n\\n    * Challenges:\\n\\n        * Ensuring that the external prover acts honestly.\\n\\n        * Avoiding centralized points of failure.\\n    \\n    * Solutions:\\n\\n        * Adopt a decentralized verification approach, with multiple provers cross-verifying each other\u2019s work.\\n\\n        * Use reputation systems to rank provers based on their past performances, creating a trust hierarchy.\\n\\n3. ** Incentivization Framework:** A system that rewards honesty and penalizes dishonest actions, ensuring provers\' commitment to the task.\\n\\n    * Challenges:\\n\\n        * Determining the right balance of rewards and penalties.\\n\\n        * Ensuring that the system cannot be gamed for undue advantage.\\n\\n    * Solutions[^1]: \\n\\n        * Implement a dynamic reward system that adjusts based on network metrics and provers\' performance.\\n\\n        * Use a staking mechanism where provers need to lock up a certain amount of assets. \\n        Honest behavior earns rewards, while dishonest behavior could lead to loss of staked assets.\\n\\n4. **Economic Viability through Cost Dynamics:** Making verification more cost-effective than execution to deter spamming and malicious attacks.\\n\\n    * Challenges: \\n\\n        * Setting the right cost metrics for both verification and execution.\\n\\n        * Ensuring that genuine users aren\u2019t priced out of the system.\\n\\n    * Solutions:\\n\\n        * Use a dynamic pricing model, adjusting costs in real-time based on network demand.\\n\\n        * Implement gas-like mechanisms to differentiate operation costs and ensure fairness.\\n\\n5. **  Maintaining Trustworthiness:** Create a system that\'s transparent, holds all actors accountable, and is economically sound.\\n\\n    * Challenges: \\n\\n        * Keeping the balance where users feel their privacy is intact, while provers feel incentivized.\\n\\n        * Ensuring the system remains resilient against adversarial attacks.\\n\\n    * Solutions:\\n\\n        * Implement layered checks and balances.\\n\\n        * Foster community involvement, allowing them to participate in decision-making, potentially through a decentralized autonomous organization (DAO).\\n\\nEach of these options can be combined or customized to suit the specific requirements of your project, striking a balance between user incentives, \\ncost dynamics, and verification integrity. A thoughtful combination of these mechanisms ensures that the system remains robust, resilient, \\nand conducive to the objectives of user-initiated proof creation, incentivized verification, and cost- effective validation.\\n\\n<center>\\n\\n| Aspect                        \\t| Details                                                                                                                                                                                                                      \\t|\\n|-------------------------------\\t|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\t|\\n| **Design Principle**          \\t| - **User Responsibility:** Generating proofs for private state updates. - **External Prover:** Delegated the task of verifying proofs and executing public VM functions.                                                     \\t|\\n| **Trust & Privacy**           \\t| - **Minimized Trust Assumptions:** Place proof generation in users\' hands. - **Enhanced Privacy:** Ensure confidentiality of private transactions and prevent information leakage.                                           \\t|\\n| **Incentivization Framework** \\t| - **Rewards:** Compensate honest behavior. - **Penalties:** Deter and penalize dishonest behavior.                                                                                                                           \\t|\\n| **Economic Considerations**   \\t| - **Verification vs. Execution:** Make verification more cost-effective than execution to prevent spurious proofs flooding. - **Cost Balance:** Strengthen resilience against fraudulent activities and maintain efficiency. \\t|\\n| **Outcome**                   \\t| An ecosystem where: - Users\' privacy is paramount. - Incentives are appropriately aligned. - The system is robust against adversarial actions.                                                                               \\t|\\n\\n</center>\\n\\n[^1]: Incentive Mechanisms:\\n    * Token Rewards: Design a token-based reward system where honest provers are compensated with tokens for their verification services. \\n    This incentivizes participation and encourages integrity.\\n   \\n    * Staking and Slashing: Introduce a staking mechanism where provers deposit tokens as collateral. \\n    Dishonest behavior results in slashing (partial or complete loss) of the staked tokens, while honest actions are rewarded.\\n   \\n    * Proof of Work/Proof of Stake: Implement a proof-of-work or proof-of- stake consensus mechanism for verification, \\n    aligning incentives with the blockchain\u2019s broader consensus mechanism.\\n\\n\\n## Goal 4: Kernel-based Architecture Implementation\\n\\nThis goal centers on the establishment of a kernel-based architecture, akin to the model observed in ZEXE, to facilitate the attestation of accurate private function executions. \\nThis innovative approach employs recursion to construct a call stack, which is then validated through iterative recursive computations. \\nAt its core, this technique harnesses a recursive Succinct Non-Interactive Argument of Knowledge (SNARK) mechanism, where each function call\u2019s proof accumulates within the call stack.\\n\\nThe subsequent verification of this stack\u2019s authenticity leverages recursive SNARK validation. \\nWhile this method offers robust verification of private function executions, it\u2019s essential to acknowledge its associated intricacies.\\n\\nThe generation of SNARK proofs necessitates a substantial computational effort, which, in turn, may lead to elevated gas fees for users. \\nMoreover, the iterative recursive computations could potentially exhibit computational expansion as the depth of recursion increases. \\nThis calls for a meticulous balance between the benefits of recursive verification and the resource implications it may entail.\\n\\nIn essence, Goal 4 embodies a pursuit of enhanced verification accuracy through a kernel-based architecture. \\nBy weaving recursion and iterative recursive computations into the fabric of our system, we aim to establish a mechanism that accentuates the trustworthiness of private function executions, \\nwhile conscientiously navigating the computational demands that ensue.\\n\\nTo accomplish the goal of implementing a kernel-based architecture for recursive verification of private function executions, \\nseveral strategic steps and considerations can be undertaken: recursion handling and depth management.\\n\\n<ins> Recursion Handling </ins>\\n\\n* _Call Stack Management:_ \\n\\n    * Implement a data structure to manage the call stack, recording each recursive function call\u2019s details, parameters, and state.\\n\\n* _Proof Accumulation: _\\n\\n    * Design a mechanism to accumulate proof data for each function call within the call stack. \\n    This includes cryptographic commitments, intermediate results, and cryptographic challenges.\\n\\n    * Ensure that the accumulated proof data remains secure and tamper-resistant throughout the recursion process.\\n\\n* _Intermediary SNARK Proofs:_\\n\\n    * Develop an intermediary SNARK proof for each function call\u2019s correctness within the call stack. \\n    This proof should demonstrate that the function executed correctly and produced expected outputs.\\n\\n    * Ensure that the intermediary SNARK proof for each recursive call can be aggregated and verified together, maintaining the integrity of the entire call stack.\\n\\n<ins> Depth management </ins>\\n\\n* _Depth Limitation:_\\n\\n    * Define a threshold for the maximum allowable recursion depth based on the system\u2019s computational capacity, gas limitations, and performance considerations.\\n\\n    * Implement a mechanism to prevent further recursion beyond the defined depth limit, safeguarding against excessive computational growth.\\n\\n* _Graceful Degradation:_\\n\\n    * Design a strategy for graceful degradation when the recursion depth approaches or reaches the defined limit. \\n    This may involve transitioning to alternative execution modes or optimization techniques.\\n\\n    * Communicate the degradation strategy to users and ensure that the system gracefully handles scenarios where recursion must be curtailed.\\n\\n* _Resource Monitoring:_\\n\\n    * Develop tools to monitor resource consumption (such as gas usage and computational time) as recursion progresses. \\n    Provide real-time feedback to users about the cost and impact of recursive execution.\\n\\n* _Dynamic Depth Adjustment:_\\n\\n    * Consider implementing adaptive depth management that dynamically adjusts the recursion depth based on network conditions, transaction fees, and available resources.\\n\\n    * Utilize algorithms to assess the optimal recursion depth for efficient execution while adhering to gas cost constraints.\\n\\n* _Fallback Mechanisms:_\\n\\n    * Create fallback mechanisms that activate if the recursion depth limit is reached or if the system encounters resource constraints. \\n    These mechanisms could involve alternative verification methods or delayed execution.\\n\\n* _User Notifications:_\\n\\n    * Notify users when the recursion depth limit is approaching, enabling them to make informed decisions about the complexity of their transactions and potential resource usage.\\n\\n\\nGoal 4 underscores the project\'s ambition to integrate the merits of a kernel-based architecture with recursive verifications to bolster the reliability of private function executions. \\nWhile the approach promises robust outcomes, it\'s pivotal to maneuver through its intricacies with astute strategies, ensuring computational efficiency and economic viability. \\nBy striking this balance, the architecture can realize its full potential in ensuring trustworthy and efficient private function executions.\\n\\n\\n## Goal 5: Seamless Interaction Design\\n\\nGoal 5 revolves around the meticulous design of a seamless interaction between public and private states within the blockchain ecosystem. \\nThis objective envisions achieving not only composability between contracts but also the harmonious integration of private and public functions.\\n\\nA notable challenge in this endeavor lies in the intricate interplay between public and private states, \\nwherein the potential linkage of a private transaction to a public one raises concerns about unintended information leakage.\\n\\nThe essence of this goal entails crafting an architecture that facilitates the dynamic interaction of different states while ensuring that the privacy and confidentiality of private transactions remain unbreached. \\nThis involves the formulation of mechanisms that enable secure composability between contracts, guaranteeing the integrity of interactions across different layers of functionality.\\n\\nA key focus of this goal is to surmount the challenge of information leakage by implementing robust safeguards. \\nThe solution involves devising strategies to mitigate the risk of revealing private transaction details when connected to corresponding public actions. \\nBy creating a nuanced framework that com- partmentalizes private and public interactions, the architecture aims to uphold privacy while facilitating seamless interoperability.\\n\\nGoal 5 encapsulates a multifaceted undertaking, calling for the creation of an intricate yet transparent framework that empowers users to confidently engage in both public and private functions, \\nwithout compromising the confidentiality of private transactions. The successful realization of this vision hinges on a delicate blend of architectural ingenuity, cryptographic sophistication, and user-centric design.\\n\\nTo achieve seamless interaction between public and private states, composability, and privacy preservation, a combination of solutions and approaches can be employed. \\nIn the table below, a comprehensive list of solutions that address these objectives:\\n\\n<center>\\n\\n|           **Solution Category**           \\t|                                                                                     **Description**                                                                                    \\t|\\n|:-----------------------------------------:\\t|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\\t|\\n| **Layer 2 Solutions**                     \\t| Employ zk-Rollups, Optimistic Rollups, and state channels to handle private interactions off-chain and settle them on-chain periodically. Boost scalability and cut transaction costs. \\t|\\n| **Intermediary Smart Contracts**          \\t|                            Craft smart contracts as intermediaries for secure public-private interactions. Use these to manage data exchange confidentially.                           \\t|\\n| **Decentralized Identity & Pseudonymity** \\t|                                  Implement decentralized identity systems for pseudonymous interactions. Validate identity using cryptographic proofs.                                 \\t|\\n| **Confidential Sidechains & Cross-Chain** \\t|                                 Set up confidential sidechains and employ cross-chain protocols to ensure private and composability across blockchains.                                \\t|\\n| **Temporal Data Structures**              \\t|                               Create chronological data structures for secure interactions. Utilize cryptographic methods for data integrity and privacy.                              \\t|\\n| **Homomorphic Encryption & MPC**          \\t|                                     Apply homomorphic encryption and MPC for computations on encrypted data and interactions between state layers.                                     \\t|\\n| **Commit-Reveal Schemes**                 \\t|                                     Introduce commit-reveal mechanisms for private transactions, revealing data only post necessary public actions.                                    \\t|\\n| **Auditability & Verifiability**          \\t|                                Use on-chain tools for auditing and verifying interactions. Utilize cryptographic commitments for third-party validation.                               \\t|\\n| **Data Fragmentation & Sharding**         \\t|                               Fragment data across shards for private interactions and curtailed data exposure. Bridge shards securely with cryptography.                              \\t|\\n| **Ring Signatures & CoinJoin**            \\t|                                  Incorporate ring signatures and CoinJoin protocols to mask transaction details and mix transactions collaboratively.                                  \\t|\\n\\n</center>\\n\\n## Goal 6: Integration of DeFi Protocols with a Privacy-Preserving Framework\\n\\nThe primary aim of Goal 6 is to weave key DeFi protocols, such as AMMs and staking, into a user-centric environment that accentuates privacy. \\nThis endeavor comes with inherent challenges, especially considering the heterogeneity of existing DeFi protocols, predominantly built on Ethereum. \\nThese variations in programming languages and VMs exacerbate the quest for interoperability. Furthermore, the success and functionality of DeFi protocols is closely tied to liquidity, \\nwhich in turn is influenced by user engagement and the amount of funds locked into the system.\\n\\n## <ins> Strategic Roadmap for Goal 6 </ins>\\n\\n1. _** Pioneering Privacy-Centric DeFi Models: **_ Initiate the development of AMMs and staking solutions that are inherently protective of users\' transactional privacy and identity.\\n\\n2. _** Specialized Smart Contracts with Privacy: **_ Architect distinct smart contracts infused with privacy elements, setting the stage for secure user interactions within this new, confidential DeFi landscape.\\n\\n3. _** Optimized User Interfaces: **_ Craft interfaces that resonate with user needs, simplifying the journey through the private DeFi space without compromising on security.\\n\\n4. _** Tackling Interoperability: **_ \\n\\n    * Deploy advanced bridge technologies and middleware tools to foster efficient data exchanges and guarantee operational harmony across a spectrum of programming paradigms and virtual environments.\\n    \\n    * Design and enforce universal communication guidelines that bridge the privacy-centric DeFi entities with the larger DeFi world seamlessly.\\n\\n\\n5. _** Enhancing and Sustaining Liquidity: **_\\n\\n    * Unveil innovative liquidity stimuli and yield farming incentives, compelling users to infuse liquidity into the private DeFi space.\\n    \\n    * Incorporate adaptive liquidity frameworks that continually adjust based on the evolving market demands, ensuring consistent liquidity.\\n\\n    * Forge robust alliances with other DeFi stalwarts, jointly maximizing liquidity stores and honing sustainable token distribution strategies.\\n\\n\\n6. _** Amplifying Community Engagement:**_ Design and roll out enticing incentive schemes to rally users behind privacy-focused AMMs and staking systems, \\nthereby nurturing a vibrant, privacy-advocating DeFi community.\\n\\n\\nThrough the integration of these approaches, we aim to achieve Goal 6, providing users with a privacy-focused platform for engaging effortlessly in core DeFi functions such as AMMs and staking, \\nall while effectively overcoming the obstacles related to interoperability and liquidity concerns.\\n\\n\\n## Summary of the Architecture\\n\\nIn our quest to optimize privacy, we\'re proposing a Zero-Knowledge Virtual Machine (Zkvm) that harnesses the power of Zero-Knowledge Proofs (ZKPs).\\nThese proofs ensure that while private state data remains undisclosed, public state transitions can still be carried out and subsequently verified by third parties. \\nThis blend of public and private state is envisaged to be achieved through a state tree representing the public state, while the encrypted state leaves stand for the private state. \\nEach user\'s private state indicates validity through the absence of a corresponding nullifier.\\nA nullifier is a unique cryptographic value generated in privacy-preserving blockchain transactions to prevent double-spending, \\nensuring that each private transaction is spent only once without revealing its details.\\n\\nPrivate functions\' execution mandates users to offer a proof underscoring the accurate execution of all encapsulated private calls. \\nFor validating a singular private function call, we\'re leaning into the kernel-based model inspired by the ZEXE protocol. \\nDefined as kernel circuits, these functions validate the correct execution of each private function call. \\nDue to their recursive circuit structure, a succession of private function calls can be executed by calculating proofs in an iterative manner. \\nExecution-relevant data, like private and public call stacks and additions to the state tree, are incorporated as public inputs.\\n\\nOur method integrates the verification keys for these functions within a merkle tree. Here\'s the innovation: a user\'s ZKP showcases the existence of the verification key in this tree, yet keeps the executed function concealed. \\nThe unique function identifier can be presented as the verification key, with all contracts merkleized for hiding functionalities.\\n\\nWe suggest a nuanced shift from the ZEXE protocol\'s identity function, which crafts an identity for smart contracts delineating its behavior, access timeframes, and other functionalities. \\nInstead of the ZEXE protocol\'s structure, our approach pivots to a method anchored in the \\nsecurity of a secret combined with the uniqueness from hashing with the contract address. \\nThe underlying rationale is straightforward: the sender, equipped with a unique nonce and salt for the transaction, hashes the secret, payload, nonce, and salt. \\nThis result is then hashed with the contract address for the final value. The hash function\'s unidirectional nature ensures that the input cannot be deduced easily from its output. \\nA specific concern, however, is the potential repetition of secret and payload values across transactions, which could jeopardize privacy. \\nYet, by embedding the function\'s hash within the hash of the contract address, users can validate a specific function\'s execution without divulging the function, navigating this limitation.\\n\\nAlternative routes do exist: We could employ signature schemes like ECDSA, focusing on uniqueness and authenticity, albeit at the cost of complex key management. \\nFully Homomorphic Encryption (FHE) offers another pathway, enabling function execution on encrypted data, or Multi-Party Computation (MPC) which guarantees non-disclosure of function or inputs. \\nYet, integrating ZKPs with either FHE or MPC presents a challenge. Combining cryptographic functions like SHA-3 and BLAKE2 can also bolster security and uniqueness. \\nIt\'s imperative to entertain these alternatives, especially when hashing might not serve large input/output functions effectively or might fall short in guaranteeing uniqueness.\\n\\n## Current State\\n\\nOur aim is to revolutionize the privacy and security paradigms through Nescience.\\nAs we strive to set milestones and achieve groundbreaking advancements, \\nour current focus narrows onto the realization of Goal 2 and Goal 3.\\n\\nOur endeavors to build a powerful virtual machine tailored for Zero-Knowledge applications have led us down the path of rigorous exploration and testing. \\nWe believe that integrating the right proof system is pivotal to our project\'s success, which brings us to Nova [[8](https://eprint.iacr.org/2021/370)].\\nIn our project journey, we have opted to integrate the Nova proof system, recognizing its potential alignment with our overarching goals. \\nHowever, as part of our meticulous approach to innovation and optimization, we acknowledge the need to thoroughly examine Nova\u2019s performance capabilities, \\nparticularly due to its status as a pioneering and relatively unexplored proof system.\\n\\nThis critical evaluation entails a comprehensive process of benchmarking and comparative analysis [[9]](https://github.com/vacp2p/zk-explorations), \\npitting Nova against other prominent proof systems in the field, including Halo2 [[10](https://electriccoin.co/blog/explaining-halo-2/)], \\nPlonky2 [[11](https://polygon.technology/blog/introducing-plonky2)], and Starky [[12](https://eprint.iacr.org/2021/582)]. \\nThis ongoing and methodical initiative is designed to ensure a fair and impartial assessment, enabling us to draw meaningful conclusions about Nova\u2019s strengths and limitations in relation to its counterparts. \\nBy leveraging the Poseidon recursion technique, we are poised to conduct an exhaustive performance test that delves into intricate details. \\nThrough this testing framework, we aim to discern whether Nova possesses the potential to outshine its contemporaries in terms of efficiency, scalability, and overall performance. \\nThe outcome of this rigorous evaluation will be pivotal in shaping our strategic decisions moving forward. \\nArmed with a comprehensive understanding of Nova\u2019s performance metrics vis-\xe0-vis other proof systems, \\nwe can confidently chart a course that maximizes the benefits of our project\u2019s optimization efforts.\\n\\nMoreover, as we ambitiously pursue the establishment of a robust mechanism for proof creation and verification, our focus remains resolute on preserving user privacy, \\nincentivizing honest behaviour, and ensuring the cost-effective verification of transactions. \\nAt the heart of this endeavor is our drive to empower users by allowing them the autonomy of generating proofs for private state updates, \\nthereby reducing dependencies and enhancing privacy.\\nWe would like to actively work on providing comprehensive documentation, user-friendly tools, \\nand tutorials to aid users in this intricate process.\\n\\nParallelly, we\'re looking into decentralized verification processes, harnessing the strength of multiple external provers that cross-verify each other\'s work. \\nOur commitment is further cemented by our efforts to introduce a dynamic reward system that adjusts based on network metrics and prover performance. \\nThis intricate balance, while challenging, aims to fortify our system against potential adversarial actions, aligning incentives, and preserving the overall integrity of the project.\\n\\n\\n# References\\n\\n[1] Nakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. Retrieved from https://bitcoin.org/bitcoin.pdf\\n\\n[2] Sanchez, F. (2021). Cardano\u2019s Extended UTXO accounting model. Retrived from https://iohk.io/en/blog/posts/2021/03/11/cardanos-extended-utxo-accounting-model/\\n\\n[3] Morgan, D. (2020). HD Wallets Explained: From High Level to Nuts and Bolts. Retrieved from https://medium.com/mycrypto/the-journey-from-mnemonic-phrase-to-address-6c5e86e11e14\\n\\n[4] Wuille, P. (012). Bitcoin Improvement Proposal (BIP) 44. Retrieved from https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\\n\\n[5] Jedusor, T. (2020). Introduction to Mimblewimble and Grin. Retrieved from https://github.com/mimblewimble/grin/blob/master/doc/intro.md\\n\\n[6]  Bitcoin\'s official wiki overview of the CoinJoin method. Retrieved from https://en.bitcoin.it/wiki/CoinJoin\\n\\n[7] TornadoCash official Github page. Retrieved from https://github.com/tornadocash/tornado-classic-ui\\n\\n[8] Kothapalli, A., Setty, S., Tzialla, I. (2021). Nova: Recursive Zero-Knowledge Arguments from Folding Schemes. Retrieved from https://eprint.iacr.org/2021/370\\n\\n[9] ZKvm Github page. Retrieved from https://github.com/vacp2p/zk-explorations\\n\\n[10] Electric Coin Company (2020). Explaining Halo 2. Retrieved from https://electriccoin.co/blog/explaining-halo-2/\\n\\n[11] Polygon Labs (2022). Introducing Plonky2. Retrieved from https://polygon.technology/blog/introducing-plonky2\\n\\n[12] StarkWare (2021). ethSTARK Documentation. Retrieved from https://eprint.iacr.org/2021/582"},{"id":"device-pairing-in-js-waku-and-go-waku","metadata":{"permalink":"/rlog/device-pairing-in-js-waku-and-go-waku","source":"@site/rlog/2023-04-24-device-pairing-in-js-waku-and-go-waku.mdx","title":"Device Pairing in Js-waku and Go-waku","description":"Device pairing and secure message exchange using Waku and noise protocol.","date":"2023-04-24T12:00:00.000Z","formattedDate":"April 24, 2023","tags":[],"readingTime":4.09,"hasTruncateMarker":true,"authors":[{"name":"Richard","twitter":"richardramos_me","github":"richard-ramos","website":"https://richard-ramos.github.io/","key":"rramos"}],"frontMatter":{"layout":"post","name":"Device Pairing in Js-waku and Go-waku","title":"Device Pairing in Js-waku and Go-waku","date":"2023-04-24T12:00:00.000Z","authors":"rramos","published":true,"slug":"device-pairing-in-js-waku-and-go-waku","categories":"platform"},"prevItem":{"title":"Nescience - A zkVM leveraging hiding properties","permalink":"/rlog/Nescience-A-zkVM-leveraging-hiding-properties"},"nextItem":{"title":"The Future of Waku Network: Scaling, Incentivization, and Heterogeneity","permalink":"/rlog/future-of-waku-network"}},"content":"Device pairing and secure message exchange using Waku and noise protocol.\\n\\n\x3c!--truncate--\x3e\\n\\nAs the world becomes increasingly connected through the internet, the need for secure and reliable communication becomes paramount. In [this article](https://vac.dev/wakuv2-noise) it is described how the Noise protocol can be used as a key-exchange mechanism for Waku.\\n\\nRecently, this feature was introduced in [js-waku](https://github.com/waku-org/js-noise) and [go-waku](https://github.com/waku-org/go-waku), providing a simple API for developers to implement secure communication protocols using the Noise Protocol framework. These open-source libraries provide a solid foundation for building secure and decentralized applications that prioritize data privacy and security.\\n\\nThis functionality is designed to be simple and easy to use, even for developers who are not experts in cryptography. The library offers a clear and concise API that abstracts away the complexity of the Noise Protocol framework and provides an straightforward interface for developers to use. Using this, developers can effortlessly implement secure communication protocols on top of their JavaScript and Go applications, without having to worry about the low-level details of cryptography.\\n\\nOne of the key benefits of using Noise is that it provides end-to-end encryption, which means that the communication between two parties is encrypted from start to finish. This is essential for ensuring the security and privacy of sensitive information\\n\\n### Device Pairing\\n\\nIn today\'s digital world, device pairing has become an integral part of our lives. Whether it\'s connecting our smartphones with other computers or web applications, the need for secure device pairing has become more crucial than ever. With the increasing threat of cyber-attacks and data breaches, it\'s essential to implement secure protocols for device pairing to ensure data privacy and prevent unauthorized access.\\n\\nTo demonstrate how device pairing can be achieved using Waku and Noise, we have examples available at https://examples.waku.org/noise-js/. You can try pairing different devices, such as mobile and desktop, via a web application. This can be done by scanning a QR code or opening a URL that contains the necessary data for a secure handshake.\\n\\nThe process works as follows:\\n\\nActors:\\n\\n- Alice the initiator\\n- Bob the responder\\n\\n1. The first step in achieving secure device pairing using Noise and Waku is for Bob generate the pairing information which could be transmitted out-of-band. For this, Bob opens https://examples.waku.org/noise-js/ and a QR code is generated, containing the data required to do the handshake. This pairing QR code is timeboxed, meaning that after 2 minutes, it will become invalid and a new QR code must be generated\\n2. Alice scans the QR code using a mobile phone. This will open the app with the QR code parameters initiating the handshake process which is described in [WAKU2-DEVICE-PAIRING](https://github.com/waku-org/specs/blob/master/standards/application/device-pairing.md/#protocol-flow). These messages are exchanged between two devices over Waku to establish a secure connection. The handshake messages consist of three main parts: the initiator\'s message, the responder\'s message, and the final message, which are exchanged to establish a secure connection. While using js-noise, the developer is abstracted of this process, since the messaging happens automatically depending on the actions performed by the actors in the pairing process.\\n3. Both Alice and Bob will be asked to verify each other\'s identity. This is done by confirming if an 8-digits authorization code match in both devices. If both actors confirm that the authorization code is valid, the handshake concludes succesfully\\n4. Alice and Bob receive a set of shared keys that can be used to start exchanging encrypted messages. The shared secret keys generated during the handshake process are used to encrypt and decrypt messages sent between the devices. This ensures that the messages exchanged between the devices are secure and cannot be intercepted or modified by an attacker.\\n\\nThe above example demonstrates device pairing using js-waku. Additionally, You can also try building and experimenting with other noise implementations like nwaku, or go-waku, with an example available at https://github.com/waku-org/go-waku/tree/master/examples/noise in which the same flow described before is done with Bob (the receiver) using go-waku instead of js-waku.\\n\\n### Conclusion\\n\\nWith its easy to use API built on top of the Noise Protocol framework and the LibP2P networking stack, if you are a developer looking to implement secure messaging in their applications that are both decentralized and censorship resistant, Waku is definitely an excellent choice worth checking out!\\n\\nWaku is also Open source with a MIT and APACHEv2 licenses, which means that developers are encouraged to contribute code, report bugs, and suggest improvements to make it even better.\\n\\nDon\'t hesitate to try the live example at https://examples.waku.org/noise-js and build your own webapp using https://github.com/waku-org/js-noise, https://github.com/waku-org/js-waku and https://github.com/waku-org/go-waku. This will give you a hands-on experience of implementing secure communication protocols using the Noise Protocol framework in a practical setting. Happy coding!\\n\\n### References\\n\\n- [Noise handshakes as key-exchange mechanism for Waku](https://vac.dev/wakuv2-noise)\\n- [Noise Protocols for Waku Payload Encryption](https://github.com/waku-org/specs/blob/master/standards/application/noise.md)\\n- [Session Management for Waku Noise](https://github.com/waku-org/specs/blob/master/standards/application/noise-sessions.md)\\n- [Device pairing and secure transfers with Noise](https://github.com/waku-org/specs/blob/master/standards/application/device-pairing.md)\\n- [go-waku Noise\'s example](https://github.com/waku-org/go-waku/tree/master/examples/noise)\\n- [js-waku Noise\'s example](https://github.com/waku-org/js-waku-examples/tree/master/examples/noise-js)\\n- [js-noise](https://github.com/waku-org/js-noise/)\\n- [go-noise](https://github.com/waku-org/js-noise/)"},{"id":"future-of-waku-network","metadata":{"permalink":"/rlog/future-of-waku-network","source":"@site/rlog/2023-04-03-waku-as-a-network.mdx","title":"The Future of Waku Network: Scaling, Incentivization, and Heterogeneity","description":"Learn how the Waku Network is evolving through scaling, incentivization, and diverse ecosystem development and what the future might look like.","date":"2023-04-03T00:00:00.000Z","formattedDate":"April 3, 2023","tags":[],"readingTime":5.745,"hasTruncateMarker":true,"authors":[{"name":"Franck","twitter":"fryorcraken","github":"fryorcraken","key":"franck"}],"frontMatter":{"layout":"post","name":"The Future of Waku Network: Scaling, Incentivization, and Heterogeneity","title":"The Future of Waku Network: Scaling, Incentivization, and Heterogeneity","date":"2023-04-03T00:00:00.000Z","authors":"franck","published":true,"slug":"future-of-waku-network","categories":"platform, operator, network","image":"/img/black-waku-logo-with-name.png","discuss":"https://forum.vac.dev/t/discussion-the-future-of-waku-network-scaling-incentivization-and-heterogeneity/173","hide_table_of_contents":false},"prevItem":{"title":"Device Pairing in Js-waku and Go-waku","permalink":"/rlog/device-pairing-in-js-waku-and-go-waku"},"nextItem":{"title":"Waku for All Decentralized Applications and Infrastructures","permalink":"/rlog/waku-for-all"}},"content":"Learn how the Waku Network is evolving through scaling, incentivization, and diverse ecosystem development and what the future might look like.\\n\\n\x3c!--truncate--\x3e\\n\\nWaku is preparing for production with a focus on the Status Communities use case. In this blog post, we will provide an\\noverview of recent discussions and research outputs, aiming to give you a better understanding of how the Waku network\\nmay look like in terms of scaling and incentivization.\\n\\n## DOS Mitigation for Status Communities\\n\\nWaku is actively exploring DOS mitigation mechanisms suitable for Status Communities. While RLN\\n(Rate Limiting Nullifiers) remains the go-to DOS protection solution due to its privacy-preserving and\\ncensorship-resistant properties, there is still more work to be done. We are excited to collaborate with PSE\\n(Privacy & Scaling Explorations) in this endeavor. Learn more about their latest progress in this [tweet](https://twitter.com/CPerezz19/status/1640373940634939394?s=20).\\n\\n## A Heterogeneous Waku Network\\n\\nAs we noted in a previous [forum post](https://forum.vac.dev/t/waku-payment-models/166/3), Waku\'s protocol\\nincentivization model needs to be flexible to accommodate various business models. Flexibility ensures that projects\\ncan choose how they want to use Waku based on their specific needs.\\n\\n### Reversing the Incentivization Question\\n\\nTraditionally, the question of incentivization revolves around how to incentivize operators to run nodes. We\'d like to\\nreframe the question and instead ask, \\"How do we pay for the infrastructure?\\"\\n\\nWaku does not intend to offer a free lunch.\\nEthereum\'s infrastructure is supported by transaction fees and inflation, with validators receiving rewards from both sources.\\nHowever, this model does not suit a communication network like Waku.\\nUsers and platforms would not want to pay for every single message they send. Additionally, Waku aims to support instant\\nephemeral messages that do not require consensus or long-term storage.\\n\\nProjects that use Waku to enable user interactions, whether for chat messages, gaming, private DeFi, notifications, or\\ninter-wallet communication, may have different value extraction models. Some users might provide services for the\\nproject and expect to receive value by running nodes, while others may pay for the product or run infrastructure to\\ncontribute back. Waku aims to support each of these use cases, which means there will be various ways to \\"pay for the\\ninfrastructure.\\"\\n\\nIn [his talk](https://vac.dev/building-privacy-protecting-infrastructure), Oskar addressed two strategies: RLN and service credentials.\\n\\n### RLN and Service Credentials\\n\\nRLN enables DOS protection across the network in a privacy-preserving and permission-less manner: stake in a contract,\\nand you can send messages.\\n\\nService credentials establish a customer-provider relationship. Users might pay to have messages they are interested in\\nstored and served by a provider. Alternatively, a community owner could pay a service provider to host their community.\\n\\nProviders could offer trial or limited free services to Waku users, similar to Slack or Discord. Once a trial is expired or outgrown,\\na community owner could pay for more storage or bandwidth, similar to Slack\'s model.\\nAlternatively, individual users could contribute financially, akin to Discord\'s Server Boost, or by sharing their own\\nresources with their community.\\n\\nWe anticipate witnessing various scenarios across the spectrum: from users sharing resources to users paying for access to the network and everything in between.\\n\\n## Waku Network: Ethereum or Cosmos?\\n\\nAnother perspective is to consider whether the Waku network will resemble Ethereum or Cosmos.\\n\\nFor those not familiar with the difference between both, in a very concise manner:\\n\\n- Ethereum is a set of protocols and software that are designed to operate on one common network and infrastructure\\n- Cosmos is a set of protocols and software (SDKs) designed to be deployed in separate yet interoperable networks and infrastructures by third parties\\n\\nWe want Waku to be decentralized to provide censorship resistance and privacy-preserving communication.\\nIf each application has to deploy its own network, we will not achieve this goal.\\nTherefore, we aim Waku to be not only an open source set of protocols, but also a shared infrastructure that anyone can leverage to build applications on top, with some guarantees in terms of decentralization and anonymity.\\nThis approach is closer in spirit to Ethereum than Cosmos.\\nDo note that, similarly to Ethereum, anyone is free to take Waku software and protocols and deploy their own network.\\n\\nYet, because of the difference in the fee model, the Waku Network is unlikely to be as unified as Ethereum\'s.\\nWe currently assume that there will be separate gossipsub networks with different funding models.\\nSince there is no consensus on Waku, each individual operator can decide which network to support, enabling Waku to maintain its permission-less property.\\n\\nMost likely, the Waku network will be heterogeneous, and node operators will choose the incentivization model they prefer.\\n\\n## Scalability and Discovery Protocols\\n\\nTo enable scalability, the flow of messages in the Waku network will be divided in shards,\\nso that not every node has to forward every message of the whole network.\\nDiscovery protocols will facilitate users connecting to the right nodes to receive the messages they are interested in.\\n\\nDifferent shards could be subject to a variety of rate limiting techniques (globally, targeted to that shard or something in-between).\\n\\nMarketplace protocols may also be developed to help operators understand how they can best support the network and where\\ntheir resources are most needed. However, we are still far from establishing or even assert that such a marketplace will be needed.\\n\\n## Open Problems\\n\\nSplitting traffic between shards reduces bandwidth consumption for every Waku Relay node.\\nThis improvement increases the likelihood that users with home connections can participate and contribute to the gossipsub network without encountering issues.\\n\\nHowever, it does not cap traffic.\\nThere are still open problems regarding how to guarantee that someone can use Waku with lower Internet bandwidth or run critical services, such as a validation node, on the same connection.\\n\\nWe have several ongoing initiatives:\\n\\n- Analyzing the Status Community protocol to confirm efficient usage of Waku [[4]](https://github.com/vacp2p/research/issues/177)\\n- Simulating the Waku Network to measure actual bandwidth usage [[5]](https://github.com/waku-org/pm/issues/2)\\n- Segregating chat messages from control and media messages [[6]](https://github.com/waku-org/specs/blob/master/standards/core/relay-sharding.md/#control-message-shards)\\n\\nThe final solution will likely be a combination of protocols that reduce bandwidth usage or mitigate the risk of DOS attacks, providing flexibility for users and platforms to enable the best experience.\\n\\n## The Evolving Waku Network\\n\\nThe definition of the \\"Waku Network\\" will likely change over time. In the near future, it will transition from a single\\ngossipsub network to a sharded set of networks unified by a common discovery layer. This change will promote scalability\\nand allow various payment models to coexist within the Waku ecosystem.\\n\\nIn conclusion, the future of Waku Network entails growth, incentivization, and heterogeneity while steadfastly\\nmaintaining its core principles. As Waku continues to evolve, we expect it to accommodate a diverse range of use cases\\nand business models, all while preserving privacy, resisting censorship, avoiding surveillance, and remaining accessible\\nto devices with limited resources.\\n\\n## References\\n\\n1. [WAKU2-RELAY-SHARDING](https://github.com/waku-org/specs/blob/master/standards/core/relay-sharding.md)\\n2. [57/STATUS-Simple-Scaling](https://rfc.vac.dev/status/raw/simple-scaling)\\n3. [RLN-V2](https://rfc.vac.dev/vac/raw/rln-v2)\\n4. [Scaling Status Communities: Potential Problems](https://github.com/vacp2p/research/issues/177)\\n5. [Waku Network Testing](https://github.com/waku-org/pm/issues/2)\\n6. [WAKU2-RELAY-SHARDING: Control Message Shards](https://github.com/waku-org/specs/blob/master/standards/core/relay-sharding.md/#control-message-shards)"},{"id":"waku-for-all","metadata":{"permalink":"/rlog/waku-for-all","source":"@site/rlog/2022-11-08-waku-for-all-decentralize-applications.mdx","title":"Waku for All Decentralized Applications and Infrastructures","description":"Waku is an open communication protocol and network. Decentralized apps and infrastructure can use Waku for their","date":"2022-11-08T00:00:00.000Z","formattedDate":"November 8, 2022","tags":[],"readingTime":6.145,"hasTruncateMarker":true,"authors":[{"name":"Franck","twitter":"fryorcraken","github":"fryorcraken","key":"franck"}],"frontMatter":{"layout":"post","name":"Waku for All Decentralized Applications and Infrastructures","title":"Waku for All Decentralized Applications and Infrastructures","date":"2022-11-08T00:00:00.000Z","authors":"franck","published":true,"slug":"waku-for-all","categories":"waku, dapp, infrastructure, public good, platform, operator","image":"/img/black-waku-logo-with-name.png","discuss":"https://forum.vac.dev/t/discussion-waku-for-all-decentralized-applications-and-infrastructures/163"},"prevItem":{"title":"The Future of Waku Network: Scaling, Incentivization, and Heterogeneity","permalink":"/rlog/future-of-waku-network"},"nextItem":{"title":"Building Privacy-Protecting Infrastructure","permalink":"/rlog/building-privacy-protecting-infrastructure"}},"content":"Waku is an open communication protocol and network. Decentralized apps and infrastructure can use Waku for their\\ncommunication needs. It is designed to enable dApps and decentralized infrastructure projects to have secure, private,\\nscalable communication. Waku is available in several languages and platforms, from Web to mobile to desktop to cloud.\\nInitially, We pushed Waku adoption to the Web ecosystem, we learned that Waku is usable in a variety of complex applications\\nand infrastructure projects. We have prioritized our effort to make Waku usable on various platforms and environments.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nWe have built Waku to be the communication layer for Web3. Waku is a collection of protocols to chose from for your\\nmessaging needs. It enables secure, censorship-resistant, privacy-preserving, spam-protected communication for its user.\\nIt is designed to run on any device, from mobile to the cloud.\\n\\nWaku is available on many systems and environments and used by several applications and SDKs for decentralized communications.\\n\\nThis involved research efforts in various domains: conversational security, protocol incentivization, zero-knowledge,\\netc.\\n\\nWaku uses novel technologies. Hence, we knew that early dogfooding of Waku was necessary. Even if research\\nwas still _in progress_ [[1]](#references). Thus, as soon as Waku protocols and software were usable, we started to push\\nfor the adoption of Waku. This started back in 2021.\\n\\nWaku is the communication component of the Web3 trifecta. This trifecta was Ethereum (contracts), Swarm\\n(storage) and Whisper (communication). Hence, it made sense to first target dApps which already uses one of the pillars:\\nEthereum.\\n\\nAs most dApps are web apps, we started the development of [js-waku for the browser](https://vac.dev/presenting-js-waku).\\n\\nOnce ready, we reached out to dApps to integrate Waku, added [prizes to hackathons](https://twitter.com/waku_org/status/1451400128791605254?s=20&t=Zhc0BEz6RVLkE_SeE6UyFA)\\nand gave [talks](https://docs.wakuconnect.dev/docs/presentations/).\\n\\nWe also assumed we would see patterns in the usage of Waku, that we would facilitate with the help of\\n[SDKs](https://github.com/status-im/wakuconnect-vote-poll-sdk).\\n\\nFinally, we created several web apps:\\n[examples](https://docs.wakuconnect.dev/docs/examples/)\\nand [PoCs](https://github.com/status-iM/gnosis-safe-waku).\\n\\nBy discussing with Waku users and watching it being used, we learned a few facts:\\n\\n1. The potential use cases for Waku are varied and many:\\n\\n- Wallet &lt;&gt; dApp communication: [WalletConnect](https://medium.com/walletconnect/walletconnect-v2-0-protocol-whats-new-3243fa80d312), [XMTP](https://xmtp.org/docs/dev-concepts/architectural-overview/)\\n- Off-chain (and private) marketplace:\\n  [RAILGUN](https://twitter.com/RAILGUN_Project/status/1556780629848727552?s=20&t=NEKQJiJAfg5WJqvuF-Ym_Q) &\\n  [Decentralized Uber](https://twitter.com/TheBojda/status/1455557282318721026)\\n- Signature exchange for a multi-sign wallet: [Gnosis Safe x Waku](https://github.com/status-im/gnosis-safe-waku)\\n- Off-chain Game moves/actions: [Super Card Game (EthOnline 2021)](https://showcase.ethglobal.com/ethonline2021/super-card-game)\\n- Decentralized Pastebin: [Debin](https://debin.io/)\\n\\n2. Many projects are interested in having an embedded chat in their dApp,\\n3. There are complex applications that need Waku as a solution. Taking RAILGUN as an example:\\n\\n- Web wallet\\n- \\\\+ React Native mobile wallet\\n- \\\\+ NodeJS node/backend.\\n\\n(1) means that it is not that easy to create SDKs for common use cases.\\n\\n(2) was a clear candidate for an SDK. Yet, building a chat app is a complex task. Hence, the Status app team tackled\\nthis in the form of [Status Web](https://github.com/status-im/status-web/).\\n\\nFinally, (3) was the most important lesson. We learned that multi-tier applications need Waku for decentralized and\\ncensorship-resistant communications. For these projects, js-waku is simply not enough. They need Waku to work in their\\nGolang backend, Unity desktop game and React Native mobile app.\\n\\nWe understood that we should see the whole Waku software suite\\n([js-waku](https://github.com/waku-org/js-waku),\\n[nwaku](https://github.com/status-im/nwaku),\\n[go-waku](https://github.com/status-im/go-waku),\\n[waku-react-native](https://github.com/waku-org/waku-react-native),\\n[etc](https://github.com/waku-org)) as an asset for its success.\\nThat we should not limit outreach, marketing, documentation efforts to the web, but target all platforms.\\n\\nFrom a market perspective, we identified several actors:\\n\\n- platforms: Projects that uses Waku to handle communication,\\n- operators: Operators run Waku nodes and are incentivized to do so,\\n- developers: Developers are usually part of a platforms or solo hackers learning Web3,\\n- contributors: Developers and researchers with interests in decentralization, privacy, censorship-resistance,\\n  zero-knowledge, etc.\\n\\n## Waku for All Decentralized Applications and Infrastructures\\n\\nIn 2022, we shifted our focus to make the various Waku implementations **usable and used**.\\n\\nWe made Waku [multi-plaform](https://github.com/status-im/go-waku/tree/master/examples).\\n\\nWe shifted Waku positioning to leverage all Waku implementations and better serve the user\'s needs:\\n\\n- Running a node for your projects and want to use Waku? Use [nwaku](https://github.com/status-im/nwaku).\\n- Going mobile? Use [Waku React Native](https://github.com/status-im/waku-react-native).\\n- C++ Desktop Game? Use [go-waku\'s C-Bindings](https://github.com/status-im/go-waku/tree/master/examples/c-bindings).\\n- Web app? Use [js-waku](https://github.com/status-im/js-waku).\\n\\nWe are consolidating the documentation for all implementations on a single website ([work in progress](https://github.com/waku-org/waku.org/issues/15))\\nto improve developer experience.\\n\\nThis year, we also started the _operator outreach_ effort to push for users to run their own Waku nodes. We have\\nrecently concluded our [first operator trial run](https://github.com/status-im/nwaku/issues/828).\\n[Nwaku](https://vac.dev/introducing-nwaku)\'s documentation, stability and performance has improved. It is now easier to\\nrun your [own Waku node](https://github.com/status-im/nwaku/tree/master/docs/operators).\\n\\nToday, operator wannabes most likely run their own nodes to support or use the Waku network.\\nWe are [dogfooding](https://twitter.com/oskarth/status/1582027828295790593?s=20&t=DPEP6fXK6KWbBjV5EBCBMA)\\n[Waku RLN](https://github.com/status-im/nwaku/issues/827), our novel economic spam protection protocol,\\nand looking at [incentivizing the Waku Store protocol](https://github.com/vacp2p/research/issues/99).\\nThis way, we are adding reasons to run your own Waku node.\\n\\nFor those who were following us in 2021, know that we are retiring the _Waku Connect_ branding in favour of the _Waku_\\nbranding.\\n\\n## Waku for Your Project\\n\\nAs discussed, Waku is now available on various platforms. The question remains: How can Waku benefit **your** project?\\n\\nHere are a couple of use cases we recently investigated:\\n\\n## Layer-2 Decentralization\\n\\nMost ([[2] [3]](#references) roll-ups use a centralized sequencer or equivalent. Running several sequencers is not as straightforward as running several execution nodes.\\nWaku can help:\\n\\n- Provide a neutral marketplace for a mempool: If sequencers compete for L2 tx fees, they may not be incentivized to\\n  share transactions with other sequencers. Waku nodes can act as a neutral network to enable all sequences to access\\n  transactions.\\n- Enable censorship-resistant wallet&lt;&gt;L2 communication,\\n- Provide rate limiting mechanism for spam protection: Using [RLN](https://rfc.vac.dev/vac/32/rln-v1) to prevent DDOS.\\n\\n## Device pairing and communication\\n\\nWith [Waku Device Pairing](https://github.com/waku-org/specs/blob/master/standards/application/device-pairing.md), a user can setup a secure encrypted communication channel\\nbetween their devices. As this channel would operate over Waku, it would be censorship-resistant and privacy preserving.\\nThese two devices could be:\\n\\n- Ethereum node and mobile phone to access a remote admin panel,\\n- Alice\'s phone and Bob\'s phone for any kind of secure communication,\\n- Mobile wallet and desktop/browser dApp for transaction and signature exchange.\\n\\nCheck [js-waku#950](https://github.com/waku-org/js-waku/issues/950) for the latest update on this.\\n\\n## Get Involved\\n\\nDeveloper? Grab any of the Waku implementations and integrate it in your app: https://waku.org/platform.\\n\\nResearcher? See https://vac.dev/contribute to participate in Waku research.\\n\\nTech-savvy? Try to run your own node: https://waku.org/operator.\\n\\nOtherwise, play around with the various [web examples](https://github.com/waku-org/js-waku-examples#readme).\\n\\nIf you want to help, we are [hiring](https://jobs.status.im/)!\\n\\n## Moving Forward\\n\\nWhat you can expect next:\\n\\n- [Scalability and performance studies](https://forum.vac.dev/t/waku-v2-scalability-studies/142/9) and improvement across Waku software,\\n- [New websites](https://github.com/waku-org/waku.org/issues/15) to easily find documentation about Waku and its implementations,\\n- New Waku protocols implemented in all code bases and cross client PoCs\\n  ([noise](https://github.com/waku-org/specs/blob/master/standards/application/noise.md), [noise-sessions](https://github.com/waku-org/specs/blob/master/standards/application/noise-sessions.md),\\n  [waku-rln-relay](https://rfc.vac.dev/waku/standards/core/17/rln-relay), etc),\\n- Easier to [run your own waku node](https://github.com/status-im/nwaku/issues/828), more operator trials,\\n- Dogfooding and Improvement of existing protocols (e.g. [Waku Filter](https://github.com/vacp2p/rfc/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc++12%2FWAKU2-FILTER)),\\n- Continue our focus Waku portability: Browser,\\n  [Raspberry Pi Zero](https://twitter.com/richardramos_me/status/1574405469912932355?s=20&t=DPEP6fXK6KWbBjV5EBCBMA) and other restricted-resource environments,\\n- More communication & marketing effort around Waku and the Waku developer community.\\n\\n---\\n\\n## References\\n\\n- \\\\[1\\\\] Waku is modular; it is a suite of protocols; hence some Waku protocols may be mature, while\\n  new protocols are still being designed. Which means that research continues to be _ongoing_ while\\n  Waku is already used in production.\\n- [[2]](https://community.optimism.io/docs/how-optimism-works/#block-production) The Optimism Foundation runs the only block produce on the Optimism network.\\n- [[3]](https://l2beat.com/) Top 10 L2s are documented has having a centralized operator."},{"id":"building-privacy-protecting-infrastructure","metadata":{"permalink":"/rlog/building-privacy-protecting-infrastructure","source":"@site/rlog/2022-11-04-building-privacy-protecting-infrastructure.mdx","title":"Building Privacy-Protecting Infrastructure","description":"What is privacy-protecting infrastructure? Why do we need it and how we can build it? We\'ll look at Waku, the communication layer for Web3. We\'ll see how it uses ZKPs to incentivize and protect the Waku network. We\'ll also look at Zerokit, a library that makes it easier to use ZKPs in different environments. After reading this, I hope you\'ll better understand the importance of privacy-protecting infrastructure and how we can build it.","date":"2022-11-04T12:00:00.000Z","formattedDate":"November 4, 2022","tags":[],"readingTime":18.61,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"Building Privacy-Protecting Infrastructure","title":"Building Privacy-Protecting Infrastructure","date":"2022-11-04T12:00:00.000Z","authors":"oskarth","published":true,"slug":"building-privacy-protecting-infrastructure","categories":"research","image":"/img/building_private_infra_intro.png","discuss":"https://forum.vac.dev/t/discussion-building-privacy-protecting-infrastructure/161"},"prevItem":{"title":"Waku for All Decentralized Applications and Infrastructures","permalink":"/rlog/waku-for-all"},"nextItem":{"title":"Waku Privacy and Anonymity Analysis Part I: Definitions and Waku Relay","permalink":"/rlog/wakuv2-relay-anon"}},"content":"What is privacy-protecting infrastructure? Why do we need it and how we can build it? We\'ll look at Waku, the communication layer for Web3. We\'ll see how it uses ZKPs to incentivize and protect the Waku network. We\'ll also look at Zerokit, a library that makes it easier to use ZKPs in different environments. After reading this, I hope you\'ll better understand the importance of privacy-protecting infrastructure and how we can build it.\\n\\n\x3c!--truncate--\x3e\\n\\n_This write-up is based on a talk given at DevCon 6 in Bogota, a video can be found [here](https://www.youtube.com/watch?v=CW1DYJifdhs)_\\n\\n### Intro\\n\\nIn this write-up, we are going to talk about building privacy-protecting\\ninfrastructure. What is it, why do we need it and how can we build it?\\n\\nWe\'ll look at Waku, the communication layer for Web3. We\'ll look at how we are\\nusing Zero Knowledge (ZK) technology to incentivize and protect the Waku\\nnetwork. We\'ll also look at Zerokit, a library we are writing to make ZKP easier\\nto use in different environments.\\n\\nAt the end of this write-up, I hope you\'ll come away with an understanding of\\nthe importance of privacy-protecting infrastructure and how we can build it.\\n\\n### About\\n\\nFirst, briefly about Vac. We build public good protocols for the decentralized\\nweb, with a focus on privacy and communication. We do applied research based on\\nwhich we build protocols, libraries and publications. We are also the custodians\\nof protocols that reflect a set of principles.\\n\\n![Principles](/img/building_private_infra_principles.png)\\n\\nIt has its origins in the [Status app](https://status.im/) and trying to improve\\nthe underlying protocols and infrastructure. We build [Waku](https://waku.org/),\\namong other things.\\n\\n### Why build privacy-protecting infrastructure?\\n\\nPrivacy is the power to selectively reveal yourself. It is a requirement for\\nfreedom and self-determination.\\n\\nJust like you need decentralization in order to get censorship-resistance, you\\nneed privacy to enable freedom of expression.\\n\\nTo build applications that are decentralized and privacy-protecting, you need\\nthe base layer, the infrastructure itself, to have those properties.\\n\\nWe see this a lot. It is easier to make trade-offs at the application layer than\\ndoing them at the base layer. You can build custodial solutions on top of a\\ndecentralized and non-custodial network where participants control their own\\nkeys, but you can\'t do the opposite.\\n\\nIf you think about it, buildings can be seen as a form of privacy-protecting\\ninfrastructure. It is completely normal and obvious in many ways, but when it\\ncomes to the digital realm our mental models and way of speaking about it hasn\'t\\ncaught up yet for most people.\\n\\nI\'m not going too much more into the need for privacy or what happens when you\\ndon\'t have it, but suffice to say it is an important property for any open\\nsociety.\\n\\nWhen we have conversations, true peer-to-peer offline conversations, we can talk\\nprivately. If we use cash to buy things we can do commerce privately.\\n\\nOn the Internet, great as it is, there are a lot of forces that makes this\\nnatural state of things not the default. Big Tech has turned users into a\\ncommodity, a product, and monetized user\'s attention for advertising. To\\noptimize for your attention they need to surveil your habits and activities, and\\nhence breach your privacy. As opposed to more old-fashioned models, where\\nsomeone is buying a useful service from a company and the incentives are more\\naligned.\\n\\nWe need to build credibly neutral infrastructure that protects your privacy at\\nthe base layer, in order to truly enable applications that are\\ncensorship-resistant and encourage meaningful freedom of expression.\\n\\n### Web3 infrastructure\\n\\nInfrastructure is what lies underneath. Many ways of looking at this but I\'ll\\nkeep it simple as per the original Web3 vision. You had Ethereum for\\ncompute/consensus, Swarm for storage, and Whisper for messaging. Waku has taken\\nover the mantle from Whisper and is a lot more\\n[usable](https://vac.dev/fixing-whisper-with-waku) today than Whisper ever was,\\nfor many reasons.\\n\\n![Web3 Infrastructure](/img/web3_holy_trinity.png)\\n\\nOn the privacy-front, we see how Ethereum is struggling. It is a big UX problem,\\nespecially when you try to add privacy back \\"on top\\". It takes a lot of effort\\nand it is easier to censor. We see this with recent action around Tornado Cash.\\nCompare this with something like Zcash or Monero, where privacy is there by\\ndefault.\\n\\nThere are also problems when it comes to the p2p networking side of things, for\\nexample with Ethereum validator privacy and hostile actors and jurisdictions. If\\nsomeone can easily find out where a certain validator is physically located,\\nthat\'s a problem in many parts of the world. Being able to have stronger\\nprivacy-protection guarantees would be very useful for high-value targets.\\n\\nThis doesn\'t begin to touch on the so called \\"dapps\\" that make a lot of\\nsacrifices in how they function, from the way domains work, to how websites are\\nhosted and the reliance on centralized services for communication. We see this\\ntime and time again, where centralized, single points of failure systems work\\nfor a while, but then eventually fail.\\n\\nIn many cases an individual user might not care enough though, and for platforms\\nthe lure to take shortcuts is strong. That is why it is important to be\\nprincipled, but also pragmatic in terms of the trade-offs that you allow on top.\\nWe\'ll touch more on this in the design goals around modularity that Waku has.\\n\\n### ZK for privacy-protecting infrastructure\\n\\nZKPs are a wonderful new tool. Just like smart contracts enables programmable\\nmoney, ZKPs allow us to express fundamentally new things. In line with the great\\ntradition of trust-minimization, we can prove statement while revealing the\\nabsolute minimum information necessary. This fits the definition of privacy, the\\npower to selectively reveal yourself, perfectly. I\'m sure I don\'t need to tell\\nanyone reading this but this is truly revolutionary. The technology is advancing\\nextremely fast and often it is our imagination that is the limit.\\n\\n![Zero knowledge](/img/building_private_infra_zk.png)\\n\\n### Waku\\n\\nWhat is Waku? It is a set of modular protocols for p2p communication. It has a\\nfocus on privacy, security and being able to run anywhere. It is the spiritual\\nsuccess to Whisper.\\n\\nBy modular we mean that you can pick and choose protocols and how you use them\\ndepending on constraints and trade-offs. For example, bandwidth usage vs\\nprivacy.\\n\\nIt is designed to work in resource restricted environments, such as mobile\\nphones and in web browsers. It is important that infrastructure meets users\\nwhere they are and supports their real-world use cases. Just like you don\'t need\\nyour own army and a castle to have your own private bathroom, you shouldn\'t need\\nto have a powerful always-on node to get reasonable privacy and\\ncensorship-resistance. We might call this self-sovereignty.\\n\\n### Waku - adaptive nodes\\n\\nOne way of looking at Waku is as an open service network. There are nodes with\\nvarying degrees of capabilities and requirements. For example when it comes to\\nbandwidth usage, storage, uptime, privacy requirements, latency requirements,\\nand connectivity restrictions.\\n\\nWe have a concept of adaptive nodes that can run a variety of protocols. A node\\noperator can choose which protocols they want to run. Naturally, there\'ll be\\nsome nodes that do more consumption and other nodes that do more provisioning.\\nThis gives rise to the idea of a service network, where services are provided\\nfor and consumed.\\n\\n![Adaptive Nodes](/img/building_private_infra_adaptive.png)\\n\\n### Waku - protocol interactions\\n\\nThere are many protocols that interact. Waku Relay protocol is based on libp2p\\nGossipSub for p2p messaging. We have filter for bandwidth-restricted nodes to\\nonly receive subset of messages. Lightpush for nodes with short connection\\nwindows to push messages into network. Store for nodes that want to retrieve\\nhistorical messages.\\n\\nOn the payload layer, we provide support for Noise handshakes/key-exchanges.\\nThis means that as a developers, you can get end-to-end encryption and expected\\nguarantees out of the box. We have support for setting up a secure channel from\\nscratch, and all of this paves the way for providing Signal\'s Double Ratchet at\\nthe protocol level much easier. We also have experimental support for\\nmulti-device usage. Similar features have existed in for example the Status app\\nfor a while, but with this we make it easier for any platform using Waku to use\\nit.\\n\\nThere are other protocols too, related to peer discovery, topic usage, etc. See\\n[specs](https://rfc.vac.dev/) for more details.\\n\\n<img src=\\"/img/building_private_infra_interactions.png\\" alt=\\"Protocol Interactions\\" />\\n\\n### Waku - Network\\n\\nFor the Waku network, there are a few problems. For example, when it comes to\\nnetwork spam and incentivizing service nodes. We want to address these while\\nkeeping privacy-guarantees of the base layer. I\'m going to go into both of\\nthese.\\n\\nThe spam problem arises on the gossip layer when anyone can overwhelm the\\nnetwork with messages. The service incentivization is a problem when nodes don\'t\\ndirectly benefit from the provisioning of a certain service. This can happen if\\nthey are not using the protocol directly themselves as part of normal operation,\\nor if they aren\'t socially inclined to provide a certain service. This depends a\\nlot on how an individual platform decides to use the network.\\n\\n![Waku Network](/img/building_private_infra_network.png)\\n\\n### Dealing with network spam and RLN Relay\\n\\nSince the p2p relay network is open to anyone, there is a problem with spam. If\\nwe look at existing solutions for dealing with spam in traditional messaging\\nsystems, a lot of entities like Google, Facebook, Twitter, Telegram, Discord use\\nphone number verification. While this is largely sybil-resistant, it is\\ncentralized and not private at all.\\n\\nHistorically, Whisper used PoW which isn\'t good for heterogenerous networks.\\nPeer scoring is open to sybil attacks and doesn\'t directly address spam\\nprotection in an anonymous p2p network.\\n\\nThe key idea here is to use RLN for private economic spam protection using\\nzkSNARKs.\\n\\nI\'m not going to go into too much detail of RLN here. If you are interested, I\\ngave a [talk](https://www.youtube.com/watch?v=g41nHQ0mLoA) in Amsterdam at\\nDevconnect about this. We have some write-ups on RLN\\n[here](https://vac.dev/rln-relay) by Sanaz who has been pushing a lot of this\\nfrom our side. There\'s also another talk at Devcon by Tyler going into RLN in\\nmore detail. Finally, here\'s the [RLN spec](https://rfc.vac.dev/vac/32/rln-v1).\\n\\nI\'ll briefly go over what it is, the interface and circuit and then talk about\\nhow it is used in Waku.\\n\\n### RLN - Overview and Flow\\n\\nRLN stands for Rate Limiting Nullifier. It is an anonyomous rate limiting\\nmechanism based on zkSNARKs. By rate limiting we mean you can only send N\\nmessages in a given period. By anonymity we mean that you can\'t link message to\\na publisher. We can think of it as a voting booth, where you are only allowed to\\nvote once every election.\\n\\n![Voting Booth](/img/building_private_infra_vote.png)\\n\\nIt can be used for spam protection in p2p messaging systems, and also rate\\nlimiting in general, such as for a decentralized captcha.\\n\\nThere are three parts to it. You register somewhere, then you can signal and\\nfinally there\'s a verification/slashing phase. You put some capital at risk,\\neither economic or social, and if you double signal you get slashed.\\n\\n### RLN - Circuit\\n\\nHere\'s what the private and public inputs to the circuit look like. The identity\\nsecret is generated locally, and we create an identity commitment that is\\ninserted into a Merkle tree. We then use Merkle proofs to prove membership.\\nRegistered member can only signal once for a given epoch or external nullifier,\\nfor example every ten seconds in Unix time. RLN identifer is for a specific RLN\\napp.\\n\\nWe also see what the circuit output looks like. This is calculated locally. `y`\\nis a share of the secret equation, and the (internal) nullifier acts as a unique\\nfingerprint for a given app/user/epoch combination. How do we calculate `y` and\\nthe internal nullifier?\\n\\n```\\n// Private input\\nsignal input identity_secret;\\nsignal input path_elements[n_levels][1];\\nsignal input identity_path_index[n_levels];\\n\\n// Public input\\nsignal input x; // signal_hash\\nsignal input epoch; // external_nullifier\\nsignal input rln_identifier;\\n\\n// Circuit output\\nsignal output y;\\nsignal output root;\\nsignal output nullifier;\\n```\\n\\n### RLN - Shamir\'s secret sharing\\n\\nThis is done using [Shamir\'s secret\\nsharing](https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing). Shamir\u2019s\\nsecret sharing is based on idea of splitting a secret into shares. This is how\\nwe enable slashing of funds.\\n\\nIn this case, we have two shares. If a given identity `a0` signals twice in\\nepoch/external nullifier, `a1` is the same. For a given RLN app,\\n`internal_nullifier` then stays the same. `x` is signal hash which is different,\\nand `y` is public, so we can reconstruct `identity_secret`. With the identity\\nsecret revealed, this gives access to e.g. financial stake.\\n\\n```\\na_0 = identity_secret // secret S\\na_1 = poseidonHash([a0, external_nullifier])\\n\\ny = a_0 + x * a_1\\n\\ninternal_nullifier = poseidonHash([a_1, rln_identifier])\\n```\\n\\n![Shamir\'s secret sharing](/img/building_private_infra_shamir.png)\\n\\n### RLN Relay\\n\\nThis is how RLN is used with Relay/GossipSub protocol. A node registers and\\nlocks up funds, and after that it can send messages. It publishes a message\\ncontaining the Zero Knowledge proof and some other details.\\n\\nEach relayer node listens to the membership contract for new members, and it\\nalso keeps track of relevant metadata and merkle tree. Metadata is needed to be\\nable to detect double signaling and perform slashing.\\n\\nBefore forwarding a message, it does some verification checks to ensure there\\nare no duplicate messages, ZKP is valid and no double signaling has occured. It\\nis worth noting that this can be combined with peer scoring, for example for\\nduplicate messages or invalid ZK proofs.\\n\\nIn line of Waku\'s goals of modularity, RLN Relay is applied on a specific subset\\nof pubsub and content topics. You can think of it as an extra secure channel.\\n\\n![RLN Relay](/img/building_private_infra_rlnrelay.png)\\n\\n### RLN Relay cross-client testnet\\n\\nWhere are we with RLN Relay deployment? We\'ve recently launched our second\\ntestnet. This is using RLN Relay with a smart contract on Goerli. It integrates\\nwith our example p2p chat application, and it does so through three different\\nclients, nwaku, go-waku and js-waku for browsers. This is our first p2p\\ncross-client testnet for RLN Relay.\\n\\nHere\'s a [video](https://www.youtube.com/watch?v=-vVrJWW0fls) that shows a user\\nregistering in a browser, signaling through JS-Waku. It then gets relayed to a\\nnwaku node, that verifies the proof. The second\\n[video](https://www.youtube.com/watch?v=Xz5q2ZhkFYs) shows what happens in the\\nspam case. when more than one message is sent in a given epoch, it detects it as\\nspam and discards it. Slashing hasn\'t been implemented fully yet in the client\\nand is a work in progress.\\n\\nIf you are curious and want to participate, you can join the effort on our [Vac\\nDiscord](https://discord.gg/PQFdubGt6d). We also have\\n[tutorials](https://github.com/status-im/nwaku/blob/master/docs/tutorial/rln-chat-cross-client.md)\\nsetup for all clients so you can play around with it.\\n\\nAs part of this, and to make it work in multiple different environments, we\'ve\\nalso been developing a new library called Zerokit. I\'ll talk about this a bit\\nlater.\\n\\n### Private settlement / Service credentials\\n\\nGoing back to the service network idea, let\'s talk about service credentials.\\nThe idea behind service credentials and private settlement is to enable two\\nactors to pay for and provide services without compromising their privacy. We do\\nnot want the payment to create a direct public link between the service provider\\nand requester.\\n\\nRecall the Waku service network illustration with adaptive nodes that choose\\nwhich protocols they want to run. Many of these protocols aren\'t very heavy and\\njust work by default. For example the relay protocol is enabled by default.\\nOther protocols are much heavier to provide, such as storing historical\\nmessages.\\n\\nIt is desirable to have additional incentives for this, especially for platforms\\nthat aren\'t community-based where some level of altruism can be assumed (e.g.\\nStatus Communities, or WalletConnect cloud infrastructure).\\n\\nYou have a node Alice that is often offline and wants to consume historical\\nmessages on some specific content topics. You have another node Bob that runs a\\nserver at home where they store historical messages for the last several weeks.\\nBob is happy to provide this service for free because he\'s excited about running\\nprivacy-preserving infrastructure and he\'s using it himself, but his node is\\ngetting overwhelmed by freeloaders and he feels like he should be paid something\\nfor continuing to provide this service.\\n\\nAlice deposits some funds in a smart contract which registers it in a tree,\\nsimilar to certain other private settlement mechanisms. A fee is taken or\\nburned. In exchange, she gets a set of tokens or service credentials. When she\\nwants to do a query with some criteria, she sends this to Bob. Bob responds with\\nsize of response, cost, and receiver address. Alice then sends a proof of\\ndelegation of a service token as a payment. Bob verifies the proof and resolves\\nthe query.\\n\\nThe end result is that Alice has consumed some service from Bob, and Bob has\\nreceived payment for this. There\'s no direct transaction link between Alice and\\nBob, and gas fees can be minimized by extending the period before settling on\\nchain.\\n\\nThis can be complemented with altruistic service provisioning, for example by\\nsplitting the peer pool into two slots, or only providing a few cheap queries\\nfor free.\\n\\nThe service provisioning is general, and can be generalized for any kind of\\nrequest/response service provisoning that we want to keep private.\\n\\nThis isn\'t a perfect solution, but it is an incremental improvement on top of\\nthe status quo. It can be augmented with more advanced techniques such as better\\nnon-repudiable node reputation, proof of correct service provisioning, etc.\\n\\nWe are currently in the raw spec / proof of concept stage of this. We expect to\\nlaunch a testnet of this later this year or early next year.\\n\\n![Service credentials flow](/img/building_private_infra_servicecred.png)\\n\\n### Zerokit\\n\\n[Zerokit](https://github.com/vacp2p/zerokit) is a set of Zero Knowledge modules,\\nwritten in Rust and designed to be used in many different environments. The\\ninitial goal is to get the best of both worlds with Circom/Solidity/JS and\\nRust/ZK ecosystem. This enables people to leverage Circom-based constructs from\\nnon-JS environments.\\n\\nFor the RLN module, it is using Circom circuits via ark-circom and Rust for\\nscaffolding. It exposes a C FFI API that can be used through other system\\nprogramming environments, like Nim and Go. It also exposes an experimental WASM\\nAPI that can be used through web browsers.\\n\\nWaku is p2p infrastructure running in many different environments, such as\\nNim/JS/Go/Rust, so this a requirement for us.\\n\\nCircom and JS strengths are access to Dapp developers, tooling, generating\\nverification code, circuits etc. Rust strengths is that it is systems-based and\\neasy to interface with other language runtime such as Nim, Go, Rust, C. It also\\ngives access to other Rust ZK ecosystems such as arkworks. This opens door for\\nusing other constructs, such as Halo2. This becomes especially relevant for\\nconstructs where you don\'t want to do a trusted setup or where circuits are more\\ncomplex/custom and performance requirements are higher.\\n\\nIn general with Zerokit, we want to make it easy to build and use ZKP in a\\nmultitude of environments, such as mobile phones and web browsers. Currently it\\nis too complex to write privacy-protecting infrastructure with ZKPs considering\\nall the languages and tools you have to learn, from JS, Solidity and Circom to\\nRust, WASM and FFI. And that isn\'t even touching on things like secure key\\nstorage or mobile dev. Luckily more and more projects are working on this,\\nincluding writing DSLs etc. It\'d also be exciting if we can make a useful\\ntoolstack for JS-less ZK dev to reduce cognitive overhead, similar to what we\\nhave with something like Foundry.\\n\\n### Other research\\n\\nI also want to mention a few other things we are doing. One thing is\\n[protocol specifications](https://rfc.vac.dev/). We think this is very important\\nfor p2p infra, and we see a lot of other projects that claim to do it p2p\\ninfrastructure but they aren\'t clear about guarantees or how stable something\\nis. That makes it hard to have multiple implementations, to collaborate across\\ndifferent projects, and to analyze things objectively.\\n\\nRelated to that is publishing [papers](https://vac.dev/publications). We\'ve put\\nout three so far, related to Waku and RLN-Relay. This makes it easier to\\ninterface with academia. There\'s a lot of good researchers out there and we want\\nto build a better bridge between academia and industry.\\n\\nAnother thing is [network](https://vac.dev/wakuv2-relay-anon)\\n[privacy](https://github.com/vacp2p/research/issues/107). Waku is modular with\\nrespect to privacy guarantees, and there are a lot of knobs to turn here\\ndepending on specific deployments. For example, if you are running the full\\nrelay protocol you currently have much stronger receiver anonymity than if you\\nare running filter protocol from a bandwidth or connectivity-restricted node.\\n\\nWe aim to make this pluggable depending on user needs. E.g. mixnets such as Nym\\ncome with some trade-offs but are a useful tool in the arsenal. A good mental\\nmodel to keep in mind is the anonymity trilemma, where you can only pick 2/3 out\\nof low latency, low bandwidth usage and strong anonymity.\\n\\nWe are currently exploring [Dandelion-like\\nadditions](https://github.com/vacp2p/research/issues/119) to the relay/gossip\\nprotocol, which would provide for stronger sender anonymity, especially in a\\nmulti-node/botnet attacker model. As part of this we are looking into different\\nparameters choices and general possibilities for lower latency usage. This could\\nmake it more amenable for latency sensitive environments, such as validator\\nprivacy, for specific threat models. The general theme here is we want to be\\nrigorous with the guarantees we provide, under what conditions and for what\\nthreat models.\\n\\nAnother thing mentioned earlier is [Noise payload\\nencryption](https://vac.dev/wakuv2-noise), and specifically things like allowing\\nfor pairing different devices with e.g. QR codes. This makes it easier for\\ndevelopers to provide secure messaging in many realistic scenarios in a\\nmulti-device world.\\n\\n![Other research](/img/building_private_infra_misc.png)\\n\\n### Summary\\n\\nWe\'ve gone over what privacy-protecting infrastructure is, why we want it and\\nhow we can build it. We\'ve seen how ZK is a fundamental building block for this.\\nWe\'ve looked at Waku, the communication layer for Web3, and how it uses Zero\\nKnowledge proofs to stay private and function better. We\'ve also looked at\\nZerokit and how we can make it easier to do ZKP in different environments.\\n\\nFinally we also looked at some other research we\'ve been doing. All of the\\nthings mentioned in this article, and more, is available as\\n[write-ups](https://vac.dev/research), [specs](https://rfc.vac.dev/), or\\ndiscussions on our [forum](forum.vac.dev/) or [Github](github.com/vacp2p/).\\n\\nIf you find any of this exciting to work on, feel free to reach out on our\\nDiscord. We are also [hiring](https://jobs.status.im/), and we have started\\nexpanding into other privacy infrastructure tech like private and provable\\ncomputation with ZK-WASM."},{"id":"wakuv2-relay-anon","metadata":{"permalink":"/rlog/wakuv2-relay-anon","source":"@site/rlog/2022-07-22-relay-anonymity.mdx","title":"Waku Privacy and Anonymity Analysis Part I: Definitions and Waku Relay","description":"Introducing a basic threat model and privacy/anonymity analysis for the Waku v2 relay protocol.","date":"2022-07-22T10:00:00.000Z","formattedDate":"July 22, 2022","tags":[],"readingTime":16.785,"hasTruncateMarker":true,"authors":[{"name":"Daniel","github":"kaiserd","key":"kaiserd"}],"frontMatter":{"layout":"post","name":"Waku Privacy and Anonymity Analysis Part I: Definitions and Waku Relay","title":"Waku Privacy and Anonymity Analysis Part I: Definitions and Waku Relay","date":"2022-07-22T10:00:00.000Z","authors":"kaiserd","published":true,"slug":"wakuv2-relay-anon","categories":"research","image":"/img/anonymity_trilemma.svg","discuss":"https://forum.vac.dev/t/discussion-waku-privacy-and-anonymity-analysis/149","_includes":["math"],"toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Building Privacy-Protecting Infrastructure","permalink":"/rlog/building-privacy-protecting-infrastructure"},"nextItem":{"title":"Noise handshakes as key-exchange mechanism for Waku","permalink":"/rlog/wakuv2-noise"}},"content":"Introducing a basic threat model and privacy/anonymity analysis for the Waku v2 relay protocol.\\n\\n\x3c!--truncate--\x3e\\n\\n[Waku v2](https://rfc.vac.dev/waku/standards/core/10/waku2) enables secure, privacy preserving communication using a set of modular P2P protocols.\\nWaku v2 also aims at protecting the user\'s anonymity.\\nThis post is the first in a series about Waku v2 security, privacy, and anonymity.\\nThe goal is to eventually have a full privacy and anonymity analysis for each of the Waku v2 protocols, as well as covering the interactions of various Waku v2 protocols.\\nThis provides transparency with respect to Waku\'s current privacy and anonymity guarantees, and also identifies weak points that we have to address.\\n\\nIn this post, we first give an informal description of security, privacy and anonymity in the context of Waku v2.\\nFor each definition, we summarize Waku\'s current guarantees regarding the respective property.\\nWe also provide attacker models, an attack-based threat model, and a first anonymity analysis of [Waku v2 relay](https://rfc.vac.dev/waku/standards/core/11/relay) within the respective models.\\n\\nWaku comprises many protocols that can be combined in a modular way.\\nFor our privacy and anonymity analysis, we start with the relay protocol because it is at the core of Waku v2 enabling Waku\'s publish subscribe approach to P2P messaging.\\nIn its current form, Waku relay is a minor extension of [libp2p GossipSub](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/README.md).\\n\\n![Figure 1: The Waku v2 relay mesh is based on the [GossipSub mesh](https://docs.libp2p.io/concepts/publish-subscribe#types-of-peering)](/img/libp2p_gossipsub_types_of_peering.png)\\n\\n## Informal Definitions: Security, Privacy, and Anonymity\\n\\nThe concepts of security, privacy, and anonymity are linked and have quite a bit of overlap.\\n\\n### Security\\n\\nOf the three, [Security](https://en.wikipedia.org/wiki/Information_security) has the clearest agreed upon definition,\\nat least regarding its key concepts: _confidentiality_, _integrity_, and _availability_.\\n\\n- confidentiality: data is not disclosed to unauthorized entities.\\n- integrity: data is not modified by unauthorized entities.\\n- availability: data is available, i.e. accessible by authorized entities.\\n\\nWhile these are the key concepts, the definition of information security has been extended over time including further concepts,\\ne.g. [authentication](https://en.wikipedia.org/wiki/Authentication) and [non-repudiation](https://en.wikipedia.org/wiki/Non-repudiation).\\nWe might cover these in future posts.\\n\\n### Privacy\\n\\nPrivacy allows users to choose which data and information\\n\\n- they want to share\\n- and with whom they want to share it.\\n\\nThis includes data and information that is associated with and/or generated by users.\\nProtected data also comprises metadata that might be generated without users being aware of it.\\nThis means, no further information about the sender or the message is leaked.\\nMetadata that is protected as part of the privacy-preserving property does not cover protecting the identities of sender and receiver.\\nIdentities are protected by the [anonymity property](#anonymity).\\n\\nOften privacy is realized by the confidentiality property of security.\\nThis neither makes privacy and security the same, nor the one a sub category of the other.\\nWhile security is abstract itself (its properties can be realized in various ways), privacy lives on a more abstract level using security properties.\\nPrivacy typically does not use integrity and availability.\\nAn adversary who has no access to the private data, because the message has been encrypted, could still alter the message.\\n\\nWaku offers confidentiality via secure channels set up with the help of the [Noise Protocol Framework](https://noiseprotocol.org/).\\nUsing these secure channels, message content is only disclosed to the intended receivers.\\nThey also provide good metadata protection properties.\\nHowever, we do not have a metadata protection analysis as of yet,\\nwhich is part of our privacy/anonymity roadmap.\\n\\n### Anonymity\\n\\nPrivacy and anonymity are closely linked.\\nBoth the identity of a user and data that allows inferring a user\'s identity should be part of the privacy policy.\\nFor the purpose of analysis, we want to have a clearer separation between these concepts.\\n\\nWe define anonymity as _unlinkablity of users\' identities and their shared data and/or actions_.\\n\\nWe subdivide anonymity into _receiver anonymity_ and _sender anonymity_.\\n\\n#### Receiver Anonymity\\n\\nWe define receiver anonymity as _unlinkability of users\' identities and the data they receive and/or related actions_.\\nThe data transmitted via Waku relay must be a [Waku message](https://rfc.vac.dev/waku/standards/core/14/message), which contains a content topic field.\\nBecause each message is associated with a content topic, and each receiver is interested in messages with specific content topics,\\nreceiver anonymity in the context of Waku corresponds to _subscriber-topic unlinkability_.\\nAn example for the \\"action\\" part of our receiver anonymity definition is subscribing to a specific topic.\\n\\nThe Waku message\'s content topic is not related to the libp2p pubsub topic.\\nFor now, Waku uses a single libp2p pubsub topic, which means messages are propagated via a single mesh of peers.\\nWith this, the receiver discloses its participation in Waku on the gossipsub layer.\\nWe will leave the analysis of libp2p gossipsub to a future article within this series, and only provide a few hints and pointers here.\\n\\nWaku offers k-anonymity regarding content topic interest in the global adversary model.\\n[K-anonymity](https://en.wikipedia.org/wiki/K-anonymity) in the context of Waku means an attacker can link receivers to content topics with a maximum certainty of $1/k$.\\nThe larger $k$, the less certainty the attacker gains.\\nReceivers basically hide in a pool of $k$ content topics, any subset of which could be topics they subscribed to.\\nThe attacker does not know which of those the receiver actually subscribed to,\\nand the receiver enjoys [plausible deniability](https://en.wikipedia.org/wiki/Plausible_deniability#Use_in_cryptography) regarding content topic subscription.\\nAssuming there are $n$ Waku content topics, a receiver has $n$-anonymity with respect to association to a specific content topic.\\n\\nTechnically, Waku allows distributing messages over several libp2p pubsub topics.\\nThis yields $k$-anonymity, assuming $k$ content topics share the same pubsub topic.\\nHowever, if done wrongly, such sharding of pubsub topics can breach anonymity.\\nA formal specification of anonymity-preserving topic sharding building on the concepts of [partitioned topics](https://rfc.vac.dev/status/deprecated/10/waku-usage#partitioned-topic) is part of our roadmap.\\n\\nAlso, Waku is not directly concerned with 1:1 communication, so for this post, 1:1 communication is out of scope.\\nChannels for 1:1 communication can be implemented on top of Waku relay.\\nIn the future, a 1:1 communication protocol might be added to Waku.\\nSimilar to topic sharding, it would maintain receiver anonymity leveraging [partitioned topics](https://rfc.vac.dev/status/deprecated/10/waku-usage/#partitioned-topic).\\n\\n#### Sender Anonymity\\n\\nWe define sender anonymity as _unlinkability of users\' identities and the data they send and/or related actions_.\\nBecause the data in the context of Waku is Waku messages, sender anonymity corresponds to _sender-message unlinkability_.\\n\\nIn summary, Waku offers weak sender anonymity because of [Waku\'s strict no sign policy](https://rfc.vac.dev/waku/standards/core/11/relay),\\nwhich has its origins in the [Ethereum consensus specs](https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/p2p-interface.md#why-are-we-using-the-strictnosign-signature-policy).\\n[17/WAKU-RLN-RELAY](https://rfc.vac.dev/waku/standards/core/17/rln-relay) and [18/WAKU2-SWAP](https://rfc.vac.dev/waku/deprecated/18/swap) mitigate replay and injection attacks.\\n\\nWaku currently does not offer sender anonymity in stronger attacker models, as well as cannot protect against targeted attacks in weaker attacker models like the single or multi node attacker.\\nWe will cover this in more detail in later sections.\\n\\n### Anonymity Trilemma\\n\\n[The Anonymity trilemma](https://freedom.cs.purdue.edu/projects/trilemma.html) states that only two out of _strong anonymity_, _low bandwidth_, and _low latency_ can be guaranteed in the global on-net attacker model.\\nWaku\'s goal, being a modular set of protocols, is to offer any combination of two out of these three properties, as well as blends.\\nAn example for blending is an adjustable number of pubsub topics and peers in the respective pubsub topic mesh; this allows tuning the trade-off between anonymity and bandwidth.\\n\\n![Figure 2: Anonymity Trilemma: pick two. ](/img/anonymity_trilemma.svg)\\n\\nA fourth factor that influences [the anonymity trilemma](https://freedom.cs.purdue.edu/projects/trilemma.html) is _frequency and patterns_ of messages.\\nThe more messages there are, and the more randomly distributed they are, the better the anonymity protection offered by a given anonymous communication protocol.\\nSo, incentivising users to use the protocol, for instance by lowering entry barriers, helps protecting the anonymity of all users.\\nThe frequency/patterns factor is also related to the above described k-anonymity.\\n\\n### Censorship Resistance\\n\\nAnother security related property that Waku aims to offer is censorship resistance.\\nCensorship resistance guarantees that users can participate even if an attacker tries to deny them access.\\nSo, censorship resistance ties into the availability aspect of security.\\nIn the context of Waku that means users should be able to send messages as well as receive all messages they are interested in,\\neven if an attacker tries to prevent them from disseminating messages or tries to deny them access to messages.\\n\\nCurrently, Waku only guarantees censorship resistance in the weak single node attacker model.\\nWhile currently employed secure channels mitigate targeted censorship, e.g. blocking specific content topics,\\ngeneral censorship resistance in strong attacker models is part of our roadmap.\\nAmong other options, we will investigate [Pluggable Transports](https://www.pluggabletransports.info/about/) in future articles.\\n\\n## Attacker Types\\n\\nThe following lists various attacker types with varying degrees of power.\\nThe more power an attacker has, the more difficult it is to gain the respective attacker position.\\n\\nEach attacker type comes in a passive and an active variant.\\nWhile a passive attacker can stay hidden and is not suspicious,\\nthe respective active attacker has more (or at least the same) deanonymization power.\\n\\nWe also distinguish between internal and external attackers.\\n\\n### Internal\\n\\nWith respect to Waku relay, an internal attacker participates in the same pubsub topic as its victims.\\nWithout additional measures on higher layer protocols, access to an internal position is easy to get.\\n\\n#### Single Node\\n\\nThis attacker controls a single node.\\nBecause this position corresponds to normal usage of Waku relay, it is trivial to obtain.\\n\\n#### Multi Node\\n\\nThis attacker controls several nodes. We assume a smaller static number of controlled nodes.\\nThe multi node position can be achieved relatively easily by setting up multiple nodes.\\nBotnets might be leveraged to increase the number of available hosts.\\nMulti node attackers could use [Sybil attacks](https://en.wikipedia.org/wiki/Sybil_attack) to increase the number of controlled nodes.\\nA countermeasure is for nodes to only accept libp2p gossipsub graft requests from peers with different IP addresses, or even different subnets.\\n\\n#### Linearly Scaling Nodes\\n\\nThis attacker controls a number of nodes that scales linearly with the number of nodes in the network.\\nThis attacker is especially interesting to investigate in the context of DHT security,\\nwhich Waku uses for ambient peer discovery.\\n\\n### External\\n\\nAn external attacker can only see encrypted traffic (protected by a secure channel set up with [WAKU2-NOISE](https://github.com/waku-org/specs/blob/master/standards/application/noise.md)).\\nBecause an internal position can be easily obtained,\\nin practice external attackers would mount combined attacks that leverage both internal an external attacks.\\nWe cover this more below when describing attacks.\\n\\n#### Local\\n\\nA local attacker has access to communication links in a local network segment.\\nThis could be a rogue access point (with routing capability).\\n\\n#### AS\\n\\nAn AS attacker controls a single AS (autonomous system).\\nA passive AS attacker can listen to traffic on arbitrary links within the AS.\\nAn active AS attacker can drop, inject, and alter traffic on arbitrary links within the AS.\\n\\nIn practice, a malicious ISP would be considered as an AS attacker.\\nA malicious ISP could also easily setup a set of nodes at specific points in the network,\\ngaining internal attack power similar to a strong multi node attacker.\\n\\n#### Global On-Net\\n\\nA global on-net attacker has complete overview over the whole network.\\nA passive global attacker can listen to traffic on all links,\\nwhile the active global attacker basically carries the traffic: it can freely drop, inject, and alter traffic at all positions in the network.\\nThis basically corresponds to the [Dolev-Yao model](https://en.wikipedia.org/wiki/Dolev%E2%80%93Yao_model).\\n\\nAn entity with this power would, in practice, also have the power of the internal linearly scaling nodes attacker.\\n\\n## Attack-based Threat Analysis\\n\\nThe following lists various attacks including the weakest attacker model in which the attack can be successfully performed.\\nThe respective attack can be performed in all stronger attacker models as well.\\n\\nAn attack is considered more powerful if it can be successfully performed in a weaker attacker model.\\n\\nIf not stated otherwise, we look at these attacks with respect to their capability to deanonymize the message sender.\\n\\n### Scope\\n\\nIn this post, we introduce a simple tightly scoped threat model for Waku v2 Relay, which will be extended in the course of this article series.\\n\\nIn this first post, we will look at the relay protocol in isolation.\\nEven though many threats arise from layers Waku relay is based on, and layers that in turn live on top of relay,\\nwe want to first look at relay in isolation because it is at the core of Waku v2.\\nAddressing and trying to solve all security issues of a complex system at once is an overwhelming task, which is why we focus on the soundness of relay first.\\n\\nThis also goes well with the modular design philosophy of Waku v2, as layers of varying levels of security guarantees can be built on top of relay, all of which can relay on the guarantees that Waku provides.\\nInstead of looking at a multiplicative explosion of possible interactions, we look at the core in this article, and cover the most relevant combinations in future posts.\\n\\nFurther restricting the scope, we will look at the data field of a relay message as a black box.\\nIn a second article on Waku v2 relay, we will look into the data field, which according to the [specification of Waku v2 relay](https://rfc.vac.dev/waku/standards/core/11/relay#message-fields) must be a [Waku v2 message](https://rfc.vac.dev/waku/standards/core/14/message).\\nWe only consider messages with version field `2`, which indicates that the payload has to be encoded using [WAKU2-NOISE](https://github.com/waku-org/specs/blob/master/standards/application/noise.md).\\n\\n### Prerequisite: Get a Specific Position in the Network\\n\\nSome attacks require the attacker node(s) to be in a specific position in the network.\\nIn most cases, this corresponds to trying to get into the mesh peer list for the desired pubsub topic of the victim node.\\n\\nIn libp2p gossipsub, and by extension Waku v2 relay, nodes can simply send a graft message for the desired topic to the victim node.\\nIf the victim node still has open slots, the attacker gets the desired position.\\nThis only requires the attacker to know the gossipsub multiaddress of the victim node.\\n\\nA linearly scaling nodes attacker can leverage DHT based discovery systems to boost the probability of malicious nodes being returned, which in turn significantly increases the probability of attacker nodes ending up in the peer lists of victim nodes.\\n[Waku v2 discv5](https://vac.dev/wakuv2-apd) will employ countermeasures that mitigate the amplifying effect this attacker type can achieve.\\n\\n### Replay Attack\\n\\nIn the scope we defined above, Waku v2 is resilient against replay attacks.\\nGossipSub nodes, and by extension Waku relay nodes, feature a `seen` cache, and only relay messages they have not seen before.\\nFurther, replay attacks will be punished by [RLN](https://rfc.vac.dev/waku/standards/core/17/rln-relay) and [SWAP](https://rfc.vac.dev/waku/deprecated/18/swap).\\n\\n### Neighbourhood Surveillance\\n\\nThis attack can be performed by a single node attacker that is connected to all peers of the victim node $v$ with respect to a specific topic mesh.\\nThe attacker also has to be connected to $v$.\\nIn this position, the attacker will receive messages $m_v$ sent by $v$ both on the direct path from $v$, and on indirect paths relayed by peers of $v$.\\nIt will also receive messages $m_x$ that are not sent by $v$. These messages $m_x$ are relayed by both $v$ and the peers of $v$.\\nMessages that are received (significantly) faster from $v$ than from any other of $v$\'s peers are very likely messages that $v$ sent,\\nbecause for these messages the attacker is one hop closer to the source.\\n\\nThe attacker can (periodically) measure latency between itself and $v$, and between itself and the peers of $v$ to get more accurate estimates for the expected timings.\\nAn AS attacker (and if the topology allows, even a local attacker) could also learn the latency between $v$ and its well-behaving peers.\\nAn active AS attacker could also increase the latency between $v$ and its peers to make the timing differences more prominent.\\nThis, however, might lead to $v$ switching to other peers.\\n\\nThis attack cannot (reliably) distinguish messages $m_v$ sent by $v$ from messages $m_y$ relayed by peers of $v$ the attacker is not connected to.\\nStill, there are hop-count variations that might be leveraged.\\nMessages $m_v$ always have a hop-count of 1 on the path from $v$ to the attacker, while all other paths are longer.\\nMessages $m_y$ might have the same hop-count on the path from $v$ as well as on other paths.\\n\\n### Controlled Neighbourhood\\n\\nIf a multi node attacker manages to control all peers of the victim node, it can trivially tell which messages originated from $v$.\\n\\n### Observing Messages\\n\\nIf Waku relay was not protected with Noise, the AS attacker could simply check for messages leaving $v$ which have not been relayed to $v$.\\nThese are the messages sent by $v$.\\nWaku relay protects against this attack by employing secure channels setup using Noise.\\n\\n### Correlation\\n\\nMonitoring all traffic (in an AS or globally), allows the attacker to identify traffic correlated with messages originating from $v$.\\nThis (alone) does not allow an external attacker to learn which message $v$ sent, but it allows identifying the respective traffic propagating through the network.\\nThe more traffic in the network, the lower the success rate of this attack.\\n\\nCombined with just a few nodes controlled by the attacker, the actual message associated with the correlated traffic can eventually be identified.\\n\\n### DoS\\n\\nAn active single node attacker could run a disruption attack by\\n\\n- (1) dropping messages that should be relayed\\n- (2) flooding neighbours with bogus messages\\n\\nWhile (1) has a negative effect on availability, the impact is not significant.\\nA linearly scaling botnet attacker, however, could significantly disrupt the network with such an attack.\\n(2) is thwarted by [RLN](https://rfc.vac.dev/waku/standards/core/17/rln-relay).\\nAlso [SWAP](https://rfc.vac.dev/waku/deprecated/18/swap) helps mitigating DoS attacks.\\n\\nA local attacker can DoS Waku by dropping all Waku traffic within its controlled network segment.\\nAn AS attacker can DoS Waku within its authority, while a global attacker can DoS the whole network.\\nA countermeasure are censorship resistance techniques like [Pluggable Transports](https://www.pluggabletransports.info/about/).\\n\\n## Summary and Future Work\\n\\nCurrently, Waku v2 relay offers k-anonymity with respect to receiver anonymity.\\nThis also includes k-anonymity towards legitimate members of the same topic.\\n\\nWaku v2 relay offers sender anonymity in the single node attacker model with its [strict no sign policy](https://rfc.vac.dev/waku/standards/core/11/relay/#signature-policy).\\nCurrently, Waku v2 does not guarantee sender anonymity in the multi node and stronger attacker models.\\nHowever, we are working on modular anonymity-preserving protocols and building blocks as part of our privacy/anonymity roadmap.\\nThe goal is to allow tunable anonymity with respect to trade offs between _strong anonymity_, _low bandwidth_, and _low latency_.\\nAll of these cannot be fully guaranteed as the [the anonymity trilemma](https://freedom.cs.purdue.edu/projects/trilemma.html) states.\\nSome applications have specific requirements, e.g. low latency, which require a compromise on anonymity.\\nAnonymity-preserving mechanisms we plan to investigate and eventually specify as pluggable anonymity protocols for Waku comprise\\n\\n- [Dandelion++](https://arxiv.org/abs/1805.11060) for lightweight anonymity;\\n- [onion routing](https://en.wikipedia.org/wiki/Onion_routing) as a building block adding a low latency anonymization layer;\\n- [a mix network](https://en.wikipedia.org/wiki/Mix_network) for providing strong anonymity (on top of onion routing) even in the strongest attacker model at the cost of higher latency.\\n\\nThese pluggable anonymity-preserving protocols will form a sub-set of the Waku v2 protocol set.\\nAs an intermediate step, we might directly employ Tor for onion-routing, and [Nym](https://nymtech.net/) as a mix-net layer.\\n\\nIn future research log posts, we will cover further Waku v2 protocols and identify anonymity problems that will be added to our roadmap.\\nThese protocols comprise\\n\\n- [13/WAKU2-STORE](https://rfc.vac.dev/waku/standards/core/13/store), which can violate receiver anonymity as it allows filtering by content topic.\\n  A countermeasure is using the content topic exclusively for local filters.\\n- [12/WAKU2-FILTER](https://rfc.vac.dev/waku/standards/core/12/filter), which discloses nodes\' interest in topics;\\n- [19/WAKU2-LIGHTPUSH](https://rfc.vac.dev/waku/standards/core/19/lightpush), which also discloses nodes\' interest in topics and links the lightpush client as the sender of a message to the lightpush service node;\\n- [21/WAKU2-FTSTORE](https://rfc.vac.dev/waku/standards/application/21/fault-tolerant-store), which discloses nodes\' interest in specific time ranges allowing to infer information like online times.\\n\\nWhile these protocols are not necessary for the operation of Waku v2, and can be seen as pluggable features,\\nwe aim to provide alternatives without the cost of lowering the anonymity level.\\n\\n## References\\n\\n- [10/WAKU2](https://rfc.vac.dev/waku/standards/core/10/waku2)\\n- [11/WAKU2-RELAY](https://rfc.vac.dev/waku/standards/core/11/relay)\\n- [libp2p GossipSub](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/README.md)\\n- [Security](https://en.wikipedia.org/wiki/Information_security)\\n- [Authentication](https://en.wikipedia.org/wiki/Authentication)\\n- [Non-repudiation](https://en.wikipedia.org/wiki/Non-repudiation)\\n- [Noise Protocol Framework](https://noiseprotocol.org/)\\n- [plausible deniability](https://en.wikipedia.org/wiki/Plausible_deniability#Use_in_cryptography)\\n- [Waku v2 message](https://rfc.vac.dev/waku/standards/core/14/message)\\n- [partitioned topics](https://rfc.vac.dev/status/deprecated/10/waku-usage\\n#partitioned-topic)\\n- [Sybil attack](https://en.wikipedia.org/wiki/Sybil_attack)\\n- [Dolev-Yao model](https://en.wikipedia.org/wiki/Dolev%E2%80%93Yao_model)\\n- [WAKU2-NOISE](https://github.com/waku-org/specs/blob/master/standards/application/noise.md)\\n- [33/WAKU2-DISCV5](https://vac.dev/wakuv2-apd)\\n- [strict no sign policy](https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/p2p-interface.md#why-are-we-using-the-strictnosign-signature-policy)\\n- [Waku v2 strict no sign policy](https://rfc.vac.dev/waku/standards/core/11/relay#signature-policy)\\n- [17/WAKU-RLN-RELAY](https://rfc.vac.dev/waku/standards/core/17/rln-relay)\\n- [anonymity trilemma](https://freedom.cs.purdue.edu/projects/trilemma.html)\\n- [18/WAKU2-SWAP](https://rfc.vac.dev/waku/deprecated/18/swap)\\n- [Pluggable Transports](https://www.pluggabletransports.info/about/)\\n- [Nym](https://nymtech.net/)\\n- [Dandelion++](https://arxiv.org/abs/1805.11060)\\n- [13/WAKU2-STORE](https://rfc.vac.dev/waku/standards/core/13/store)\\n- [12/WAKU2-FILTER](https://rfc.vac.dev/waku/standards/core/12/filter)\\n- [19/WAKU2-LIGHTPUSH](https://rfc.vac.dev/waku/standards/core/19/lightpush)\\n- [21/WAKU2-FTSTORE](https://rfc.vac.dev/waku/standards/application/21/fault-tolerant-store)"},{"id":"wakuv2-noise","metadata":{"permalink":"/rlog/wakuv2-noise","source":"@site/rlog/2022-05-17-noise.mdx","title":"Noise handshakes as key-exchange mechanism for Waku","description":"We provide an overview of the Noise Protocol Framework as a tool to design efficient and secure key-exchange mechanisms in Waku2.","date":"2022-05-17T10:00:00.000Z","formattedDate":"May 17, 2022","tags":[],"readingTime":21.115,"hasTruncateMarker":true,"authors":[{"name":"s1fr0","github":"s1fr0","key":"s1fr0"}],"frontMatter":{"layout":"post","name":"Noise handshakes as key-exchange mechanism for Waku","title":"Noise handshakes as key-exchange mechanism for Waku","date":"2022-05-17T10:00:00.000Z","authors":"s1fr0","published":true,"slug":"wakuv2-noise","categories":"research","summary":null,"image":"/img/noise/NM.png","discuss":"https://forum.vac.dev/t/discussion-noise-handshakes-as-key-exchange-mechanism-for-waku/137","_includes":["math"]},"prevItem":{"title":"Waku Privacy and Anonymity Analysis Part I: Definitions and Waku Relay","permalink":"/rlog/wakuv2-relay-anon"},"nextItem":{"title":"Waku v2 Ambient Peer Discovery","permalink":"/rlog/wakuv2-apd"}},"content":"We provide an overview of the Noise Protocol Framework as a tool to design efficient and secure key-exchange mechanisms in Waku2.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nIn this post we will provide an overview of how [Waku v2](https://rfc.vac.dev/waku/standards/core/10/waku2) users can adopt [Noise handshakes](http://www.noiseprotocol.org/noise.html) to agree on cryptographic keys used to securely encrypt messages.\\n\\nThis process belongs to the class of _key-exchange_ mechanisms, consisting of all those protocols that, with different levels of complexity and security guarantees, allow two parties to publicly agree on a secret without letting anyone else know what this secret is.\\n\\nBut why do we need key-exchange mechanisms in the first place?\\n\\nWith the advent of [public-key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography), it become possible to decouple encryption from decryption through use of two distinct cryptographic keys: one _public_, used to encrypt information and that can be made available to anyone, and one _private_ (kept secret), which enables decryption of messages encrypted with its corresponding public key. The same does not happen in the case of [symmetric encryption schemes](https://en.wikipedia.org/wiki/Symmetric-key_algorithm) where, instead, the same key is used for both encryption and decryption operations and hence cannot be publicly revealed as for public keys.\\n\\nIn order to address specific application needs, many different public, symmetric and hybrid cryptographic schemes were designed: [Waku v1](https://rfc.vac.dev/spec/6/) and [Waku v2](https://rfc.vac.dev/waku/standards/core/10/waku2), which inherits part of their design from the Ethereum messaging protocol [Whisper](https://ethereum.org/en/developers/docs/networking-layer/#whisper), provide [support](https://rfc.vac.dev/waku/standards/application/26/payload) to both public-key primitives ([`ECIES`](https://en.wikipedia.org/wiki/Integrated_Encryption_Scheme), [`ECDSA`](https://en.wikipedia.org/wiki/Elliptic_Curve_Digital_Signature_Algorithm)) and symmetric primitives ([`AES-256-GCM`](https://en.wikipedia.org/wiki/Galois/Counter_Mode), [`KECCAK-256`](https://en.wikipedia.org/wiki/SHA-3)), used to sign, hash, encrypt and decrypt exchanged messages.\\n\\nIn principle, when communications employ public-key based encryption schemes (`ECIES`, in the case of Waku), there is no need for a key-agreement among parties: messages can be directly encrypted using the recipient\'s public-key before being sent over the network. However, public-key encryption and decryption primitives are usually very inefficient in processing large amount of data, and this may constitute a bottleneck for many of today\'s applications. Symmetric encryption schemes such as `AES-256-GCM`, on the other hand, are much more efficient, but the encryption/decryption key needs to be shared among users beforehand any encrypted messages is exchanged.\\n\\nTo counter the downsides given by each of these two approaches while taking advantage of their strengths, hybrid constructions were designed. In these, public-key primitives are employed to securely agree on a secret key which, in turn, is used with a symmetric cipher for encrypting messages. In other words, such constructions specify a (public-key based) key-agreement mechanism!\\n\\nWaku, up to [payload version 1](https://rfc.vac.dev/waku/standards/core/14/message/#payload-encryption), does not implement nor recommend any protocol for exchanging symmetric ciphers\' keys, leaving such task to the application layer. It is important to note that the kind of key-agreement employed has a direct impact on the security properties that can be granted on later encrypted messages, while security requirements usually depend on the specific application for which encryption is needed in the first place.\\n\\nIn this regard, [Status](https://status.im), which builds on top of Waku, [implements](https://rfc.vac.dev/status/deprecated/secure-transport) a custom version of the [X3DH](https://signal.org/docs/specifications/x3dh/) key-agreement protocol, in order to allow users to instantiate end-to-end encrypted communication channels. However, although such a solution is optimal when applied to (distributed) E2E encrypted chats, it is not flexible enough to fit or simplify the variety of applications Waku aims to address.\\nHence, proposing and implementing one or few key-agreements which provide certain (presumably _strong_) security guarantees, would inevitably degrade performances of all those applications for which, given their security requirements, more tailored and efficient key-exchange mechanisms can be employed.\\n\\nGuided by different examples, in the following sections we will overview Noise, a protocol framework we are [currently integrating](https://github.com/waku-org/specs/blob/master/standards/application/noise.md) in Waku, for building secure key-agreements between two parties. One of the great advantage of using Noise is that it is possible to add support to new key-exchanges by just specifying users\' actions from a predefined list, requiring none to minimal modifications to existing implementations. Furthermore, Noise provides a framework to systematically analyze protocols\' security properties and the corresponding attacker threat models. This allows not only to easily design new key-agreements eventually optimized for specific applications we want to address, but also to easily analyze or even [formally verify](https://noiseexplorer.com/) any of such custom protocol!\\n\\nWe believe that with its enormous flexibility and features, Noise represents a perfect candidate for bringing key-exchange mechanisms in Waku.\\n\\n## The Diffie-Hellman Key-exchange\\n\\nThe formalization of modern public-key cryptography started with the pioneering work of Whitefield Diffie and Martin Hellman, who detailed one of the earliest known key-agreement protocols: the famous [Diffie-Hellman Key-Exchange](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange).\\n\\nDiffie-Hellman (DH) key-exchange is largely used today and represents the main cryptographic building block on which Noise handshakes\' security is based.\\n\\nIn turn, the security of DH is based on a mathematical problem called [discrete logarithm](https://en.wikipedia.org/wiki/Discrete_logarithm) which is believed to be hard when the agreement is practically instantiated using certain [elliptic curves](https://en.wikipedia.org/wiki/Elliptic_curve) $E$ defined over finite fields $\\\\mathbb{F}_p$.\\n\\nInformally, a DH exchange between Alice and Bob proceeds as follows:\\n\\n- Alice picks a secret scalar $s_A\\\\in\\\\mathbb{F}_p$ and computes, using the underlying [curve\'s arithmetic](https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplication), the point $P_A = s_A\\\\cdot P\\\\in E(\\\\mathbb{F}_p)$ for a certain pre-agreed public generator $P$ of the elliptic curve $E(\\\\mathbb{F}_p)$. She then sends $P_A$ to Bob.\\n- Similarly, Bob picks a secret scalar $s_B\\\\in\\\\mathbb{F}_p$, computes $P_B = s_B\\\\cdot P\\\\in E(\\\\mathbb{F}_p)$ and sends $P_B$ to Alice.\\n- By commutativity of scalar multiplication, both Alice and Bob can now compute the point $P_{AB} = s_As_B\\\\cdot P$, using the elliptic curve point received from the other party and their secret scalar.\\n\\nThe assumed hardness of computing discrete logarithms in the elliptic curve, ensures that it is not possible to compute $s_A$ or $s_B$ from $P_A$ and $P_B$, respectively. Another security assumption (named [Computational Diffie-Hellman assumption](https://en.wikipedia.org/wiki/Computational_Diffie%E2%80%93Hellman_assumption)) ensures that it is not possible to compute $P_{AB}$ from $P$, $P_A$ and $P_B$. Hence the point $P_{AB}$ shared by Alice and Bob at the end of the above protocol cannot be efficiently computed by an attacker intercepting $P_A$ and $P_B$, and can then be used to generate a secret to be later employed, for example, as a symmetric encryption key.\\n\\nOn a side note, this protocol shows the interplay between two components typical to public-key based schemes: the scalars $s_A$ and $s_B$ can be seen as _private keys_ associated to the _public keys_ $P_A$ and $P_B$, respectively, which allow Alice and Bob only to compute the shared secret point $P_{AB}$.\\n\\n## Ephemeral and Static Public Keys\\n\\nAlthough we assumed that it is practically impossible for an attacker to compute the randomly picked secret scalar from the corresponding public elliptic curve point, it may happen that such scalar gets compromised or can be guessed due to a faulty employed random number generator. In such cases, an attacker will be able to recover the final shared secret and all encryption keys eventually derived from that, with clear catastrophic consequences for the privacy of exchanged messages.\\n\\nTo mitigate such issues, multiple DH operations can be combined using two different types of exchanged elliptic curve points or, better, _public keys_: _ephemeral keys_, that is random keys used only once in a DH operation, and long-term _static keys_, used mainly for authentication purposes since employed multiple times.\\n\\nJust to provide an example, let us suppose Alice and Bob perform the following custom DH-based key-exchange protocol:\\n\\n- Alice generates an ephemeral key $E_A=e_A\\\\cdot P$ by picking a random scalar $e_A$ and sends $E_A$ to Bob;\\n- Similarly, Bob generates an ephemeral key $E_B=e_B\\\\cdot P$ and sends $E_B$ to Alice;\\n- Alice and Bob computes $E_{AB} = e_Ae_B \\\\cdot P$ and from it derive a secret encryption key $k$.\\n- Bob sends to Alice his static key $S_B = s_B\\\\cdot P$ encrypted with $k$.\\n- Alice encrypts with $k$ her static key $S_A = s_A\\\\cdot P$ and sends it to Bob.\\n- Alice and Bob decrypt the received static keys, compute the secret $S_{AB} = s_As_B \\\\cdot P$ and use it together with $E_{AB}$ to derive a new encryption key $\\\\tilde{k}$ to be later used with a symmetric cipher.\\n\\nIn this protocol, if Alice\'s and/or Bob\'s static keys get compromised, it would not possible to derive the final secret key $\\\\tilde{k}$, since at least one ephemeral key among $E_A$ and $E_B$ has to be compromised too in order to recover the secret $E_{AB}$. Furthermore, since Alice\'s and Bob\'s long-term static keys are encrypted, an attacker intercepting exchanged (encrypted) public keys will not be able to link such communication to Alice or Bob, unless one of the ephemeral key is compromised (and, even in such case, none of the messages encrypted under the key $\\\\tilde{k}$ can be decrypted).\\n\\n## The Noise Protocol Framework\\n\\nIn previous section we gave a small intuition on how multiple DH operations over ephemeral and static users\' public keys can be combined to create different key-exchange protocols.\\n\\nThe [Noise Protocol Framework](http://www.noiseprotocol.org/noise.html), defines various rules for building custom key-exchange protocols while allowing easy analysis of the security properties and threat models provided given the type and order of the DH operations employed.\\n\\nIn Noise terminology, a key-agreement or _Noise protocol_ consists of one or more _Noise handshakes_. During a Noise handshake, Alice and Bob exchange multiple (handshake) messages containing their ephemeral keys and/or static keys. These public keys are then used to perform a handshake-dependent sequence of Diffie-Hellman operations, whose results are all hashed into a shared secret key. Similarly as we have seen above, after a handshake is complete, each party will use the derived secret key to send and receive [authenticated encrypted data](https://en.wikipedia.org/wiki/Authenticated_encryption) by employing a symmetric cipher.\\n\\nDepending on the _handshake pattern_ adopted, different security guarantees can be provided on messages encrypted using a handshake-derived key.\\n\\nThe Noise handshakes we support in Waku all provide the following security properties:\\n\\n- **Confidentiality**: the adversary should not be able to learn what data is being sent between Alice and Bob.\\n- **Strong forward secrecy**: an active adversary cannot decrypt messages nor infer any information on the employed encryption key, even in the case he has access to Alice\'s and Bob\'s long-term private keys (during or after their communication).\\n- **Authenticity**: the adversary should not be able to cause either Alice or Bob to accept messages coming from a party different than their original senders.\\n- **Integrity**: the adversary should not be able to cause Alice or Bob to accept data that has been tampered with.\\n- **Identity-hiding**: once a secure communication channel is established, a passive adversary should not be able to link exchanged encrypted messages to their corresponding sender and recipient by knowing their long-term static keys.\\n\\nWe refer to [Noise specification](http://www.noiseprotocol.org/noise.html) for more formal security definitions and precise threat models relative to Waku [supported Noise Handshake patterns](#Supported-Noise-Handshakes-in-Waku).\\n\\n## Message patterns\\n\\nNoise handshakes involving DH operations over ephemeral and static keys can be succinctly sketched using the following set of _handshake message tokens_: `e`,`s`,`ee`,`se`,`es`,`ss`.\\n\\nTokens employing single letters denote (the type of) users\' public keys: `e` refers to randomly generated ephemeral key(s), while `s` indicates the users\' long-term static key(s).\\n\\nTwo letters tokens, instead, denotes DH operations over the two users\' public keys the token refers to, given that the left token letter refers to the handshake _initiator\'s_ public key, while the right token letter indicates the used _responder\'s_ public key. Thus, if Alice started a handshake with Bob, the `es` token will shortly represent a DH operation among Alice\'s ephemeral key `e` and Bob\'s static key `s`.\\n\\nSince, in order to perform any DH operations users need to share (or pre-share) the corresponding public keys, Noise compactly represents messages\' exchanges using the two direction `->` and `<-`, where the `->` denotes a message (arbitrary and/or DH public key) from the initiator to the responder, while `<-` the opposite.\\n\\nHence a _message pattern_ consisting of a direction and one or multiple tokens such as `<- e, s, es` has to be interpreted one token at a time: in this example, the responder is sending his ephemeral and static key to the initiator and is then executing a DH operation over the initiator\'s ephemeral key `e` (shared in a previously exchanged message pattern) and his static key `s`. On the other hand, such message indicates also that the initiator received the responder\'s ephemeral and static keys `e` and `s`, respectively, and performed a DH operation over his ephemeral key and the responder\'s just received static key `s`. In this way, both parties will be able to derive at the end of each message pattern processed the same shared secret, which is eventually used to update any derived symmetric encryption keys computed so far.\\n\\nIn some cases, DH public keys employed in a handshake are pre-shared before the handshake itself starts. In order to chronologically separate exchanged keys and DH operations performed before and during a handshake, Noise employs the `...` delimiter.\\n\\nFor example, the following message patterns\\n\\n```\\n<- e\\n...\\n-> e, ee\\n```\\n\\nindicates that the initiator knew the responder\'s ephemeral key before he sends his own ephemeral key and executes a DH operation between both parties ephemeral keys (similarly, the responder receives the initiator\'s ephemeral key and does a `ee` DH operation).\\n\\nAt this point it should be clear how such notation is able to compactly represent a large variety of DH based key-agreements. Nevertheless, we can easily define additional tokens and processing rules in order to address specific applications and security requirements, such as the [`psk`](http://www.noiseprotocol.org/noise.html#handshake-tokens) token used to process arbitrary pre-shared key material.\\n\\nAs an example of Noise flexibility, the custom protocol we detailed [above](#Ephemeral-and-Static-Public-Keys) can be shortly represented as _(Alice is on the left)_:\\n\\n```\\n-> e\\n<- e, ee, s\\n-> s, ss\\n```\\n\\nwhere after each DH operation an encryption key is derived (along with the secrets computed by all previously executed DH operations) in order to encrypt/decrypt any subsequent sent/received message.\\n\\nAnother example is given by the possibility to replicate within Noise the well established Signal\'s [X3DH](https://signal.org/docs/specifications/x3dh/) key-agreement protocols, thus making the latter a general framework to design and study security of many practical and widespread DH-based key-exchange protocols.\\n\\n## The Noise State Objects\\n\\nWe mentioned multiple times that parties derive an encryption key each time they perform a DH operation, but how does this work in more details?\\n\\nNoise defines three _state object_: a _Handshake State_, a _Symmetric State_ and a _Cipher State_, each encapsulated into each other and instantiated during the execution of a handshake.\\n\\nThe Handshake State object stores the user\'s and other party\'s received ephemeral and static keys (if any) and embeds a Symmetric State object.\\n\\nThe Symmetric State, instead, stores a handshake hash value `h`, iteratively updated with any message read/received and DH secret computed, and a chaining key `ck`, updated using a key derivation function every time a DH secret is computed. This object further embeds a Cipher State.\\n\\nLastly, the Cipher State stores a symmetric encryption `k` key and a counter `n` used to encrypt and decrypt messages exchanged during the handshake (not only static keys, but also arbitrary payloads). These key and counter are refreshed every time the chaining key is updated.\\n\\nWhile processing each handshake\'s message pattern token, all these objects are updated according to some specific _processing rules_ which employ a combination of public-key primitives, hash and key-derivation functions and symmetric ciphers. It is important to note, however, that at the end of each processed message pattern, the two users will share the same Symmetric and Cipher State embedded in their respective Handshake States.\\n\\nOnce a handshake is complete, users derive two new Cipher States and can then discard the Handshake State object (and, thus, the embedded Symmetric State and Cipher State objects)\\nemployed during the handshake.\\n\\nThese two Cipher states are used to encrypt and decrypt all outbound and inbound after-handshake messages, respectively, and only to these will be granted the confidentiality, authenticity, integrity and identity-hiding properties we detailed above.\\n\\nFor more details on processing rules, we refer to [Noise specifications](http://www.noiseprotocol.org/noise.html).\\n\\n## Supported Noise Handshakes in Waku\\n\\nThe Noise handshakes we provided support to in Waku address four typical scenarios occurring when an encrypted communication channel between Alice and Bob is going to be created:\\n\\n- Alice and Bob know each others\' static key.\\n- Alice knows Bob\'s static key;\\n- Alice and Bob share no key material and they don\'t know each others\' static key.\\n- Alice and Bob share some key material, but they don\'t know each others\' static key.\\n\\nThe possibility to have handshakes based on the reciprocal knowledge parties have of each other, allows designing Noise handshakes that can quickly reach the desired level of security on exchanged encrypted messages while keeping the number of interactions between Alice and Bob minimum.\\n\\nNonetheless, due to the pure _token-based_ nature of handshake processing rules, implementations can easily add support to any custom handshake pattern with minor modifications, in case more specific application use-cases need to be addressed.\\n\\nOn a side note, we already mentioned that identity-hiding properties can be guaranteed against a passive attacker that only reads the communication occurring between Alice and Bob. However, an active attacker who compromised one party\'s static key and actively interferes with the parties\' exchanged messages, may lower the identity-hiding security guarantees provided by some handshake patterns. In our security model we exclude such adversary, but, for completeness, in the following we report a summary of possible de-anonymization attacks that can be performed by such an active attacker.\\n\\nFor more details on supported handshakes and on how these are implemented in Waku, we refer to [WAKU2-NOISE](https://github.com/waku-org/specs/blob/master/standards/application/noise.md) RFC.\\n\\n### The K1K1 Handshake\\n\\nIf Alice and Bob know each others\' static key (e.g., these are public or were already exchanged in a previous handshake) , they MAY execute a `K1K1` handshake. In Noise notation _(Alice is on the left)_ this can be sketched as:\\n\\n```\\n K1K1:\\n    ->  s\\n    <-  s\\n       ...\\n    ->  e\\n    <-  e, ee, es\\n    ->  se\\n```\\n\\nWe note that here only ephemeral keys are exchanged. This handshake is useful in case Alice needs to instantiate a new separate encrypted communication channel with Bob, e.g. opening multiple parallel connections, file transfers, etc.\\n\\n**Security considerations on identity-hiding (active attacker)**: no static key is transmitted, but an active attacker impersonating Alice can check candidates for Bob\'s static key.\\n\\n### The XK1 Handshake\\n\\nHere, Alice knows how to initiate a communication with Bob and she knows his public static key: such discovery can be achieved, for example, through a publicly accessible register of users\' static keys, smart contracts, or through a previous public/private advertisement of Bob\'s static key.\\n\\nA Noise handshake pattern that suits this scenario is `XK1`:\\n\\n```\\n XK1:\\n    <-  s\\n       ...\\n    ->  e\\n    <-  e, ee, es\\n    ->  s, se\\n```\\n\\nWithin this handshake, Alice and Bob reciprocally authenticate their static keys `s` using ephemeral keys `e`. We note that while Bob\'s static key is assumed to be known to Alice (and hence is not transmitted), Alice\'s static key is sent to Bob encrypted with a key derived from both parties ephemeral keys and Bob\'s static key.\\n\\n**Security considerations on identity-hiding (active attacker)**: Alice\'s static key is encrypted with forward secrecy to an authenticated party. An active attacker initiating the handshake can check candidates for Bob\'s static key against recorded/accepted exchanged handshake messages.\\n\\n### The XX and XXpsk0 Handshakes\\n\\nIf Alice is not aware of any static key belonging to Bob (and neither Bob knows anything about Alice), she can execute an `XX` handshake, where each party tran**X**mits to the other its own static key.\\n\\nThe handshake goes as follows:\\n\\n```\\n XX:\\n    ->  e\\n    <-  e, ee, s, es\\n    ->  s, se\\n```\\n\\nWe note that the main difference with `XK1` is that in second step Bob sends to Alice his own static key encrypted with a key obtained from an ephemeral-ephemeral Diffie-Hellman exchange.\\n\\nThis handshake can be slightly changed in case both Alice and Bob pre-shares some secret `psk` which can be used to strengthen their mutual authentication during the handshake execution. One of the resulting protocol, called `XXpsk0`, goes as follow:\\n\\n```\\n XXpsk0:\\n    ->  psk, e\\n    <-  e, ee, s, es\\n    ->  s, se\\n```\\n\\nThe main difference with `XX` is that Alice\'s and Bob\'s static keys, when transmitted, would be encrypted with a key derived from `psk` as well.\\n\\n**Security considerations on identity-hiding (active attacker)**: Alice\'s static key is encrypted with forward secrecy to an authenticated party for both `XX` and `XXpsk0` handshakes. In `XX`, Bob\'s static key is encrypted with forward secrecy but is transmitted to a non-authenticated user which can then be an active attacker. In `XXpsk0`, instead, Bob\'s secret key is protected by forward secrecy to a partially authenticated party (through the pre-shared secret `psk` but not through any static key), provided that `psk` was not previously compromised (in such case identity-hiding properties provided by the `XX` handshake applies).\\n\\n## Session Management and Multi-Device Support\\n\\nWhen two users complete a Noise handshake, an encryption/decryption session - or _Noise session_ - consisting of two Cipher States is instantiated.\\n\\nBy identifying Noise session with a `session-id` derived from the handshake\'s cryptographic material, we can take advantage of the [PubSub/GossipSub](https://github.com/libp2p/specs/tree/master/pubsub) protocols used by Waku for relaying messages in order to manage instantiated Noise sessions.\\n\\nThe core idea is to exchange after-handshake messages (encrypted with a Cipher State specific to the Noise session), over a content topic derived from the (secret) `session-id` the corresponding session refers to.\\n\\nThis allows to decouple the handshaking phase from the actual encrypted communication, thus improving users\' identity-hiding capabilities.\\n\\nFurthermore, by publicly revealing a value derived from `session-id` on the corresponding session content topic, a Noise session can be marked as _stale_, enabling peers to save resources by discarding any eventually [stored](https://rfc.vac.dev/waku/standards/core/13/store) message sent to such content topic.\\n\\nOne relevant aspect in today\'s applications is the possibility for users to employ different devices in their communications. In some cases, this is non-trivial to achieve since, for example, encrypted messages might be required to be synced on different devices which do not necessarily share the necessary key material for decryption and may be temporarily offline.\\n\\nWe address this by requiring each user\'s device to instantiate multiple Noise sessions either with all user\'s other devices which, in turn, all together share a Noise session with the other party, or by directly instantiating a Noise session with all other party\'s devices.\\n\\nWe named these two approaches $N11M$ and $NM$, respectively, which are in turn loosely based on the paper [\u201cMulti-Device for Signal\u201d](https://eprint.iacr.org/2019/1363.pdf) and [Signal\u2019s Sesame Algorithm](https://signal.org/docs/specifications/sesame/).\\n\\n![](//img/noise/N11M.png)\\n\\nInformally, in the $N11M$ session management scheme, once the first Noise session between any of Alice\u2019s and Bob\u2019s device is instantiated, its session information is securely propagated to all other devices using previously instantiated Noise sessions. Hence, all devices are able to send and receive new messages on the content topic associated to such session.\\n\\n![](//img/noise/NM.png)\\n\\nIn the $NM$ session management scheme, instead, all pairs of Alice\'s and Bob\'s devices have a distinct Noise session: a message is then sent from the currently-in-use sender\u2019s device to all recipient\u2019s devices, by properly encrypting and sending it to the content topics of each corresponding Noise session. If sent messages should be available on all sender\u2019s devices as well, we require each pair of sender\u2019s devices to instantiate a Noise session used for syncing purposes.\\n\\nFor more technical details on how Noise sessions are instantiated and managed within these two mechanisms and the different trade-offs provided by the latter, we refer to [WAKU2-NOISE-SESSIONS](https://github.com/waku-org/specs/blob/master/standards/application/noise-sessions.md).\\n\\n## Conclusions\\n\\nIn this post we provided an overview of Noise, a protocol framework for designing Diffie-Hellman based key-exchange mechanisms allowing systematic security and threat model analysis.\\n\\nThe flexibility provided by Noise components allows not only to fully replicate with same security guarantees well established key-exchange primitives such as X3DH, currently employed by Status [TRANSPORT-SECURITY](https://rfc.vac.dev/status/deprecated/secure-transport), but enables also optimizations based on the reciprocal knowledge parties have of each other while allowing easier protocols\' security analysis and (formal) verification.\\n\\nFurthermore, different handshakes can be combined and executed one after each other, a particularly useful feature to authenticate multiple static keys employed by different applications but also to ease keys revocation.\\n\\nThe possibility to manage Noise sessions over multiple devices and the fact that handshakes can be concretely instantiated using modern, fast and secure cryptographic primitives such as [ChaChaPoly](https://datatracker.ietf.org/doc/html/rfc7539) and [BLAKE2b](https://datatracker.ietf.org/doc/html/rfc7693), make Noise one of the best candidates for efficiently and securely address the many different needs of applications built on top of Waku requiring key-agreement.\\n\\n## Future steps\\n\\nThe available [implementation](https://github.com/status-im/nwaku/tree/master/waku/v2/waku_noise) of Noise in `nwaku`, although mostly complete, is still in its testing phase. As future steps we would like to:\\n\\n- have an extensively tested and robust Noise implementation;\\n- formalize, implement and test performances of the two proposed $N11M$ and $NM$ session management mechanisms and their suitability for common use-case scenarios;\\n- provide Waku network nodes a native protocol to readily support key-exchanges, strongly-encrypted communication and multi-device session management mechanisms with none-to-little interaction besides applications\' connection requests.\\n\\n## References\\n\\n- [6/WAKU1](https://rfc.vac.dev/waku/standards/legacy/6/waku1)\\n- [10/WAKU2](https://rfc.vac.dev/waku/standards/core/10/waku2)\\n- [13/WAKU2-STORE](https://rfc.vac.dev/waku/standards/core/13/store)\\n- [26/WAKU-PAYLOAD](https://rfc.vac.dev/waku/standards/application/26/payload)\\n- [WAKU2-NOISE](https://github.com/waku-org/specs/blob/master/standards/application/noise.md)\\n- [37/WAKU2-NOISE-SESSIONS](https://github.com/waku-org/specs/blob/master/standards/application/noise-sessions.md)\\n- [TRANSPORT-SECURITY](https://rfc.vac.dev/status/deprecated/secure-transport)\\n- [The PubSub/GossipSub Protocols](https://github.com/libp2p/specs/tree/master/pubsub)\\n- [The Noise Protocol Framework](http://www.noiseprotocol.org/noise.html)\\n- [The X3DH Key-agreement Protocol](https://signal.org/docs/specifications/x3dh/)\\n- [\u201cMulti-Device for Signal\u201d](https://eprint.iacr.org/2019/1363.pdf)\\n- [Signal\u2019s Sesame Algorithm](https://signal.org/docs/specifications/sesame/).\\n- [Public-key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography)\\n- [Elliptic curves](https://en.wikipedia.org/wiki/Elliptic_curve)\\n- [Elliptic Curve point multiplication](https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplication)\\n- [Symmetric key algorithm](https://en.wikipedia.org/wiki/Symmetric-key_algorithm)\\n- [Authenticated encryption](https://en.wikipedia.org/wiki/Authenticated_encryption)\\n- [Diffie-Hellman Key-Exchange](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange)\\n- [The Discrete Logarithm Problem](https://en.wikipedia.org/wiki/Discrete_logarithm)\\n- [Computational Diffie-Hellman Assumption](https://en.wikipedia.org/wiki/Computational_Diffie%E2%80%93Hellman_assumption)\\n- [The ECIES Encryption Algorithm](https://en.wikipedia.org/wiki/Integrated_Encryption_Scheme)\\n- [The ECDSA Signature Algorithm](https://en.wikipedia.org/wiki/Elliptic_Curve_Digital_Signature_Algorithm)\\n- [The Galois Counter Mode mode of operation](https://en.wikipedia.org/wiki/Galois/Counter_Mode)\\n- [The ChaChaPoly AEAD Cipher](https://datatracker.ietf.org/doc/html/rfc7539)\\n- [The BLAKE2b Hash Function](https://datatracker.ietf.org/doc/html/rfc7693)\\n- [The SHA-3 Hash Function](https://en.wikipedia.org/wiki/SHA-3)"},{"id":"wakuv2-apd","metadata":{"permalink":"/rlog/wakuv2-apd","source":"@site/rlog/2022-05-09-ambient-peer-discovery.mdx","title":"Waku v2 Ambient Peer Discovery","description":"Introducing and discussing ambient peer discovery methods currently used by Waku v2, as well as future plans in this area.","date":"2022-05-09T10:00:00.000Z","formattedDate":"May 9, 2022","tags":[],"readingTime":17.67,"hasTruncateMarker":true,"authors":[{"name":"Daniel","github":"kaiserd","key":"kaiserd"}],"frontMatter":{"layout":"post","name":"Waku v2 Ambient Peer Discovery","title":"Waku v2 Ambient Peer Discovery","date":"2022-05-09T10:00:00.000Z","authors":"kaiserd","published":true,"slug":"wakuv2-apd","categories":"research","image":"/img/waku_v2_discv5_random_walk_estimation.svg","discuss":"https://forum.vac.dev/t/discussion-waku-v2-ambient-peer-discovery/133","_includes":["math"]},"prevItem":{"title":"Noise handshakes as key-exchange mechanism for Waku","permalink":"/rlog/wakuv2-noise"},"nextItem":{"title":"Introducing nwaku","permalink":"/rlog/introducing-nwaku"}},"content":"Introducing and discussing ambient peer discovery methods currently used by Waku v2, as well as future plans in this area.\\n\\n\x3c!--truncate--\x3e\\n\\n[Waku v2](https://rfc.vac.dev/waku/standards/core/10/waku2) comprises a set of modular protocols for secure, privacy preserving communication.\\nAvoiding centralization, these protocols exchange messages over a P2P network layer.\\nIn order to build a P2P network, participating nodes first have to discover peers within this network.\\nThis is where [_ambient peer discovery_](https://docs.libp2p.io/concepts/publish-subscribe/#discovery) comes into play:\\nit allows nodes to find peers, making it an integral part of any decentralized application.\\n\\nIn this post the term _node_ to refers to _our_ endpoint or the endpoint that takes action,\\nwhile the term _peer_ refers to other endpoints in the P2P network.\\nThese endpoints can be any device connected to the Internet: e.g. servers, PCs, notebooks, mobile devices, or applications like a browser.\\nAs such, nodes and peers are the same. We use these terms for the ease of explanation without loss of generality.\\n\\nIn Waku\'s modular design, ambient peer discovery is an umbrella term for mechanisms that allow nodes to find peers.\\nVarious ambient peer discovery mechanisms are supported, and each is specified as a separate protocol.\\nWhere do these protocols fit into Waku\'s protocol stack?\\nThe P2P layer of Waku v2 builds on [libp2p gossipsub](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/README.md).\\nNodes participating in a gossipsub protocol manage a mesh network that is used for routing messages.\\nThis mesh network is an [unstructured P2P network](https://en.wikipedia.org/wiki/Peer-to-peer#Unstructured_networks)\\noffering high robustness and resilience against attacks.\\nGossipsub implements many improvements overcoming the shortcomings typically associated with unstructured P2P networks, e.g. inefficient flooding based routing.\\nThe gossipsub mesh network is managed in a decentralized way, which requires each node to know other participating peers.\\nWaku v2 may use any combination of its ambient discovery protocols to find appropriate peers.\\n\\nSummarizing, Waku v2 comprises a _peer management layer_ based on libp2p gossipsub,\\nwhich manages the peers of nodes, and an _ambient peer discovery layer_,\\nwhich provides information about peers to the peer management layer.\\n\\nWe focus on ambient peer discovery methods that are in line with our goal of building a fully decentralized, generalized, privacy-preserving and censorship-resistant messaging protocol.\\nSome of these protocols still need adjustments to adhere to our privacy and anonymity requirements. For now, we focus on operational stability and feasibility.\\nHowever, when choosing techniques, we pay attention to selecting mechanisms that can feasibly be tweaked for privacy in future research efforts.\\nBecause of the modular design and the fact that Waku v2 has several discovery methods at its disposal, we could even remove a protocol in case future evaluation deems it not fitting our standards.\\n\\nThis post covers the current state and future considerations of ambient peer discovery for Waku v2,\\nand gives reason for changes and modifications we made or plan to make.\\nThe ambient peer discovery protocols currently supported by Waku v2 are a modified version of Ethereum\'s [Discovery v5](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5.md)\\nand [DNS-based discovery](https://vac.dev/dns-based-discovery).\\nWaku v2 further supports [gossipsub\'s peer exchange protocol](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/gossipsub-v1.1.md#prune-backoff-and-peer-exchange).\\nIn addition, we plan to introduce protocols for general peer exchange and capability discovery, respectively.\\nThe former allows resource restricted nodes to outsource querying for peers to stronger peers,\\nthe latter allows querying peers for their supported capabilities.\\nBesides these new protocols, we are working on integrating capability discovery in our existing ambient peer discovery protocols.\\n\\n## Static Node Lists\\n\\nThe simplest method of learning about peers in a P2P network is via static node lists.\\nThese can be given to nodes as start-up parameters or listed in a config-file.\\nThey can also be provided in a script-parseable format, e.g. in JSON.\\nWhile this method of providing bootstrap nodes is very easy to implement, it requires static peers, which introduce centralized elements.\\nAlso, updating static peer information introduces significant administrative overhead:\\ncode and/or config files have to be updated and released.\\nTypically, static node lists only hold a small number of bootstrap nodes, which may lead to high load on these nodes.\\n\\n## DNS-based Discovery\\n\\nCompared to static node lists,\\n[DNS-based discovery](https://vac.dev/dns-based-discovery) (specified in [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459))\\nprovides a more dynamic way of discovering bootstrap nodes.\\nIt is very efficient, can easily be handled by resource restricted devices and provides very good availability.\\nIn addition to a naive DNS approach, Ethereum\'s DNS-based discovery introduces efficient authentication leveraging [Merkle trees](https://en.wikipedia.org/wiki/Merkle_tree).\\n\\nA further advantage over static node lists is the separation of code/release management and bootstrap node management.\\nHowever, changing and updating the list of bootstrap nodes still requires administrative privileges because DNS records have to be added or updated.\\n\\nWhile this method of discovery still requires centralized elements,\\nnode list management can be delegated to various DNS zones managed by other entities mitigating centralization.\\n\\n## Discovery V5\\n\\nA much more dynamic method of ambient peer discovery is [Discovery v5](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5.md), which is Ethereum\'s peer discovery protocol.\\nIt is based on the [Kademlia](https://en.wikipedia.org/wiki/Kademlia) distributed hashtable (DHT).\\nAn [introduction to discv5 and its history](https://vac.dev/kademlia-to-discv5), and a [discv5 Waku v2 feasibility study](https://vac.dev/feasibility-discv5)\\ncan be found in previous posts on this research log.\\n\\nWe use Discovery v5 as an ambient peer discovery method for Waku v2 because it is decentralized, efficient, actively researched, and has web3 as its main application area.\\nDiscv5 also offers mitigation techniques for various attacks, which we cover later in this post.\\n\\nUsing a DHT (structured P2P network) as a means for ambient peer discovery, while using the gossipsub mesh network (unstructured P2P network) for transmitting actual messages,\\nWaku v2 leverages advantages from both worlds.\\nOne of the main benefits of DHTs is offering a global view over participating nodes.\\nThis, in turn, allows sampling random sets of nodes which is important for equally distributing load.\\nGossipsub, on the other hand, offers great robustness and resilience against attacks.\\nEven if discv5 discovery should not work in advent of a DoS attack, Waku v2 can still operate switching to different discovery methods.\\n\\nDiscovery methods that use separate P2P networks still depend on bootstrapping,\\nwhich Waku v2 does via parameters on start-up or via DNS-based discovery.\\nThis might raise the question of why such discovery methods are beneficial.\\nThe answer lies in the aforementioned global view of DHTs. Without discv5 and similar methods, the bootstrap nodes are used as part of the gossipsub mesh.\\nThis might put heavy load on these nodes and further, might open pathways to inference attacks.\\nDiscv5, on the other hand, uses the bootstrap nodes merely as an entry to the discovery network and can provide random sets of nodes (sampled from a global view)\\nfor bootstrapping or expanding the mesh.\\n\\n### DHT Background\\n\\nDistributed Hash Tables are a class of structured P2P overlay networks.\\nA DHT can be seen as a distributed node set of which each node is responsible for a part of the hash space.\\nIn contrast to unstructured P2P networks, e.g. the mesh network maintained by gossipsub,\\nDHTs have a global view over the node set and the hash space (assuming the participating nodes behave well).\\n\\nDHTs are susceptible to various kinds of attacks, especially [Sybil attacks](https://en.wikipedia.org/wiki/Sybil_attack)\\nand [eclipse attacks](https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/heilman).\\nWhile security aspects have been addressed in various research papers, general practical solutions are not available.\\nHowever, discv5 introduced various practical mitigation techniques.\\n\\n### Random Walk Discovery\\n\\nWhile discv5 is based on the Kademlia DHT, it only uses the _distributed node set_ aspect of DHTs.\\nIt does not map values (items) into the distributed hash space.\\nThis makes sense, because the main purpose of discv5 is discovering other nodes that support discv5, which are expected to be Ethereum nodes.\\nEthereum nodes that want to discover other Ethereum nodes simply query the discv5 network for a random set of peers.\\nIf Waku v2 would do the same, only a small subset of the retrieved nodes would support Waku v2.\\n\\nA first naive solution for Waku v2 discv5 discovery is\\n\\n- retrieve a random node set, which is achieved by querying for a set of randomly chosen node IDs\\n- filter the returned nodes on the query path based on Waku v2 capability via the [Waku v2 ENR](https://github.com/waku-org/specs/blob/master/standards/core/enr.md)\\n- repeat until enough Waku v2 capable nodes are found\\n\\nThis query process boils down to random walk discovery, which is very resilient against attacks, but also very inefficient if the number of nodes supporting the desired capability is small.\\nWe refer to this as the needle-in-the-haystack problem.\\n\\n### Random Walk Performance Estimation\\n\\nThis subsection provides a rough estimation of the overhead introduced by random walk discovery.\\n\\nGiven the following parameters:\\n\\n- $n$ number of total nodes participating in discv5\\n- $p$ percentage of nodes supporting Waku\\n- $W$ the event of having at least one Waku node in a random sample\\n- $k$ the size of a random sample (default = 16)\\n- $\\\\alpha$ the number of parallel queries started\\n- $b$ bits per hop\\n- $q$ the number of queries\\n\\nA query takes $log_{2^b}n$ hops to retrieve a random sample of nodes.\\n\\n$P(W) = 1 - (1-p/100)^k$ is the probability of having at least one Waku node in the sample.\\n\\n$P(W^q) = 1 - (1-p/100)^{kq}$ is the probability of having at least one Waku node in the union of $q$ samples.\\n\\nExpressing this in terms of $q$, we can write:\\n$$P(W^q) = 1 - (1-p/100)^{kq} \\\\iff  q = log_{(1-p/100)^k}(1-P(W^q))$$\\n\\nFigure 1 shows a log-log plot for $P(W^q) = 90\\\\%$.\\n\\n![Figure 1: log-log plot showing the number of queries necessary to retrieve a Waku v2 node with a probability of 90% in relation to the Waku v2 node concentration in the network.](/img/waku_v2_discv5_random_walk_estimation.svg)\\n\\nAssuming $p=0.1$, we would need\\n\\n$$0.9 = 1 - (1-0.1/100)^{16q} => q \\\\approx 144$$\\n\\nqueries to get a Waku node with 90% probability, which leads to $\\\\approx 144 * 18 = 2592$ overlay hops.\\nChoosing $b=3$ would reduce the number to $\\\\approx 144 * 6 = 864$.\\nEven when choosing $\\\\alpha = 10$ we would have to wait at least 80 RTTs.\\nThis effort is just for retrieving a single Waku node. Ideally, we want at least 3 Waku nodes for bootstrapping a Waku relay.\\n\\n[The discv5 doc](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5-theory.md#ad-placement-and-topic-radius) roughly estimates $p=1%$ to be the threshold for acceptably efficient random walk discovery.\\nThis is in line with our estimation:\\n\\n$$0.9 = 1 - (1-1/100)^{16q} => q \\\\approx 14$$\\n\\nThe number of necessary queries is linearly dependent on the percentage $p$ of Waku nodes.\\nThe number of hops per query is logarithmically dependent on $n$.\\nThus, random walk searching is inefficient for small percentages $p$.\\nStill, random walks are more resilient against attacks.\\n\\nWe can conclude that a Waku node concentration below 1% renders vanilla discv5 unfit for our needs.\\nOur current solution and future plans for solving this issue are covered in the next subsections.\\n\\n### Simple Solution: Separate Discovery Network\\n\\nThe simple solution we currently use for [Waku v2 discv5](https://rfc.vac.dev/waku/standards/core/33/discv5) is a separate discv5 network.\\nAll (well behaving) nodes in this network support Waku v2, resulting in a very high query efficiency.\\nHowever, this solution reduces resilience because the difficulty of attacking a DHT scales with the number of participating nodes.\\n\\n### Discv5 Topic Discovery\\n\\nWe did not base our solution on the [current version of discv5 topic discovery](https://github.com/ethereum/devp2p/blob/master/discv5/discv5-theory.md#topic-advertisement),\\nbecause, similar to random walk discovery, it suffers from poor performance for relatively rare capabilities/topics.\\n\\nHowever, there is [ongoing research](https://github.com/harnen/service-discovery-paper) in discv5 topic discovery which is close to ideas we explored when pondering efficient and resilient Waku discv5 solutions.\\nWe keep a close eye on this research, give feedback, and make suggestions, as we plan to switch to this version of topic discovery in the future.\\n\\nIn a nutshell, topic discovery will manage separate routing tables for each topic.\\nThese topic specific tables are initialized with nodes from the discv5 routing table.\\nWhile the buckets of the discv5 routing table represent distance intervals from the node\'s `node ID`, the topic table buckets represent distance intervals from `topic ID`s.\\n\\nNodes that want to register a topic try to register that topic at one random peer per bucket.\\nThis leads to registering the topic at peers in closer and closer neighbourhoods around the topic ID, which\\nyields a very efficient and resilient compromise between random walk discovery and DHT discovery.\\nPeers in larger neighbourhoods around the topic ID are less efficient to discover, however more resilient against eclipse attacks and vice versa.\\n\\nFurther, this works well with the overload and DoS protection discv5 employs.\\nDiscv5 limits the amount of nodes registered per topic on a single peer. Further, discv5 enforces a waiting time before nodes can register topics at peers.\\nSo, for popular topics, a node might fail to register the topic in a close neighbourhood.\\nHowever, because the topic is popular (has a high occurrence percentage $p$), it can still be efficiently discovered.\\n\\nIn the future, we also plan to integrate Waku v2 capability discovery, which will not only allow asking for nodes that support Waku v2,\\nbut asking for Waku v2 nodes supporting specific Waku v2 protocols like filter or store.\\nFor the store protocol we envision sub-capabilities reflecting message topics and time frames of messages.\\nWe will also investigate related security implications.\\n\\n### Attacks on DHTs\\n\\nIn this post, we only briefly describe common attacks on DHTs.\\nThese attacks are mainly used for denial of service (DoS),\\nbut can also used as parts of more sophisticated attacks, e.g. deanonymization attacks.\\nA future post on this research log will cover security aspects of ambient peer discovery with a focus on privacy and anonymity.\\n\\n_Sybil Attack_\\n\\nThe power of an attacker in a DHT is proportional to the number of controlled nodes.\\nControlling nodes comes at a high resource cost and/or requires controlling a botnet via a preliminary attack.\\n\\nIn a Sybil attack, an attacker generates lots of virtual node identities.\\nThis allows the attacker to control a large portion of the ID space in a DHT at a relatively low cost.\\nSybil attacks are especially powerful when the attacker can freely choose the IDs of generated nodes,\\nbecause this allows positioning at chosen points in the DHT.\\n\\nBecause Sybil attacks amplify the power of many attacks against DHTs,\\nmaking Sybil attacks as difficult as possible is the basis for resilient DHT operation.\\nThe typical abstract mitigation approach is binding node identities to physical network interfaces.\\nTo some extend, this can be achieved by introducing IP address based limits.\\nFurther, generating node IDs can be bound by proof of work (PoW),\\nwhich, however, comes with a set of shortcomings, e.g. relatively high costs on resource restricted devices.\\n[The discv5 doc](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5-rationale.md#sybil-and-eclipse-attacks)\\ndescribes both Sybil and eclipse attacks, as well as concrete mitigation techniques employed by discv5.\\n\\n_Eclipse Attack_\\n\\nIn an eclipse attack, nodes controlled by the attacker poison the routing tables of other nodes in a way that parts of the DHT become eclipsed, i.e. invisible.\\nWhen a controlled node is asked for the next step in a path,\\nit provides another controlled node as the next step,\\neffectively navigating the querying node around or away from certain areas of the DHT.\\nWhile several mitigation techniques have been researched, there is no definitive protection against eclipse attacks available as of yet.\\nOne mitigation technique is increasing $\\\\alpha$, the number of parallel queries, and following each concurrent path independently for the lookup.\\n\\nThe eclipse attack becomes very powerful in combination with a successful Sybil attack;\\nespecially when the attacker can freely choose the position of the Sybil nodes.\\n\\nThe aforementioned new topic discovery of discv5 provides a good balance between protection against eclipse attacks and query performance.\\n\\n## Peer Exchange Protocol\\n\\nWhile discv5 based ambient peer discovery has many desirable properties, resource restricted nodes and nodes behind restrictive NAT setups cannot run discv5 satisfactory.\\nWith these nodes in mind, we started working on a simple _peer exchange protocol_ based on ideas proposed [here](https://github.com/libp2p/specs/issues/222).\\nThe peer exchange protocol will allow nodes to ask peers for additional peers.\\nSimilar to discv5, the peer exchange protocol will also support capability discovery.\\n\\nThe new peer exchange protocol can be seen as a simple replacement for the [Rendezvous protocol](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/rendezvous/README.md), which Waku v2 does not support.\\nWhile the rendezvous protocol involves nodes registering at rendezvous peers, the peer exchange protocol simply allows nodes to ask any peer for a list of peers (with a certain set of capabilities).\\nRendezvous tends to introduce centralized elements as rendezvous peers have a super-peer role.\\n\\nIn the future, we will investigate resource usage of [Waku v2 discv5](https://rfc.vac.dev/spec/33/) and provide suggestions for minimal resources nodes should have to run discv5 satisfactory.\\n\\n## Further Protocols Related to Discovery\\n\\nWaku v2 comprises further protocols related to ambient peer discovery. We shortly mention them for context, even though they are not strictly ambient peer discovery protocols.\\n\\n### Gossipsub Peer Exchange Protocol\\n\\nGossipsub provides an integrated [peer exchange](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/gossipsub-v1.1.md#prune-backoff-and-peer-exchange) mechanism which is also supported by Waku v2.\\nGossipsub peer exchange works in a _push_ manner. Nodes send peer lists to peers they prune from the active mesh.\\nThis pruning is part of the gossipsub peer management, blurring the boundaries of _peer management_ and _ambient peer discovery_.\\n\\nWe will investigate anonymity implications of this protocol and might disable it in favour of more anonymity-preserving protocols.\\nSending a list of peers discloses information about the sending node.\\nWe consider restricting these peer lists to cached peers that are currently not used in the active gossipsub mesh.\\n\\n### Capability Negotiation\\n\\nSome of the ambient peer discovery methods used by Waku2 will support capability discovery.\\nThis allows to narrow down the set of retrieved peers to peers that support specific capabilities.\\nThis is efficient because it avoids establishing connections to nodes that we are not interested in.\\n\\nHowever, the ambient discovery interface does not require capability discovery, which will lead to nodes having peers with unknown capabilities in their peer lists.\\nWe work on a _capability negotiation protocol_ which allows nodes to ask peers\\n\\n- for their complete list of capabilities, and\\n- whether they support a specific capability\\n\\nWe will investigate security implications, especially when sending full capability lists.\\n\\n## NAT traversal\\n\\nFor [NAT traversal](https://docs.libp2p.io/concepts/nat/), Waku v2 currently supports the port mapping protocols [UPnP](https://en.wikipedia.org/wiki/Universal_Plug_and_Play) and [NAT-PMP](https://datatracker.ietf.org/doc/html/rfc6886) / [PCP](https://datatracker.ietf.org/doc/html/rfc6887).\\n\\nIn the future, we plan to add support for parts of [ICE](https://datatracker.ietf.org/doc/html/rfc8445), e.g. [STUN](https://datatracker.ietf.org/doc/html/rfc7350).\\nWe do not plan to support [TURN](https://www.rfc-editor.org/rfc/rfc5928) because TURN relays would introduce a centralized element.\\nA modified decentralized version of TURN featuring incentivization might be an option in the future;\\nstrong peers could offer a relay service similar to TURN.\\n\\nThere are [plans to integrate more NAT traversal into discv5](https://github.com/ethereum/devp2p/issues/199), in which we might participate.\\nSo far, the only traversal technique supported by discv5 is nodes receiving their external IP address in pong messages.\\n\\nWhile NAT traversal is very important, adding more NAT traversal techniques is not a priority at the moment.\\nNodes behind restrictive symmetric NAT setups cannot be discovered, but they can still discover peers in less restrictive setups.\\nWhile we wish to have as many nodes as possible to be discoverable via ambient peer discovery, two nodes behind a restrictive symmetric NAT can still exchange Waku v2 messages if they discovered a shared peer.\\nThis is one of the nice resilience related properties of flooding based routing algorithms.\\n\\nFor mobile nodes, which suffer from changing IP addresses and double NAT setups, we plan using the peer exchange protocol to ask peers for more peers.\\nBesides saving resources on resource restricted devices, this approach works as long as peers are in less restrictive environments.\\n\\n## Conclusion and Future Prospects\\n\\n_Ambient peer discovery_ is an integral part of decentralized applications. It allows nodes to learn about peers in the network.\\nAs of yet, Waku v2 supports DNS-based discovery and a slightly modified version of discv5.\\nWe are working on further protocols, including a peer exchange protocol that allows resource restricted nodes to ask stronger peers for peer lists.\\nFurther, we are working on adding capability discovery to our ambient discovery protocols, allowing nodes to find peers with desired properties.\\n\\nThese protocols can be combined in a modular way and allow Waku v2 nodes to build a strong and resilient mesh network,\\neven if some discovery methods are not available in a given situation.\\n\\nWe will investigate security properties of these discovery mechanisms with a focus on privacy and anonymity in a future post on this research log.\\nAs an outlook we can already state that DHT approaches typically allow inferring information about the querying node.\\nFurther, sending peer lists allows inferring the position of a node within the mesh, and by extension information about the node.\\nWaku v2 already provides some mitigation, because the mesh for transmitting actual messages, and the peer discovery network are separate.\\nTo mitigate information leakage by transmitting peer lists, we plan to only reply with lists of peers that nodes do not use in their active meshes.\\n\\n---\\n\\n## References\\n\\n- [Waku v2](https://rfc.vac.dev/waku/standards/core/10/waku2)\\n- [libp2p gossipsub](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/README.md)\\n- [unstructured P2P network](https://en.wikipedia.org/wiki/Peer-to-peer#Unstructured_networks)\\n- [ambient peer discovery](https://docs.libp2p.io/concepts/publish-subscribe/#discovery)\\n- [Discovery v5](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5.md)\\n- [Kademlia](https://en.wikipedia.org/wiki/Kademlia)\\n- [Discv5 history](https://vac.dev/kademlia-to-discv5)\\n- [Discv5 Waku v2 feasibility study](https://vac.dev/feasibility-discv5)\\n- [DNS-based discovery](https://vac.dev/dns-based-discovery)\\n- [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459)\\n- [Merkle trees](https://en.wikipedia.org/wiki/Merkle_tree)\\n- [Sybil attack](https://en.wikipedia.org/wiki/Sybil_attack)\\n- [eclipse attack](https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/heilman)\\n- [Waku v2 ENR](https://github.com/waku-org/specs/blob/master/standards/core/enr.md)\\n- [Discv5 topic discovery](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5-theory.md#ad-placement-and-topic-radius)\\n- [Discv5 paper](https://github.com/harnen/service-discovery-paper)\\n- [Discv5 vs Sybil and eclipse attacks](https://github.com/ethereum/devp2p/blob/6b0abc3d956a626c28dce1307ee9f546db17b6bd/discv5/discv5-rationale.md#sybil-and-eclipse-attacks)\\n- [peer exchange idea](https://github.com/libp2p/specs/issues/222)\\n- [Rendezvous protocol](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/rendezvous/README.md)\\n- [Waku v2 discv5](https://rfc.vac.dev/waku/standards/core/33/discv5)\\n- [Gossipsub peer exchange](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/gossipsub-v1.1.md#prune-backoff-and-peer-exchange)\\n- [NAT traversal](https://docs.libp2p.io/concepts/nat/)\\n- [UPnP](https://en.wikipedia.org/wiki/Universal_Plug_and_Play)\\n- [NAT-PMP](https://datatracker.ietf.org/doc/html/rfc6886)\\n- [PCP](https://datatracker.ietf.org/doc/html/rfc6887).\\n- [Discv5 topic efficiency issue](https://github.com/ethereum/devp2p/issues/199)"},{"id":"introducing-nwaku","metadata":{"permalink":"/rlog/introducing-nwaku","source":"@site/rlog/2022-04-12-introducing-nwaku.mdx","title":"Introducing nwaku","description":"Introducing nwaku, a Nim-based Waku v2 client, including a summary of recent developments and preview of current and future focus areas.","date":"2022-04-12T10:00:00.000Z","formattedDate":"April 12, 2022","tags":[],"readingTime":10.765,"hasTruncateMarker":true,"authors":[{"name":"Hanno Cornelius","twitter":"4aelius","github":"jm-clius","key":"hanno"}],"frontMatter":{"layout":"post","name":"Introducing nwaku","title":"Introducing nwaku","date":"2022-04-12T10:00:00.000Z","authors":"hanno","published":true,"slug":"introducing-nwaku","categories":"research","discuss":"https://forum.vac.dev/","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Waku v2 Ambient Peer Discovery","permalink":"/rlog/wakuv2-apd"},"nextItem":{"title":"Opinion: Pseudo-ethics in the Surveillance Tech Industry","permalink":"/rlog/ethics-surveillance-tech"}},"content":"Introducing nwaku, a Nim-based Waku v2 client, including a summary of recent developments and preview of current and future focus areas.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nIf you\'ve been following our [research log](https://vac.dev/research-log/),\\nyou\'ll know that many things have happened in the world of Waku v2 since [our last general update](/waku-v2-ethereum-coscup).\\nIn line with our [long term goals](https://vac.dev/#about),\\nwe\'ve introduced new protocols,\\ntweaked our existing protocols\\nand expanded our team.\\nWe\'ve also shown [in a series of practical experiments](/waku-v1-v2-bandwidth-comparison) that Waku v2 does indeed deliver on some of the [theoretical advantages](/waku-v2-plan) it was designed to have over its predecessor, Waku v1.\\nA [sustainability and business workshop](https://forum.vac.dev/t/vac-sustainability-and-business-workshop/116) led to the formulation of a clearer vision for Vac as a team.\\n\\nFrom the beginning, our protocol development has been complemented by various client implementations of these protocols,\\nfirst in [Nim](https://github.com/status-im/nim-waku),\\nbut later also in [JavaScript](https://github.com/status-im/js-waku)\\nand [Go](https://github.com/status-im/go-waku).\\nA follow-up post will clarify the purposes, similarities and differences between these three clients.\\nThe [Nim client](https://github.com/status-im/nim-waku/tree/d2fccb5220144893f994a67f2cc26661247f101f/waku/v2), is our reference implementation,\\ndeveloped by the research team in parallel with the specs\\nand building on a home-grown implementation of [`libp2p`](https://github.com/status-im/nim-libp2p).\\nThe Nim client is suitable to run as [a standalone adaptive node](/waku-update),\\nmanaged by individual operators\\nor as an encapsulated service node in other applications.\\nThis post looks at some recent developments within the Nim client.\\n\\n## 1. _**nim-waku**_ is now known as _**nwaku**_\\n\\nPronounced NWHA-koo.\\nYou may already have seen us refer to \\"`nwaku`\\" on Vac communication channels,\\nbut it is now official:\\nThe `nim-waku` Waku v2 client has been named `nwaku`.\\nWhy? Well, we needed a recognizable name for our client that could easily be referred to in everyday conversations\\nand `nim-waku` just didn\'t roll off the tongue.\\nWe\'ve followed the example of the closely related [`nimbus` project](https://github.com/status-im/nimbus-eth2) to find a punchier name\\nthat explicitly links the client to both the Waku set of protocols and the Nim language.\\n\\n## 2. Improvements in stability and performance\\n\\nThe initial implementation of Waku v2 demonstrated how the suite of protocols can be applied\\nto form a generalized, peer-to-peer messaging network,\\nwhile addressing a wide range of adaptive requirements.\\nThis allowed us to lift several protocol [specifications](https://rfc.vac.dev/) from `raw` to `draft` status,\\nindicating that a reference implementation exists for each.\\nHowever, as internal dogfooding increased and more external applications started using `nwaku`,\\nwe stepped up our focus on the client\'s stability and performance.\\nThis is especially true where we want `nwaku` to run unsupervised in a production environment\\nwithout any degradation in the services it provides.\\n\\nSome of the more significant productionization efforts over the last couple of months included:\\n\\n1. Reworking the `store` implementation to maintain stable memory usage\\n   while storing historical messages\\n   and serving multiple clients querying history simultaneously.\\n   Previously, a `store` node would see gradual service degradation\\n   due to inefficient memory usage when responding to history queries.\\n   Queries that often took longer than 8 mins now complete in under 100 ms.\\n\\n2. Improved peer management.\\n   For example, `filter` nodes will now remove unreachable clients after a number of connection failures,\\n   whereas they would previously keep accumulating dead peers.\\n\\n3. Improved disk usage.\\n   `nwaku` nodes that persist historical messages on disk now manage their own storage size based on the `--store-capacity`.\\n   This can significantly improve node start-up times.\\n\\nMore stability issues may be addressed in future as `nwaku` matures,\\nbut we\'ve noticed a marked improvement in the reliability of running `nwaku` nodes.\\nThese include environments where `nwaku` nodes are expected to run with a long uptime.\\nVac currently operates two long-running fleets of `nwaku` nodes, `wakuv2.prod` and `wakuv2.test`,\\nfor internal dogfooding and\\nto serve as experimental bootstrapping nodes.\\nStatus has also recently deployed similar fleets for production and testing based on `nwaku`.\\nOur goal is to have `nwaku` be stable, performant and flexible enough\\nto be an attractive option for operators to run and maintain their own Waku v2 nodes.\\nSee also the [future work](#future-work) section below for more on our general goal of _`nwaku` for operators_.\\n\\n## 3. Improvements in interoperability\\n\\nWe\'ve implemented several features that improve `nwaku`\'s usability in different environments\\nand its interoperability with other Waku v2 clients.\\nOne major step forward here was adding support for both secure and unsecured WebSocket connections as `libp2p` transports.\\nThis allows direct connectivity with `js-waku`\\nand paves the way for native browser usage.\\nWe\'ve also added support for parsing and resolving DNS-type `multiaddrs`,\\ni.e. multiaddress protocol schemes [`dns`, `dns4`, `dns6` and `dnsaddr`](https://github.com/multiformats/multiaddr/blob/b746a7d014e825221cc3aea6e57a92d78419990f/protocols.csv#L8-L11).\\nA `nwaku` node can now also be [configured with its own IPv4 DNS domain name](https://github.com/status-im/nim-waku/tree/d2fccb5220144893f994a67f2cc26661247f101f/waku/v2#configuring-a-domain-name)\\nallowing dynamic IP address allocation without impacting a node\'s reachability by its peers.\\n\\n## 4. Peer discovery\\n\\n_Peer discovery_ is the method by which nodes become aware of each other\u2019s existence.\\nThe question of peer discovery in a Waku v2 network has been a focus area since the protocol was first conceptualized.\\nSince then several different approaches to discovery have been proposed and investigated.\\nWe\'ve implemented three discovery mechanisms in `nwaku` so far:\\n\\n### DNS-based discovery\\n\\n`nwaku` nodes can retrieve an authenticated, updateable list of peers via DNS to bootstrap connection to a Waku v2 network.\\nOur implementation is based on [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459).\\n\\n### GossipSub peer exchange\\n\\n[GossipSub Peer Exchange (PX)](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/gossipsub-v1.1.md#prune-backoff-and-peer-exchange) is a GossipSub v1.1 mechanism\\nwhereby a pruning peer may provide a pruned peer with a set of alternative peers\\nwhere it can connect to reform its mesh.\\nThis is a very suitable mechanism to gradually discover more peers\\nfrom an initial connection to a small set of bootstrap peers.\\nIt is enabled in a `nwaku` node by default.\\n\\n### Waku Node Discovery Protocol v5\\n\\nThis is a DHT-based discovery mechanism adapted to store and relay _node records_.\\nOur implementation is based on [Ethereum\'s Discovery v5 protocol](https://github.com/ethereum/devp2p/blob/fa6428ada7385c13551873b2ae6ad2457c228eb8/discv5/discv5-theory.md)\\nwith some [minor modifications](https://rfc.vac.dev/waku/standards/core/33/discv5) to isolate our discovery network from that of Ethereum.\\nThe decision to separate the Waku Discovery v5 network from Ethereum\'s was made on considerations of lookup efficiency.\\nThis comes at a possible tradeoff in network resilience.\\nWe are considering merging with the Ethereum Discovery v5 network in future,\\nor even implement a hybrid solution.\\n[This post](https://forum.vac.dev/t/waku-v2-discv5-roadmap-discussion/121/8) explains the decision and future steps.\\n\\n## 5. Spam protection using RLN\\n\\nAn early addition to our suite of protocols was [an extension of `11/WAKU-RELAY`](https://rfc.vac.dev/vac/32/rln-v1)\\nthat provided spam protection using [Rate Limiting Nullifiers (RLN)](https://rfc.vac.dev/vac/32/rln-v1).\\nThe `nwaku` client now contains a working demonstration and integration of RLN relay.\\nCheck out [this tutorial](https://github.com/status-im/nim-waku/blob/ee96705c7fbe4063b780ac43b7edee2f6c4e351b/docs/tutorial/rln-chat2-live-testnet.md) to see the protocol in action using a toy chat application built on `nwaku`.\\nWe\'d love for people to join us in dogfooding RLN spam protection as part of our operator incentive testnet.\\nFeel free to join our [Vac Discord](https://discord.gg/KNj3ctuZvZ) server\\nand head to the `#rln` channel for more information.\\n\\n## Future work\\n\\nAs we continue working towards our goal of a fully decentralized, generalized and censorship-resistant messaging protocol,\\nthese are some of the current and future focus areas for `nwaku`:\\n\\n### Reaching out to operators:\\n\\nWe are starting to push for operators to run and maintain their own Waku v2 nodes,\\npreferably contributing to the default Waku v2 network as described by the default pubsub topic (`/waku/2/default-waku/proto`).\\nAmongst other things, a large fleet of stable operator-run Waku v2 nodes will help secure the network,\\nprovide valuable services to a variety of applications\\nand ensure the future sustainability of both Vac as a research organization and the Waku suite of protocols.\\n\\nWe are targeting `nwaku` as the main option for operator-run nodes.  \\nSpecifically, we aim to provide through `nwaku`:\\n\\n1.  a lightweight and robust Waku v2 client.\\n    This client must be first in line to support innovative and new Waku v2 protocols,\\n    but configurable enough to serve the adaptive needs of various operators.\\n2.  an easy-to-follow guide for operators to configure,\\n    set up and maintain their own nodes\\n3.  a set of operator-focused tools to monitor and maintain a running node\\n\\n### Better conversational security layer guarantees\\n\\nConversational security guarantees in Waku v2 are currently designed around the Status application.\\nDevelopers building their own applications on top of Waku would therefore\\neither have to reimplement a set of tools similar to Status\\nor build their own security solutions on the application layer above Waku.\\nWe are working on [a set of features](https://github.com/vacp2p/research/issues/97) built into Waku\\nthat will provide the general security properties Waku users may desire\\nand do so in a modern and simple way.\\nThis is useful for applications outside of Status that want similar security guarantees.\\nAs a first step, we\'ve already made good progress toward [integrating noise handshakes](https://forum.vac.dev/t/noise-handshakes-as-key-exchange-mechanism-for-waku2/130) as a key exchange mechanism in Waku v2.\\n\\n### Protocol incentivization\\n\\nWe want to design incentivization around our protocols to encourage desired behaviors in the Waku network,\\nrewarding nodes providing costly services\\nand punishing adversarial actions.\\nThis will increase the overall security of the network\\nand encourage operators to run their own Waku nodes.\\nIn turn, the sustainability of Vac as an organization will be better guaranteed.\\nAs such, protocol incentivization was a major focus in our recent [Vac Sustainability and Business Workshop](https://forum.vac.dev/t/vac-sustainability-and-business-workshop/).\\nOur first step here is to finish integrating RLN relay into Waku\\nwith blockchain interaction to manage members,\\npunish spammers\\nand reward spam detectors.\\nAfter this, we want to design monetary incentivization for providers of `store`, `lightpush` and `filter` services.\\nThis may also tie into a reputation mechanism for service nodes based on a network-wide consensus on service quality.\\nA big challenge for protocol incentivization is doing it in a private fashion,\\nso we can keep similar metadata protection guarantees as the Waku base layer.\\nThis ties into our focus on [Zero Knowledge tech](https://forum.vac.dev/t/vac-3-zk/97).\\n\\n### Improved store capacity\\n\\nThe `nwaku` store currently serves as an efficient in-memory store for historical messages,\\ndimensioned by the maximum number of messages the store node is willing to keep.\\nThis makes the `nwaku` store appropriate for keeping history over a short term\\nwithout any time-based guarantees,\\nbut with the advantage of providing fast responses to history queries.\\nSome applications, such as Status, require longer-term historical message storage\\nwith time-based dimensioning\\nto guarantee that messages will be stored for a specified minimum period.\\nBecause of the relatively high cost of memory compared to disk space,\\na higher capacity store, with time guarantees, should operate as a disk-only database of historical messages.\\nThis is an ongoing effort.\\n\\n### Multipurpose discovery\\n\\nIn addition to [the three discovery methods](#4-peer-discovery) already implemented in `nwaku`,\\nwe are working on improving discovery on at least three fronts:\\n\\n#### _Capability discovery:_\\n\\nWaku v2 nodes may be interested in peers with specific capabilities, for example:\\n\\n1. peers within a specific pubsub topic mesh,\\n2. peers with **store** capability,\\n3. **store** peers with x days of history for a specific content topic, etc.\\n\\nCapability discovery entails mechanisms by which such capabilities can be advertised and discovered/negotiated.\\nOne major hurdle to overcome is the increased complexity of finding a node with specific capabilities within the larger network (a needle in a haystack).\\nSee the [original problem statement](https://github.com/vacp2p/rfc/issues/429) for more.\\n\\n#### _Improvements in Discovery v5_\\n\\nOf the implemented discovery methods,\\nDiscovery v5 best addresses our need for a decentralized and scalable discovery mechanism.\\nWith the basic implementation done,\\nthere are some improvements planned for Discovery v5,\\nincluding methods to increase security such as merging with the Ethereum Discovery v5 network,\\nintroducing explicit NAT traversal\\nand utilizing [topic advertisement](https://github.com/ethereum/devp2p/blob/fa6428ada7385c13551873b2ae6ad2457c228eb8/discv5/discv5-theory.md#topic-advertisement).\\nThe [Waku v2 Discovery v5 Roadmap](https://forum.vac.dev/t/waku-v2-discv5-roadmap-discussion/121) contains more details.\\n\\n#### _Generalized peer exchange_\\n\\n`nwaku` already implements [GossipSub peer exchange](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/gossipsub-v1.1.md#prune-backoff-and-peer-exchange).\\nWe now need a general request-response mechanism outside of GossipSub\\nby which a node may learn about other Waku v2 nodes\\nby requesting and receiving a list of peers from a neighbor.\\nThis could, for example, be a suitable way for resource-restricted devices to request a stronger peer\\nto perform a random Discovery v5 lookup on their behalf\\nor simply to be informed of a subset of the peers known to that neighbor.\\nSee [this issue](https://github.com/vacp2p/rfc/issues/495) for more.\\n\\n---\\n\\nThis concludes a general outline of some of the main recent developments in the `nwaku` client\\nand a summary of the current and future focus areas.\\nMuch more is happening behind the scenes, of course,\\nso for more information, or to join the conversation,\\nfeel free to join our [Vac Discord](https://discord.gg/KNj3ctuZvZ) server\\nor to check out the [`nwaku` repo on Github](https://github.com/status-im/nim-waku).\\nYou can also view the changelog for past releases [here](https://github.com/status-im/nim-waku/releases).\\n\\n## References\\n\\n- [17/WAKU-RLN-RELAY](https://rfc.vac.dev/waku/standards/core/17/rln-relay)\\n- [32/RLN](https://rfc.vac.dev/vac/32/rln-v1)\\n- [33/WAKU2-DISCV5](https://rfc.vac.dev/waku/standards/core/33/discv5)\\n- [Capabilities advertising](https://github.com/vacp2p/rfc/issues/429)\\n- [Configuring a domain name](https://github.com/status-im/nim-waku/tree/d2fccb5220144893f994a67f2cc26661247f101f/waku/v2#configuring-a-domain-name)\\n- [Conversational security](https://github.com/vacp2p/research/issues/97)\\n- [Discovery v5 Topic Advertisement](https://github.com/ethereum/devp2p/blob/fa6428ada7385c13551873b2ae6ad2457c228eb8/discv5/discv5-theory.md#topic-advertisement)\\n- [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459)\\n- [GossipSub Peer Exchange](https://github.com/libp2p/specs/blob/10712c55ab309086a52eec7d25f294df4fa96528/pubsub/gossipsub/gossipsub-v1.1.md#prune-backoff-and-peer-exchange)\\n- [go-waku](https://github.com/status-im/go-waku)\\n- [js-waku](https://github.com/status-im/js-waku)\\n- [`multiaddr` formats](https://github.com/multiformats/multiaddr/blob/b746a7d014e825221cc3aea6e57a92d78419990f/protocols.csv#L8-L11)\\n- [nimbus-eth2](https://github.com/status-im/nimbus-eth2)\\n- [nim-libp2p](https://github.com/status-im/nim-libp2p)\\n- [nim-waku](https://github.com/status-im/nim-waku)\\n- [nim-waku releases](https://github.com/status-im/nim-waku/releases)\\n- [Node Discovery Protocol v5 - Theory](https://github.com/ethereum/devp2p/blob/fa6428ada7385c13551873b2ae6ad2457c228eb8/discv5/discv5-theory.md)\\n- [Noise handshakes](https://forum.vac.dev/t/noise-handshakes-as-key-exchange-mechanism-for-waku2/130)\\n- [RLN tutorial](https://github.com/status-im/nim-waku/blob/ee96705c7fbe4063b780ac43b7edee2f6c4e351b/docs/tutorial/rln-chat2-live-testnet.md)\\n- [Vac <3 ZK](https://forum.vac.dev/t/vac-3-zk/97)\\n- [Vac About page](https://vac.dev/#about)\\n- [Vac Research log](https://vac.dev/research-log/)\\n- [Vac RFC site](https://rfc.vac.dev/)\\n- [Vac Sustainability and Business Workshop](https://forum.vac.dev/t/vac-sustainability-and-business-workshop/)\\n- [Waku Update](/waku-update)\\n- [Waku v1 vs Waku v2: Bandwidth Comparison](/waku-v1-v2-bandwidth-comparison)\\n- [Waku v2 Peer Exchange](https://github.com/vacp2p/rfc/issues/495)\\n- [Waku v2 Discovery v5 Roadmap](https://forum.vac.dev/t/waku-v2-discv5-roadmap-discussion/121)\\n- [What\'s the Plan for Waku v2?](/waku-v2-plan)"},{"id":"ethics-surveillance-tech","metadata":{"permalink":"/rlog/ethics-surveillance-tech","source":"@site/rlog/2021-12-03-ethics-surveillance-tech.mdx","title":"Opinion: Pseudo-ethics in the Surveillance Tech Industry","description":"A look at typical ethical shortfalls in the global surveillance tech industry.","date":"2021-12-03T10:00:00.000Z","formattedDate":"December 3, 2021","tags":[],"readingTime":11.715,"hasTruncateMarker":true,"authors":[{"name":"Circe","twitter":"vacp2p","github":"thecirce","key":"circe"}],"frontMatter":{"layout":"post","name":"Opinion: Pseudo-ethics in the Surveillance Tech Industry","title":"Opinion: Pseudo-ethics in the Surveillance Tech Industry","date":"2021-12-03T10:00:00.000Z","authors":"circe","published":true,"slug":"ethics-surveillance-tech","categories":"research","summary":null,"discuss":null},"prevItem":{"title":"Introducing nwaku","permalink":"/rlog/introducing-nwaku"},"nextItem":{"title":"Waku v1 vs Waku v2: Bandwidth Comparison","permalink":"/rlog/waku-v1-v2-bandwidth-comparison"}},"content":"A look at typical ethical shortfalls in the global surveillance tech industry.\\n\\n\x3c!--truncate--\x3e\\n\\n_This is an opinion piece by pseudonymous contributor, circe._\\n\\n## Preface\\n\\nThe Vac team aims to provide a public good in the form of freely available, open source tools and protocols for decentralized communication.\\nAs such, we value our independence and the usefulness of our protocols for a wide range of applications.\\nAt the same time, we realize that all technical development, including ours, has a moral component.\\nAs a diverse team we are guided by a shared devotion to the principles of human rights and liberty.\\nThis explains why we place such a high premium on security, censorship-resistance and privacy -\\na stance we [share with the wider Status Network](https://our.status.im/our-principles/).\\nThe post below takes a different approach from our usual more technical analyses,\\nby starting to peel back the curtain on the ethical shortfalls of the global surveillance tech industry.\\n\\n## Spotlight on an industry\\n\\n[Apple\'s announcement](https://www.apple.com/newsroom/2021/11/apple-sues-nso-group-to-curb-the-abuse-of-state-sponsored-spyware/) of their lawsuit against Israel\'s NSO Group\\nmarks the latest in a series of recent setbacks for the surveillance tech company.\\nIn early November, the [United States blacklisted the firm](https://public-inspection.federalregister.gov/2021-24123.pdf),\\nciting concerns about the use of their spyware by foreign governments targeting civilians such as \\"journalists, businesspeople, activists\\" and more.\\nThe company is already [embroiled in a lawsuit with Whatsapp](https://www.reuters.com/article/us-facebook-cyber-whatsapp-nsogroup-idUSKBN1X82BE)\\nover their exploit of the chat app\'s video calling service to install malware on target devices.\\nNSO Group\'s most infamous product, [Pegasus](https://forbiddenstories.org/case/the-pegasus-project/), operates as a hidden exploit installed on victims\' mobile phones,\\nsometimes without even requiring as much as an unguarded click on a malicious link.\\nIt has the potential to lay bare, and report to its owners, _everything_ within the reach of the infected device.\\nFor most people this amounts to a significant portion of their private lives and thoughts.\\nPegasus can read your private messages (even encrypted), collect your passwords, record calls, track your location and access your device\'s microphone and camera.\\nNo activity or application on an infected phone would be hidden.\\n\\nThe latest controversies are perhaps less because of the novelty of the revelations -\\nthe existence of Pegasus has been known to civil activists [since at least 2016](https://www.bbc.com/news/technology-37192670).\\nRather, the public was reminded again of the potential scope of surveillance tech\\nin the indiscriminate use of Pegasus on private citizens.\\nThis has far-reaching implications for human freedoms worldwide.\\nEarlier this year, a [leaked list of over 50,000 targets](https://www.theguardian.com/world/2021/jul/18/revealed-leak-uncovers-global-abuse-of-cyber-surveillance-weapon-nso-group-pegasus), or possible targets, of Pegasus included\\nthe phone numbers of human rights advocates, independent journalists, lawyers and political activists.\\nThis should have come as no surprise.\\nThe type of autocratically inclined agents, and governments, who would venture to buy and use such invasive cyber-arms often target those they find politically inconvenient.\\nPegasus, and similar technologies, simply extend the reach and capacity of such individuals and governments -\\nno border or distance, no political rank or social advantage, no sanctity of profession or regard for dignity,\\nprovide any indemnity from becoming a victim.\\nYour best hope is to remain uninteresting enough to escape consideration.\\n\\nThe NSO Group has, of course, denied allegations of culpability and questions the authenticity of the list.\\nAt this stage, the latter is almost beside the point:\\nAmnesty International\'s cybersecurity team, Security Lab, _did_ find [forensic evidence of Pegasus](https://www.amnesty.org/en/latest/research/2021/07/forensic-methodology-report-how-to-catch-nso-groups-pegasus/#_ftn1) on the phones of several volunteers whose numbers appeared on the original list,\\nincluding those of journalists and human rights activists.\\n(Security Lab has since opened up their [infection finding tool](https://github.com/mvt-project/mvt) to the public.)\\nFrench intelligence has similarly [inspected and confirmed](https://www.theguardian.com/news/2021/aug/02/pegasus-spyware-found-on-journalists-phones-french-intelligence-confirms) infection of at least three devices belonging to journalists.\\nThe phones of several people who were close to the Saudi-American journalist, Jamal Khashoggi, were [confirmed hacked](https://www.bbc.com/news/world-57891506)\\nboth before and after Khashoggi\'s brutal murder at the Saudi embassy in Istanbul in 2018.\\n[More reports](https://www.theguardian.com/news/2021/sep/21/hungary-journalist-daniel-nemeth-phones-infected-with-nso-pegasus-spyware) of confirmed Pegasus hacks are still published with some regularity.\\nIt is now an open secret that many authoritarian governments have bought Pegasus.\\nIt\'s not difficult to extrapolate from existing reports and such clients\' track records\\nwhat the potential injuries to human freedoms are that they can inflict with access to such a powerful cyberweapon.\\n\\n## A typical response\\n\\n[NSO\'s response](https://www.theguardian.com/news/2021/jul/18/response-from-nso-and-governments) to the allegations follows a textbook approach\\nof avoiding earnest ethical introspection on the manufacturing, and selling, of cyber-arms.\\nFirstly, shift ethical responsibility to a predetermined process, a list of checkboxes of your own making.\\nThe Group, for example, claims to sell only to \\"vetted governments\\", following a classification process\\nof which they have now [published some procedural details](https://www.nsogroup.com/wp-content/uploads/2021/06/ReportBooklet.pdf) but no tangible criteria.\\nThe next step is to reaffirm continuously, and repetitively, your dedication to the _legal_ combat against crime,\\n[\\"legitimate law enforcement agencies\\"](https://www.nsogroup.com/wp-content/uploads/2021/06/ReportBooklet.pdf) (note the almost tautological phrasing),\\nadherence to international arms trade laws,\\ncompliance clauses in customer contracts, etc.\\nThirdly, having been absolved of any moral suspicions that might exist about product and process,\\nfrom conception to engineering to trade,\\ndistance yourself from the consequences of its use in the world.\\n[\\"NSO does not operate its technology, does not collect, nor possesses, nor has any access to any kind of data of its customers.\\"](https://www.theguardian.com/news/2021/jul/18/response-from-nso-and-governments)\\nIt is interesting that directly after this statement they claim with contradictory confidence that\\ntheir \\"technology was not associated in any way with the heinous murder of Jamal Khashoggi\\".\\nThe unapologetic tone seems hardly appropriate when the same document confirms that the Group had to\\nshut down customers\' systems due to \\"confirmed misuse\\" and have had to do so \\"multiple times\\" in the past.\\nGiven all this, the response manages to evade any serious interrogation of the \\"vetting\\" process itself,\\nwhich forced the company to reject \\"approximately 15% of potential new opportunities for Pegasus\\" in one year.\\nCourageous.\\n\\nWe have heard this all before.\\nThere exists a multi-billion dollar industry of private companies and engineering firms [thriving on proceeds](https://www.economist.com/business/2019/12/12/offering-software-for-snooping-to-governments-is-a-booming-business) from\\nselling surveillance tools and cyber-arms to dubious agencies and foreign governments.\\nIn turn, the most power-hungry and oppressive regimes often _rely_ on such technological innovations -\\nfor which they lack the in-country engineering expertise -\\nto maintain control, suppress uprisings, intimidate opposing journalists, and track their citizens.\\nIt\'s a lucrative business opportunity, and resourceful companies have sprung up everywhere to supply this demand,\\noften in countries where citizens, including employees of the company, would be horrified if they were similarly subject to the oppressions of their own products.\\nWhen, in 2014, Italy\'s _HackingTeam_ were pulsed by the United Nations about their (then alleged) selling of spyware to Sudan,\\nwhich would have been a contravention of the UN\'s weapon export ban,\\nthey simply replied that their product was not controlled as a weapon and therefore not subject to such scrutiny.\\nThey remained within their legal bounds, technically.\\nFurthermore, they similarly shifted ethical responsibility to external standards of legitimacy,\\nclaiming their [\\"software is not sold to governments that are blacklisted by the EU, the US, NATO, and similar international organizations\\"](https://citizenlab.ca/2014/02/mapping-hacking-teams-untraceable-spyware/).\\nWhen the company themselves were [hacked in 2015](https://www.wired.com/2015/07/hacking-team-breach-shows-global-spying-firm-run-amok/),\\nrevelations (confirmations, that is) of widespread misuse by repressive governments were damaging enough to force them to disappear and rebrand as Memento Labs.\\n[Their website](https://www.mem3nt0.com/en/) boasts an impressive list of statutes, regulations, procedures, export controls and legal frameworks,\\nall of which the rebranded hackers proudly comply with.\\nSurely no further ethical scrutiny is necessary?\\n\\n## Ethics != the law\\n\\n### The law is trailing behind\\n\\nSuch recourse to the _legality_ of your action as ethical justification is moot for several reasons.\\nThe first is glaringly obvious -\\nour laws are ill-equipped to address the implications of modern technology.\\nLegal systems are a cumbersome inheritance built over generations.\\nThis is especially true of the statutes and regulations governing international trade, behind which these companies so often hide.\\nOur best legal systems are trailing miles behind the technology for which we seek guidelines.\\nLegislators are still struggling to make sense of technologies like face recognition,\\nthe repercussions of smart devices acting \\"on their own\\" and biases in algorithms.\\nTo claim you are performing ethical due diligence by resorting to an outdated and incomplete system of legal codes is disingenuous.\\n\\n### The law depends on ethics\\n\\nThe second reason is more central to my argument,\\nand an important flaw in these sleight of hand justifications appearing from time to time in the media.\\nEthics can in no way be confused as synonymous with legality or legitimacy.\\nThese are incommensurable concepts.\\nIn an ideal world, of course, the law is meant to track the minimum standards of ethical conduct in a society.\\nLaws are often drafted exactly from some ethical, and practical, impulse to minimize harmful conduct\\nand provide for corrective and punitive measures where transgressions do occur.\\nThe law, however, has a much narrower scope than ethics.\\nIt can be just or unjust.\\nIn fact, it is in need of ethics to constantly reform.\\nEthics and values are born out of collective self-reflection.\\nIt develops in our conversation with ourselves and others about the type of society we strive for.\\nAs such, an ethical worldview summarizes our deepest intuitions about how we should live and measure our impact on the world.\\nFor this reason, ethics is primarily enforced by social and internal pressures, not legal boundaries -\\nour desire to do what _ought_ to be done, however we define that.\\nEthics is therefore a much grander scheme than global legal systems\\nand the diplomatic frameworks that grants legitimacy to governments.\\nThese are but one limited outflow of the human aspiration to form societies in accordance with our ideologies and ethics.\\n\\n### International law is vague and exploitable\\n\\nOf course, the cyber-arms trade has a favorite recourse, _international_ law, which is even more limited.\\nSince such products are seldomly sold to governments and agencies within the country of production,\\nit enables a further distancing from consequences.\\nMany private surveillance companies are based in fairly liberal societies with (seemingly) strict emphases on human rights in their domestic laws.\\nInternational laws are much more complicated - for opportunists a synonym for \\"more grey areas in which to hide\\".\\nCompany conduct can now be governed, and excused, by a system that follows\\nthe whims of autocrats with exploitative intent and vastly different ethical conceptions from the company\'s purported aims.\\nInternational law, and the ways it is most often enforced by way of, say, UN-backed sanctions,\\nhave long been shaped by the compromises of international diplomacy.\\nTo be blunt: these laws are weak and subject to exactly the sort of narrow interests behind which mercenaries have always hidden.\\nThe surveillance tech industry is no exception.\\n\\n## Conclusion\\n\\nMy point is simple:\\nselling cyber-arms with the potential to become vast tools of oppression to governments and bodies with blatant histories of human rights violations,\\nand all but the publicly announced intention to continue operating in this way,\\nis categorically unconscionable.\\nThis seems obvious no matter what ethics system you argue from,\\nprovided it harbors any consideration for human dignity and freedom.\\nIt is a sign of poor moral discourse that such recourses to law and legitimacy are often considered synonymous with ethical justification.\\n\\"_I have acted within the bounds of law_\\", _\\"We supply only to legitimate law enforcement agencies\\"_, etc. are no substitutes.\\nEthical conduct requires an honest evaluation of an action against some conception of \\"the good\\",\\nhowever you define that.\\nToo often the surveillance tech industry precisely sidesteps this question,\\nboth in internal processes and external rationalisations to a concerned public.\\n\\nJohn Locke, he of the life-liberty-and-property, articulated the idea that government exists solely through the consent of the governed.\\nTowards the end of the 17th century, he wrote in his _Second Treatise on Civil Government_,\\n\\"[w]henever legislators endeavor to take away,\\nand destroy the property of the people, or to reduce them to slavery under arbitrary power,\\nthey put themselves in a state of war with the people, who are thereupon absolved from any further obedience\\".\\nThe inference is straightforward and humanist in essence:\\nlegitimacy is not something that is conferred by governments and institutions.\\nRather, they derive their legitimacy from us, their citizens, holding them to standards of ethics and societal ideals.\\nThis legitimacy only remains in tact as long as this mandate is honored and continuously extended by a well-informed public.\\nThis is the principle of informed consent on which all reciprocal ethics is based.\\n\\nThe surveillance tech industry may well have nothing more or less noble in mind than profit-making within legal bounds\\nwhen developing and selling their products.\\nHowever, when such companies are revealed again and again to have supplied tools of gross human rights violations to known human rights violators,\\nthey will do well to remember that ethics always _precedes_ requirements of legality and legitimacy.\\nIt is a fallacy to take normative guidance from the concept of \\"legitimacy\\"\\nif the concept itself depends on such normative guidelines for definition.\\nWithout examining the ethical standards by which institutions, governments, and laws, were created,\\nno value-judgements about their legitimacy can be made.\\nHiding behind legal compliance as substitute for moral justification is not enough.\\nTargets of increasingly invasive governmental snooping are too often chosen precisely to suppress the mechanisms from which the legitimacy of such governments flow -\\nthe consent of ordinary civilians.\\nFree and fair elections, free speech, free media, freedom of thought are all at risk.\\n\\n## References\\n\\n- [Status Principles](https://our.status.im/our-principles/)\\n- [Federal Register: Addition of Certain Entities to the Entity List](https://public-inspection.federalregister.gov/2021-24123.pdf)\\n- [forbiddenstories.org: The Pegasus Project](https://forbiddenstories.org/case/the-pegasus-project/)\\n- [theguardian.com: The Pegasus Project](https://www.theguardian.com/news/series/pegasus-project)\\n- [amnesty.org Forensic Methodology Report: How to catch NSO Group\u2019s Pegasus](https://www.amnesty.org/en/latest/research/2021/07/forensic-methodology-report-how-to-catch-nso-groups-pegasus/#_ftn1)\\n- [Apple sues NSO Group to curb the abuse of state-sponsored spyware](https://www.apple.com/newsroom/2021/11/apple-sues-nso-group-to-curb-the-abuse-of-state-sponsored-spyware/)\\n- [bbc.com: Who are the hackers who cracked the iPhone?](https://www.bbc.com/news/technology-37192670)\\n- [bbc.com: Pegasus: Who are the alleged victims of spyware targeting?](https://www.bbc.com/news/world-57891506)\\n- [citizenlab.ca: Mapping Hacking Team\u2019s \u201cUntraceable\u201d Spyware](https://citizenlab.ca/2014/02/mapping-hacking-teams-untraceable-spyware/)\\n- [economist.com: Offering software for snooping to governments is a booming business](https://www.economist.com/business/2019/12/12/offering-software-for-snooping-to-governments-is-a-booming-business)\\n- [Memento Labs](https://www.mem3nt0.com/en/)\\n- [Mobile Verification Toolkit to identify compromised devices](https://github.com/mvt-project/mvt)\\n- [NSO Group: Transparency and Responsibility Report 2021](https://www.nsogroup.com/wp-content/uploads/2021/06/ReportBooklet.pdf)\\n- [reuters.com: WhatsApp sues Israel\'s NSO for allegedly helping spies hack phones around the world](https://www.reuters.com/article/us-facebook-cyber-whatsapp-nsogroup-idUSKBN1X82BE)\\n- [wired.com: Hacking Team Breach Shows a Global Spying Firm Run Amok](https://www.wired.com/2015/07/hacking-team-breach-shows-global-spying-firm-run-amok/)"},{"id":"waku-v1-v2-bandwidth-comparison","metadata":{"permalink":"/rlog/waku-v1-v2-bandwidth-comparison","source":"@site/rlog/2021-10-25-waku-v1-vs-waku-v2.mdx","title":"Waku v1 vs Waku v2: Bandwidth Comparison","description":"A local comparison of bandwidth profiles showing significantly improved scalability in Waku v2 over Waku v1.","date":"2021-11-03T10:00:00.000Z","formattedDate":"November 3, 2021","tags":[],"readingTime":9.345,"hasTruncateMarker":true,"authors":[{"name":"Hanno Cornelius","twitter":"4aelius","github":"jm-clius","key":"hanno"}],"frontMatter":{"layout":"post","name":"Waku v1 vs Waku v2: Bandwidth Comparison","title":"Waku v1 vs Waku v2: Bandwidth Comparison","date":"2021-11-03T10:00:00.000Z","authors":"hanno","published":true,"slug":"waku-v1-v2-bandwidth-comparison","categories":"research","image":"/img/waku1-vs-waku2/waku1-vs-waku2-overall-network-size.png","discuss":"https://forum.vac.dev/t/discussion-waku-v1-vs-waku-v2-bandwidth-comparison/110"},"prevItem":{"title":"Opinion: Pseudo-ethics in the Surveillance Tech Industry","permalink":"/rlog/ethics-surveillance-tech"},"nextItem":{"title":"[Talk at COSCUP] Vac, Waku v2 and Ethereum Messaging","permalink":"/rlog/waku-v2-ethereum-coscup"}},"content":"A local comparison of bandwidth profiles showing significantly improved scalability in Waku v2 over Waku v1.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nThe [original plan](https://vac.dev/waku-v2-plan) for Waku v2 suggested theoretical improvements in resource usage over Waku v1,\\nmainly as a result of the improved amplification factors provided by GossipSub.\\nIn its turn, [Waku v1 proposed improvements](https://vac.dev/fixing-whisper-with-waku) over its predecessor, Whisper.\\n\\nGiven that Waku v2 is aimed at resource restricted environments,\\nwe are specifically interested in its scalability and resource usage characteristics.\\nHowever, the theoretical performance improvements of Waku v2 over Waku v1,\\nhas never been properly benchmarked and tested.\\n\\nAlthough we\'re working towards a full performance evaluation of Waku v2,\\nthis would require significant planning and resources,\\nif it were to simulate \\"real world\\" conditions faithfully and measure bandwidth and resource usage across different network connections,\\nrobustness against attacks/losses, message latencies, etc.\\n(There already exists a fairly comprehensive [evaluation of GossipSub v1.1](https://research.protocol.ai/publications/gossipsub-v1.1-evaluation-report/vyzovitis2020.pdf),\\non which [`11/WAKU2-RELAY`](https://rfc.vac.dev/waku/standards/core/11/relay) is based.)\\n\\nAs a starting point,\\nthis post contains a limited and local comparison of the _bandwidth_ profile (only) between Waku v1 and Waku v2.\\nIt reuses and adapts existing network simulations for [Waku v1](https://github.com/status-im/nim-waku/blob/master/waku/v1/node/quicksim.nim) and [Waku v2](https://github.com/status-im/nim-waku/blob/master/waku/v2/node/quicksim2.nim)\\nand compares bandwidth usage for similar message propagation scenarios.\\n\\n## Theoretical improvements in Waku v2\\n\\nMessages are propagated in Waku v1 using [flood routing](<https://en.wikipedia.org/wiki/Flooding_(computer_networking)>).\\nThis means that every peer will forward every new incoming message to all its connected peers (except the one it received the message from).\\nThis necessarily leads to unnecessary duplication (termed _amplification factor_),\\nwasting bandwidth and resources.\\nWhat\'s more, we expect this effect to worsen the larger the network becomes,\\nas each _connection_ will receive a copy of each message,\\nrather than a single copy per peer.\\n\\nMessage routing in Waku v2 follows the `libp2p` _GossipSub_ protocol,\\nwhich lowers amplification factors by only sending full message contents to a subset of connected peers.\\nAs a Waku v2 network grows, each peer will limit its number of full-message (\\"mesh\\") peerings -\\n`libp2p` suggests a maximum of `12` such connections per peer.\\nThis allows much better scalability than a flood-routed network.\\nFrom time to time, a Waku v2 peer will send metadata about the messages it has seen to other peers (\\"gossip\\" peers).\\n\\nSee [this explainer](https://hackmd.io/@vac/main/%2FYYlZYBCURFyO_ZG1EiteWg#11WAKU2-RELAY-gossipsub) for a more detailed discussion.\\n\\n## Methodology\\n\\nThe results below contain only some scenarios that provide an interesting contrast between Waku v1 and Waku v2.\\nFor example, [star network topologies](https://en.wikipedia.org/wiki/Star_network) do not show a substantial difference between Waku v1 and Waku v2.\\nThis is because each peer relies on a single connection to the central node for every message,\\nwhich barely requires any routing:\\neach connection receives a copy of every message for both Waku v1 and Waku v2.\\nHybrid topologies similarly show only a difference between Waku v1 and Waku v2 for network segments with [mesh-like connections](https://en.wikipedia.org/wiki/Mesh_networking),\\nwhere routing decisions need to be made.\\n\\nFor this reason, the following approach applies to all iterations:\\n\\n1. Simulations are run **locally**.\\n   This limits the size of possible scenarios due to local resource constraints,\\n   but is a way to quickly get an approximate comparison.\\n2. Nodes are treated as a **blackbox** for which we only measure bandwidth,\\n   using an external bandwidth monitoring tool.\\n   In other words, we do not consider differences in the size of the envelope (for v1) or the message (for v2).\\n3. Messages are published at a rate of **50 new messages per second** to each network,\\n   except where explicitly stated otherwise.\\n4. Each message propagated in the network carries **8 bytes** of random payload, which is **encrypted**.\\n   The same symmetric key cryptographic algorithm (with the same keys) are used in both Waku v1 and v2.\\n5. Traffic in each network is **generated from 10 nodes** (randomly-selected) and published in a round-robin fashion to **10 topics** (content topics for Waku v2).\\n   In practice, we found no significant difference in _average_ bandwidth usage when tweaking these two parameters (the number of traffic generating nodes and the number of topics).\\n6. Peers are connected in a decentralized **full mesh topology**,\\n   i.e. each peer is connected to every other peer in the network.\\n   Waku v1 is expected to flood all messages across all existing connections.\\n   Waku v2 gossipsub will GRAFT some of these connections for full-message peerings,\\n   with the rest being gossip-only peerings.\\n7. After running each iteration, we **verify that messages propagated to all peers** (comparing the number of published messages to the metrics logged by each peer).\\n\\nFor Waku v1, nodes are configured as \\"full\\" nodes (i.e. with full bloom filter),\\nwhile Waku v2 nodes are `relay` nodes, all subscribing and publishing to the same PubSub topic.\\n\\n## Network size comparison\\n\\n### Iteration 1: 10 nodes\\n\\nLet\'s start with a small network of 10 nodes only and see how Waku v1 bandwidth usage compares to that of Waku v2.\\nAt this small scale we don\'t expect to see improved bandwidth usage in Waku v2 over Waku v1,\\nsince all connections, for both Waku v1 and Waku v2, will be full-message connections.\\nThe number of connections is low enough that Waku v2 nodes will likely GRAFT all connections to full-message peerings,\\nessentially flooding every message on every connection in a similar fashion to Waku v1.\\nIf our expectations are confirmed, it helps validate our methodology,\\nshowing that it gives more or less equivalent results between Waku v1 and Waku v2 networks.\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-10-nodes.png)\\n\\nSure enough, the figure shows that in this small-scale setup,\\nWaku v1 actually has a lower per-peer bandwidth usage than Waku v2.\\nOne reason for this may be the larger overall proportion of control messages in a gossipsub-routed network such as Waku v2.\\nThese play a larger role when the total network traffic is comparatively low, as in this iteration.\\nAlso note that the average bandwidth remains more or less constant as long as the rate of published messages remains stable.\\n\\n### Iteration 2: 30 nodes\\n\\nNow, let\'s run the same scenario for a larger network of highly-connected nodes, this time consisting of 30 nodes.\\nAt this point, the Waku v2 nodes will start pruning some connections to limit the number of full-message peerings (to a maximum of `12`),\\nwhile the Waku v1 nodes will continue flooding messages to all connected peers.\\nWe therefore expect to see a somewhat improved bandwidth usage in Waku v2 over Waku v1.\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-30-nodes.png)\\n\\nBandwidth usage in Waku v2 has increased only slightly from the smaller network of 10 nodes (hovering between 2000 and 3000 kbps).\\nThis is because there are only a few more full-message peerings than before.\\nCompare this to the much higher increase in bandwidth usage for Waku v1, which now requires more than 4000 kbps on average.\\n\\n### Iteration 3: 50 nodes\\n\\nFor an even larger network of 50 highly connected nodes,\\nthe divergence between Waku v1 and Waku v2 is even larger.\\nThe following figure shows comparative average bandwidth usage for a throughput of 50 messages per second.\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-50-nodes.png)\\n\\nAverage bandwidth usage (for the same message rate) has remained roughly the same for Waku v2 as it was for 30 nodes,\\nindicating that the number of full-message peerings per node has not increased.\\n\\n### Iteration 4: 85 nodes\\n\\nWe already see a clear trend in the bandwidth comparisons above,\\nso let\'s confirm by running the test once more for a network of 85 nodes.\\nDue to local resource constraints, the effective throughput for Waku v1 falls to below 50 messages per second,\\nso the v1 results below have been normalized and are therefore approximate.\\nThe local Waku v2 simulation maintains the message throughput rate without any problems.\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-85-nodes.png)\\n\\n### Iteration 5: 150 nodes\\n\\nFinally, we simulate message propagation in a network of 150 nodes.\\nDue to local resource constraints, we run this simulation at a lower rate -\\n35 messages per second -\\nand for a shorter amount of time.\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-150-nodes.png)\\n\\nNotice how the Waku v1 bandwidth usage is now more than 10 times worse than that of Waku v2.\\nThis is to be expected, as each Waku v1 node will try to flood each new message to 149 other peers,\\nwhile the Waku v2 nodes limit their full-message peerings to no more than 12.\\n\\n### Discussion\\n\\nLet\'s summarize average bandwidth growth against network growth for a constant message propagation rate.\\nSince we are particularly interested in how Waku v1 compares to Waku v2 in terms of bandwidth usage,\\nthe results are normalised to the Waku v2 average bandwidth usage for each network size.\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-overall-network-size.png)\\n\\nExtrapolation is a dangerous game,\\nbut it\'s safe to deduce that the divergence will only grow for even larger network topologies.\\nAlthough control signalling contributes more towards overall bandwidth for Waku v2 networks,\\nthis effect becomes less noticeable for larger networks.\\nFor network segments with more than ~18 densely connected nodes,\\nthe advantage of using Waku v2 above Waku v1 becomes clear.\\n\\n## Network traffic comparison\\n\\nThe analysis above controls the average message rate while network size grows.\\nIn reality, however, active users (and therefore message rates) are likely to grow in conjunction with the network.\\nThis will have an effect on bandwidth for both Waku v1 and Waku v2, though not in equal measure.\\nConsider the impact of an increasing rate of messages in a network of constant size:\\n\\n![](//img/waku1-vs-waku2/waku1-vs-waku2-overall-message-rate.png)\\n\\nThe _rate_ of increase in bandwidth for Waku v2 is slower than that for Waku v1 for a corresponding increase in message propagation rate.\\nIn fact, for a network of 30 densely-connected nodes,\\nif the message propagation rate increases by 1 per second,\\nWaku v1 requires an increased average bandwidth of almost 70kbps at each node.\\nA similar traffic increase in Waku v2 requires on average 40kbps more bandwidth per peer, just over half that of Waku v1.\\n\\n## Conclusions\\n\\n- **Waku v2 scales significantly better than Waku v1 in terms of average bandwidth usage**,\\n  especially for densely connected networks.\\n- E.g. for a network consisting of **150** or more densely connected nodes,\\n  Waku v2 provides more than **10x** better average bandwidth usage rates than Waku v1.\\n- As the network continues to scale, both in absolute terms (number of nodes) and in network traffic (message rates) the disparity between Waku v2 and Waku v1 becomes even larger.\\n\\n## Future work\\n\\nNow that we\'ve confirmed that Waku v2\'s bandwidth improvements over its predecessor matches theory,\\nwe can proceed to a more in-depth characterisation of Waku v2\'s resource usage.\\nSome questions that we want to answer include:\\n\\n- What proportion of Waku v2\'s bandwidth usage is used to propagate _payload_ versus bandwidth spent on _control_ messaging to maintain the mesh?\\n- To what extent is message latency (time until a message is delivered to its destination) affected by network size and message rate?\\n- How _reliable_ is message delivery in Waku v2 for different network sizes and message rates?\\n- What are the resource usage profiles of other Waku v2 protocols (e.g.[`12/WAKU2-FILTER`](https://rfc.vac.dev/waku/standards/core/12/previous-versions00/filter) and [`19/WAKU2-LIGHTPUSH`](https://rfc.vac.dev/waku/standards/core/19/lightpush))?\\n\\nOur aim is to get ever closer to a \\"real world\\" understanding of Waku v2\'s performance characteristics,\\nidentify and fix vulnerabilities\\nand continually improve the efficiency of our suite of protocols.\\n\\n## References\\n\\n- [Evaluation of GossipSub v1.1](https://research.protocol.ai/publications/gossipsub-v1.1-evaluation-report/vyzovitis2020.pdf)\\n- [Fixing Whisper with Waku](https://vac.dev/fixing-whisper-with-waku)\\n- [GossipSub vs flood routing](https://hackmd.io/@vac/main/%2FYYlZYBCURFyO_ZG1EiteWg#11WAKU2-RELAY-gossipsub)\\n- [Network topologies: star](https://www.techopedia.com/definition/13335/star-topology#:~:text=Star%20topology%20is%20a%20network,known%20as%20a%20star%20network.)\\n- [Network topologies: mesh](https://en.wikipedia.org/wiki/Mesh_networking)\\n- [Waku v2 original plan](https://vac.dev/waku-v2-plan)"},{"id":"waku-v2-ethereum-coscup","metadata":{"permalink":"/rlog/waku-v2-ethereum-coscup","source":"@site/rlog/2021-08-06-coscup-waku-ethereum.mdx","title":"[Talk at COSCUP] Vac, Waku v2 and Ethereum Messaging","description":"Learn more about Waku v2, its origins, goals, protocols, implementation and ongoing research. Understand how it is used and how it can be useful for messaging in Ethereum.","date":"2021-08-06T12:00:00.000Z","formattedDate":"August 6, 2021","tags":[],"readingTime":7.12,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"[Talk at COSCUP] Vac, Waku v2 and Ethereum Messaging","title":"[Talk at COSCUP] Vac, Waku v2 and Ethereum Messaging","date":"2021-08-06T12:00:00.000Z","authors":"oskarth","published":true,"slug":"waku-v2-ethereum-coscup","categories":"research","image":"/img/coscup-waku/talk.png","discuss":"https://forum.vac.dev/t/discussion-talk-at-coscup-vac-waku-v2-and-ethereum-messaging/95"},"prevItem":{"title":"Waku v1 vs Waku v2: Bandwidth Comparison","permalink":"/rlog/waku-v1-v2-bandwidth-comparison"},"nextItem":{"title":"Presenting JS-Waku: Waku v2 in the Browser","permalink":"/rlog/presenting-js-waku"}},"content":"Learn more about Waku v2, its origins, goals, protocols, implementation and ongoing research. Understand how it is used and how it can be useful for messaging in Ethereum.\\n\\n\x3c!--truncate--\x3e\\n\\n_This is the English version of a talk originally given in Chinese at COSCUP in Taipei._\\n\\n[video recording with Chinese and English subtitles.](https://www.youtube.com/watch?v=s0ATpQ4_XFc)\\n\\n---\\n\\n## Introduction\\n\\nHi everyone!\\n\\nToday I\'ll talk to you about Waku v2. What it is, what problems it is solving,\\nand how it can be useful for things such as messaging in Ethereum. First, let me\\nstart with some brief background.\\n\\n## Brief history and background\\n\\nBack when Ethereum got started, there used to be this concept of the \\"holy\\ntrinity\\". You had Ethereum for compute/consensus, Swarm for storage, and Whisper\\nfor messaging. This is partly where the term Web3 comes from.\\n\\nStatus started out as an app with the goal of being a window onto Ethereum and\\na secure messenger. As one of the few, if not the only, apps using Whisper in\\nproduction, not to mention on a mobile phone, we quickly realized there were\\nproblems with the underlying protocols and infrastructure. Protocols such as\\nWhisper weren\'t quite ready for prime time yet when it came to things such as\\nscalability and working in the real world.\\n\\nAs we started addressing some of these challenges, and moved from app\\ndevelopement to focusing on protocols, research and infrastructure, we created\\nVac. Vac is an r&d unit doing protocol research focused on creating modular p2p\\nmessaging protocols for private, secure, censorship resistant communication.\\n\\nI won\'t go into too much detail on the issues with Whisper, if you are\\ninterested in this check out this talk\\n[here](https://www.youtube.com/watch?v=6lLT33tsJjs) or this\\n[article](https://vac.dev/fixing-whisper-with-waku).\\n\\nIn a nutshell, we forked Whisper to address immediate shortcomings and this\\nbecame Waku v1. Waku v2 is complete re-thought implementation from scratch on top\\nof libp2p. This will be the subject of today\'s talk.\\n\\n## Waku v2\\n\\n### Overview\\n\\nWaku v2 is a privacy-preserving peer-to-peer messaging protocol for resource\\nrestricted devices. We can look at Waku v2 as several things:\\n\\n- Set of protocols\\n- Set of implementations\\n- Network of nodes\\n\\nLet\'s first look at what the goals are.\\n\\n### Goals\\n\\nWaku v2 provides a PubSub based messaging protocol with the following\\ncharacteristics:\\n\\n1. **Generalized messaging**. Applications that require a messaging protocol to\\n   communicate human to human, machine to machine, or a mix.\\n2. **Peer-to-peer**. For applications that require a p2p solution.\\n3. **Resource restricted**. For example, running with limited bandwidth, being\\n   mostly-offline, or in a browser.\\n4. **Privacy**. Applications that have privacy requirements, such as pseudonymity,\\n   metadata protection, etc.\\n\\nAnd to provide these properties in a modular fashion, where applications can\\nchoose their desired trade-offs.\\n\\n### Protocols\\n\\nWaku v2 consists of several protocols. Here we highlight a few of the most\\nimportant ones:\\n\\n- 10/WAKU2 - main specification, details how all the pieces fit together\\n- 11/RELAY - thin layer on top of GossipSub for message dissemination\\n- 13/STORE - fetching of historical messages\\n- 14/MESSAGE - message payload\\n\\nThis is the recommended subset for a minimal Waku v2 client.\\n\\nIn addition to this there are many other types of specifications at various\\nstages of maturity, such as: content based filtering, bridge mode to Waku v1,\\nJSON RPC API, zkSNARKS based spam protection with RLN, accounting and\\nsettlements with SWAP, fault-tolerant store nodes, recommendations around topic\\nusage, and more.\\n\\nSee https://rfc.vac.dev/ for a full overview.\\n\\n### Implementations\\n\\nWaku v2 consists of multiple implementations. This allows for client diversity,\\nmakes it easier to strengthen the protocols, and allow people to use Waku v2 in\\ndifferent contexts.\\n\\n- nim-waku - the reference client written in Nim, most full-featured.\\n- js-waku - allow usage of Waku v2 from browsers, focus on interacting with dapps.\\n- go-waku - subset of Waku v2 to ease integration into the Status app.\\n\\n### Testnet Huilong and dogfooding\\n\\nIn order to test the protocol we have setup a testnet across all implementations\\ncalled Huilong. Yes, that\'s the Taipei subway station!\\n\\n![](/img/coscup-waku/huilong.jpg)\\n\\nAmong us core devs we have disabled the main #waku Discord channel used for\\ndevelopment, and people run their own node connected to this toy chat application.\\n\\nFeel free to join and say hi! Instructions can be found here:\\n\\n- [nim-waku chat](https://github.com/status-im/nim-waku/blob/master/docs/tutorial/chat2.md)\\n\\n- [js-waku chat](https://status-im.github.io/js-waku/)\\n\\n- [go-waku chat](https://github.com/status-im/go-waku/tree/master/examples/chat2)\\n\\n### Research\\n\\nWhile Waku v2 is being used today, we are actively researching improvements.\\nSince the design is modular, we can gracefully introduce new capabilities. Some\\nof these research areas are:\\n\\n- Privacy-preserving spam protection using zkSNARKs and RLN\\n- Accounting and settlement of resource usage to incentivize nodes to provide services with SWAP\\n- State synchronization for store protocol to make it easier to run a store node without perfect uptime\\n- Better node discovery\\n- More rigorous privacy analysis\\n- Improving interaction with wallets and dapp\\n\\n## Use cases\\n\\nLet\'s look at where Waku v2 is and can be used.\\n\\n### Prelude: Topics in Waku v2\\n\\nTo give some context, there are two different types of topics in Waku v2. One is\\na PubSub topic, for routing. The other is a content topic, which is used for\\ncontent based filtering. Here\'s an example of the default PubSub topic:\\n\\n`/waku/2/default-waku/proto`\\n\\nThis is recommended as it increases privacy for participants and it is stored by\\ndefault, however this is up to the application.\\n\\nThe second type of topic is a content topic, which is application specific. For\\nexample, here\'s the content topic used in our testnet:\\n\\n`/toychat/2/huilong/proto`\\n\\nFor more on topics, see https://rfc.vac.dev/waku/informational/23/topics\\n\\n### Status app\\n\\nIn the Status protocol, content topics - topics in Whisper/Waku v1 - are used for several things:\\n\\n- Contact code topic to discover X3DH bundles for perfect forward secrecy\\n  - Partitioned into N (currently 5000) content topics to balance privacy with efficiency\\n- Public chats correspond to hash of the plaintext name\\n- Negotiated topic for 1:1 chat with DHKE derived content topic\\n\\nSee more here http://rfc.vac.dev/status/deprecated/waku-usage\\n\\nCurrently, Status app is in the process of migrating to and testing Waku v2.\\n\\n### DappConnect: Ethereum messaging\\n\\nIt is easy to think of Waku as being for human messaging, since that\'s how it is\\nprimarily used in the Status app, but the goal is to be useful for generalized\\nmessaging, which includes Machine-To-Machine (M2M) messaging.\\n\\nRecall the concept of the holy trinity with Ethereum/Swarm/Whisper and Web3 that\\nwe mentioned in the beginning. Messaging can be used as a building block for\\ndapps, wallets, and users to communicate with each other. It can be used for\\nthings such as:\\n\\n- Multisig and DAO vote transactions only needing one on-chain operation\\n- Giving dapps ability to send push notifications to users\\n- Giving users ability to directly respond to requests from dapps\\n- Decentralized WalletConnect\\n- Etc\\n\\nBasically anything that requires communication and doesn\'t have to be on-chain.\\n\\n### WalletConnect v2\\n\\nWalletConnect is an open protocol for connecting dapps to wallets with a QR\\ncode. Version 2 is using Waku v2 as a communication channel to do so in a\\ndecentralized and private fashion.\\n\\n![](/img/coscup-waku/walletconnect.png)\\n\\nSee for more: https://docs.walletconnect.org/v/2.0/tech-spec\\n\\nWalletConnect v2 is currently in late alpha using Waku v2.\\n\\n### More examples\\n\\n- Gasless voting and vote aggregation off-chain\\n- Dapp games using Waku as player discovery mechanism\\n- Send encrypted message to someone with an Ethereum key\\n- &lt;Your dapp here&gt;\\n\\nThese are all things that are in progress / proof of concept stage.\\n\\n## Contribute\\n\\nWe\'d love to see contributions of any form!\\n\\n- You can play with it here: [nim-waku chat](https://github.com/status-im/nim-waku/blob/master/docs/tutorial/chat2.md) (/ [js-waku browser chat](https://status-im.github.io/js-waku/))\\n- Use Waku to build a dapp: [js-waku docs](https://status-im.github.io/js-waku/docs/)\\n- Contribute to code: [js-waku](https://github.com/status-im/js-waku) / [nim-waku](https://github.com/status-im/nim-waku)\\n- Contribute to specs: [vacp2p/rfc](https://github.com/vacp2p/rfc)\\n- We are hiring: Wallet & Dapp Integration Developer, Distributed Systems Engineer, Protocol Engineer, Protocol Researcher - all [job listings](https://status.im/our_team/jobs.html)\\n- Join our new [Discord](https://discord.gg/bJCTqS5H)\\n\\n## Conclusion\\n\\nIn this talk we\'ve gone over the original vision for Web3 and how Waku came to\\nbe. We\'ve also looked at what Waku v2 aims to do. We looked at its protocols,\\nimplementations, the current testnet as well as briefly on some ongoing\\nresearch for Vac.\\n\\nWe\'ve also looked at some specific use cases for Waku. First we looked at how\\nStatus uses it with different topics. Then we looked at how it can be useful for\\nmessaging in Ethereum, including for things like WalletConnect.\\n\\nI hope this talk gives you a better idea of what Waku is, why it exists, and\\nthat it inspires you to contribute, either to Waku itself or by using it in your\\nown project!"},{"id":"presenting-js-waku","metadata":{"permalink":"/rlog/presenting-js-waku","source":"@site/rlog/2021-06-04-presenting-js-waku.mdx","title":"Presenting JS-Waku: Waku v2 in the Browser","description":"JS-Waku is bringing Waku v2 to the browser. Learn what we achieved so far and what is next in our pipeline!","date":"2021-06-04T12:00:00.000Z","formattedDate":"June 4, 2021","tags":[],"readingTime":6.84,"hasTruncateMarker":true,"authors":[{"name":"Franck","twitter":"fryorcraken","github":"fryorcraken","key":"franck"}],"frontMatter":{"layout":"post","name":"Presenting JS-Waku: Waku v2 in the Browser","title":"Presenting JS-Waku: Waku v2 in the Browser","date":"2021-06-04T12:00:00.000Z","authors":"franck","published":true,"slug":"presenting-js-waku","categories":"platform","image":"/img/js-waku-gist.png","discuss":"https://forum.vac.dev/t/discussion-presenting-js-waku-waku-v2-in-the-browser/81"},"prevItem":{"title":"[Talk at COSCUP] Vac, Waku v2 and Ethereum Messaging","permalink":"/rlog/waku-v2-ethereum-coscup"},"nextItem":{"title":"Privacy-preserving p2p economic spam protection in Waku v2","permalink":"/rlog/rln-relay"}},"content":"JS-Waku is bringing Waku v2 to the browser. Learn what we achieved so far and what is next in our pipeline!\\n\\n\x3c!--truncate--\x3e\\n\\nFor the past 3 months, we have been working on bringing Waku v2 to the browser.\\nOur aim is to empower dApps with Waku v2, and it led to the creation of a new library.\\nWe believe now is good time to introduce it!\\n\\n## Waku v2\\n\\nFirst, let\'s review what Waku v2 is and what problem it is trying to solve.\\n\\nWaku v2 comes from a need to have a more scalable, better optimised solution for the Status app to achieve decentralised\\ncommunications on resource restricted devices (i.e., mobile phones).\\n\\nThe Status chat feature was initially built over Whisper.\\nHowever, Whisper has a number of caveats which makes it inefficient for mobile phones.\\nFor example, with Whisper, all devices are receiving all messages which is not ideal for limited data plans.\\n\\nTo remediate this, a Waku mode (then Waku v1), based on devp2p, was introduced.\\nTo further enable web and restricted resource environments, Waku v2 was created based on libp2p.\\nThe migration of the Status chat feature to Waku v2 is currently in progress.\\n\\nWe see the need of such solution in the broader Ethereum ecosystem, beyond Status.\\nThis is why we are building Waku v2 as a decentralised communication platform for all to use and build on.\\nIf you want to read more about Waku v2 and what it aims to achieve,\\ncheckout [What\'s the Plan for Waku v2?](/waku-v2-plan).\\n\\nSince last year, we have been busy defining and implementing Waku v2 protocols in [nim-waku](https://github.com/status-im/nim-waku),\\nfrom which you can build [wakunode2](https://github.com/status-im/nim-waku#wakunode).\\nWakunode2 is an adaptive and modular Waku v2 node,\\nit allows users to run their own node and use the Waku v2 protocols they need.\\nThe nim-waku project doubles as a library, that can be used to add Waku v2 support to native applications.\\n\\n## Waku v2 in the browser\\n\\nWe believe that dApps and wallets can benefit from the Waku network in several ways.\\nFor some dApps, it makes sense to enable peer-to-peer communications.\\nFor others, machine-to-machine communications would be a great asset.\\nFor example, in the case of a DAO,\\nWaku could be used for gas-less voting.\\nEnabling the DAO to notify their users of a new vote,\\nand users to vote without interacting with the blockchain and spending gas.\\n\\n[Murmur](https://github.com/status-im/murmur) was the first attempt to bring Whisper to the browser,\\nacting as a bridge between devp2p and libp2p.\\nOnce Waku v2 was started and there was a native implementation on top of libp2p,\\na [chat POC](https://github.com/vacp2p/waku-web-chat) was created to demonstrate the potential of Waku v2\\nin web environment.\\nIt showed how using js-libp2p with few modifications enabled access to the Waku v2 network.\\nThere was still some unresolved challenges.\\nFor example, nim-waku only support TCP connections which are not supported by browser applications.\\nHence, to connect to other node, the POC was connecting to a NodeJS proxy application using websockets,\\nwhich in turn could connect to wakunode2 via TCP.\\n\\nHowever, to enable dApp and Wallet developers to easily integrate Waku in their product,\\nwe need to give them a library that is easy to use and works out of the box:\\nintroducing [JS-Waku](https://github.com/status-im/js-waku).\\n\\nJS-Waku is a JavaScript library that allows your dApp, wallet or other web app to interact with the Waku v2 network.\\nIt is available right now on [npm](https://www.npmjs.com/package/js-waku):\\n\\n`npm install js-waku`.\\n\\nAs it is written in TypeScript, types are included in the npm package to allow easy integration with TypeScript, ClojureScript and other typed languages that compile to JavaScript.\\n\\nKey Waku v2 protocols are already available:\\n[message](https://rfc.vac.dev/waku/standards/core/14/message), [store](https://rfc.vac.dev/waku/standards/core/13/store), [relay](https://rfc.vac.dev/waku/standards/core/11/relay) and [light push](https://rfc.vac.dev/waku/standards/core/19/lightpush),\\nenabling your dApp to:\\n\\n- Send and receive near-instant messages on the Waku network (relay),\\n- Query nodes for messages that may have been missed, e.g. due to poor cellular network (store),\\n- Send messages with confirmations (light push).\\n\\nJS-Waku needs to operate in the same context from which Waku v2 was born:\\na restricted environment were connectivity or uptime are not guaranteed;\\nJS-Waku brings Waku v2 to the browser.\\n\\n## Achievements so far\\n\\nWe focused the past month on developing a [ReactJS Chat App](https://status-im.github.io/js-waku/).\\nThe aim was to create enough building blocks in JS-Waku to enable this showcase web app that\\nwe now [use for dogfooding](https://github.com/status-im/nim-waku/issues/399) purposes.\\n\\nMost of the effort was on getting familiar with the [js-libp2p](https://github.com/libp2p/js-libp2p) library\\nthat we heavily rely on.\\nJS-Waku is the second implementation of Waku v2 protocol,\\nso a lot of effort on interoperability was needed.\\nFor example, to ensure compatibility with the nim-waku reference implementation,\\nwe run our [tests against wakunode2](https://github.com/status-im/js-waku/blob/90c90dea11dfd1277f530cf5d683fb92992fe141/src/lib/waku_relay/index.spec.ts#L137) as part of the CI.\\n\\nThis interoperability effort helped solidify the current Waku v2 specifications:\\nBy clarifying the usage of topics\\n([#327](https://github.com/vacp2p/rfc/issues/327), [#383](https://github.com/vacp2p/rfc/pull/383)),\\nfix discrepancies between specs and nim-waku\\n([#418](https://github.com/status-im/nim-waku/issues/418), [#419](https://github.com/status-im/nim-waku/issues/419))\\nand fix small nim-waku & nim-libp2p bugs\\n([#411](https://github.com/status-im/nim-waku/issues/411), [#439](https://github.com/status-im/nim-waku/issues/439)).\\n\\nTo fully access the waku network, JS-Waku needs to enable web apps to connect to nim-waku nodes.\\nA standard way to do so is using secure websockets as it is not possible to connect directly to a TCP port from the browser.\\nUnfortunately websocket support is not yet available in [nim-libp2p](https://github.com/status-im/nim-libp2p/issues/407) so\\nwe ended up deploying [websockify](https://github.com/novnc/websockify) alongside wakunode2 instances.\\n\\nAs we built the [web chat app](https://github.com/status-im/js-waku/tree/main/examples/web-chat),\\nwe were able to fine tune the API to provide a simple and succinct interface.\\nYou can start a node, connect to other nodes and send a message in less than ten lines of code:\\n\\n```javascript\\nimport { Waku } from \'js-waku\'\\n\\nconst waku = await Waku.create({})\\n\\nconst nodes = await getStatusFleetNodes()\\nawait Promise.all(nodes.map((addr) => waku.dial(addr)))\\n\\nconst msg = WakuMessage.fromUtf8String(\\n  \'Here is a message!\',\\n  \'/my-cool-app/1/my-use-case/proto\',\\n)\\nawait waku.relay.send(msg)\\n```\\n\\nWe have also put a bounty at [0xHack](https://0xhack.dev/) for using JS-Waku\\nand running a [workshop](https://www.youtube.com/watch?v=l77j0VX75QE).\\nWe were thrilled to have a couple of hackers create new software using our libraries.\\nOne of the projects aimed to create a decentralised, end-to-end encrypted messenger app,\\nsimilar to what the [ETH-DM](https://rfc.vac.dev/waku/standards/application/20/toy-eth-pm) protocol aims to achieve.\\nAnother project was a decentralised Twitter platform.\\nSuch projects allow us to prioritize the work on JS-Waku and understand how DevEx can be improved.\\n\\nAs more developers use JS-Waku, we will evolve the API to allow for more custom and fine-tune usage of the network\\nwhile preserving this out of the box experience.\\n\\n## What\'s next?\\n\\nNext, we are directing our attention towards [Developer Experience](https://github.com/status-im/js-waku/issues/68).\\nWe already have [documentation](https://www.npmjs.com/package/js-waku) available but we want to provide more:\\n[Tutorials](https://github.com/status-im/js-waku/issues/56), various examples\\nand showing how [JS-Waku can be used with Web3](https://github.com/status-im/js-waku/issues/72).\\n\\nBy prioritizing DevEx we aim to enable JS-Waku integration in dApps and wallets.\\nWe think JS-Waku builds a strong case for machine-to-machine (M2M) communications.\\nThe first use cases we are looking into are dApp notifications:\\nEnabling dApp to notify their user directly in their wallets!\\nLeveraging Waku as a decentralised infrastructure and standard so that users do not have to open their dApp to be notified\\nof events such as DAO voting.\\n\\nWe already have some POC in the pipeline to enable voting and polling on the Waku network,\\nallowing users to save gas by **not** broadcasting each individual vote on the blockchain.\\n\\nTo facilitate said applications, we are looking at improving integration with Web3 providers by providing examples\\nof signing, validating, encrypting and decrypting messages using Web3.\\nWaku is privacy conscious, so we will also provide signature and encryption examples decoupled from users\' Ethereum identity.\\n\\nAs you can read, we have grand plans for JS-Waku and Waku v2.\\nThere is a lot to do, and we would love some help so feel free to\\ncheck out the new role in our team:\\n[js-waku: Wallet & Dapp Integration Developer](https://status.im/our_team/jobs.html?gh_jid=3157894).\\nWe also have a number of [positions](https://status.im/our_team/jobs.html) open to work on Waku protocol and nim-waku.\\n\\nIf you are as excited as us by JS-Waku, why not build a dApp with it?\\nYou can find documentation on the [npmjs page](https://www.npmjs.com/package/js-waku).\\n\\nWhether you are a developer, you can come chat with us using [WakuJS Web Chat](https://status-im.github.io/js-waku/)\\nor [chat2](https://github.com/status-im/nim-waku/blob/master/docs/tutorial/chat2.md).\\nYou can get support in #dappconnect-support on [Vac Discord](https://discord.gg/j5pGbn7MHZ) or [Telegram](https://t.me/dappconnectsupport).\\nIf you have any ideas on how Waku could enable a specific dapp or use case, do share, we are always keen to hear it."},{"id":"rln-relay","metadata":{"permalink":"/rlog/rln-relay","source":"@site/rlog/2021-03-03-rln-relay.mdx","title":"Privacy-preserving p2p economic spam protection in Waku v2","description":"This post is going to give you an overview of how spam protection can be achieved in Waku Relay through rate-limiting nullifiers. We will cover a summary of spam-protection methods in centralized and p2p systems, and the solution overview and details of the economic spam-protection method. The open issues and future steps are discussed in the end.","date":"2021-03-05T12:00:00.000Z","formattedDate":"March 5, 2021","tags":[],"readingTime":20.775,"hasTruncateMarker":true,"authors":[{"name":"Sanaz","twitter":"sanaz2016","github":"staheri14","key":"sanaz"}],"frontMatter":{"layout":"post","name":"Privacy-preserving p2p economic spam protection in Waku v2","title":"Privacy-preserving p2p economic spam protection in Waku v2","date":"2021-03-05T12:00:00.000Z","authors":"sanaz","published":true,"slug":"rln-relay","categories":"reserach","image":"/img/rain.png","discuss":"https://forum.vac.dev/t/privacy-preserving-p2p-economic-spam-protection-in-waku-v2-with-rate-limiting-nullfiers/66","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Presenting JS-Waku: Waku v2 in the Browser","permalink":"/rlog/presenting-js-waku"},"nextItem":{"title":"[Talk] Vac, Waku v2 and Ethereum Messaging","permalink":"/rlog/waku-v2-ethereum-messaging"}},"content":"This post is going to give you an overview of how spam protection can be achieved in Waku Relay through rate-limiting nullifiers. We will cover a summary of spam-protection methods in centralized and p2p systems, and the solution overview and details of the economic spam-protection method. The open issues and future steps are discussed in the end.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nThis post is going to give you an overview of how spam protection can be achieved in Waku Relay protocol[^2] through Rate-Limiting Nullifiers[^3] [^4] or RLN for short.\\n\\nLet me give a little background about Waku(v2)[^1]. Waku is a privacy-preserving peer-to-peer (p2p) messaging protocol for resource-restricted devices. Being p2p means that Waku relies on **No** central server. Instead, peers collaboratively deliver messages in the network. Waku uses GossipSub[^16] as the underlying routing protocol (as of the writeup of this post). At a high level, GossipSub is based on publisher-subscriber architecture. That is, _peers, congregate around topics they are interested in and can send messages to topics. Each message gets delivered to all peers subscribed to the topic_. In GossipSub, a peer has a constant number of direct connections/neighbors. In order to publish a message, the author forwards its message to a subset of neighbors. The neighbors proceed similarly till the message gets propagated in the network of the subscribed peers. The message publishing and routing procedures are part of the Waku Relay[^17] protocol.\\n![Figure 1: An overview of privacy-preserving p2p economic spam protection in Waku v2 RLN-Relay protocol.](/img/rln-relay/rln-relay-overview.png)\\n\\n## What do we mean by spamming?\\n\\nIn centralized messaging systems, a spammer usually indicates an entity that uses the messaging system to send an unsolicited message (spam) to large numbers of recipients. However, in Waku with a p2p architecture, spam messages not only affect the recipients but also all the other peers involved in the routing process as they have to spend their computational power/bandwidth/storage capacity on processing spam messages. As such, we define a spammer as an entity that uses the messaging system to publish a large number of messages in a short amount of time. The messages issued in this way are called spam. In this definition, we disregard the intention of the spammer as well as the content of the message and the number of recipients.\\n\\n## Possible Solutions\\n\\nHas the spamming issue been addressed before? Of course yes! Here is an overview of the spam protection techniques with their trade-offs and use-cases. In this overview, we distinguish between protection techniques that are targeted for centralized messaging systems and those for p2p architectures.\\n\\n### Centralized Messaging Systems\\n\\nIn traditional centralized messaging systems, spam usually signifies unsolicited messages sent in bulk or messages with malicious content like malware. Protection mechanisms include\\n\\n- authentication through some piece of personally identifiable information e.g., phone number\\n- checksum-based filtering to protect against messages sent in bulk\\n- challenge-response systems\\n- content filtering on the server or via a proxy application\\n\\nThese methods exploit the fact that the messaging system is centralized and a global view of the users\' activities is available based on which spamming patterns can be extracted and defeated accordingly. Moreover, users are associated with an identifier e.g., a username which enables the server to profile each user e.g., to detect suspicious behavior like spamming. Such profiling possibility is against the user\'s anonymity and privacy.\\n\\nAmong the techniques enumerated above, authentication through phone numbers is a some-what economic-incentive measure as providing multiple valid phone numbers will be expensive for the attacker. Notice that while using an expensive authentication method can reduce the number of accounts owned by a single spammer, cannot address the spam issue entirely. This is because the spammer can still send bulk messages through one single account. For this approach to be effective, a centralized mediator is essential. That is why such a solution would not fit the p2p environments where no centralized control exists.\\n\\n### P2P Systems\\n\\nWhat about spam prevention in p2p messaging platforms? There are two techniques, namely _Proof of Work_[^8] deployed by Whisper[^9] and _Peer scoring_[^6] method (namely reputation-based approach) adopted by LibP2P. However, each of these solutions has its own shortcomings for real-life use-cases as explained below.\\n\\n#### Proof of work\\n\\nThe idea behind the Proof Of Work i.e., POW[^8] is to make messaging a computationally costly operation hence lowering the messaging rate of **all** the peers including the spammers. In specific, the message publisher has to solve a puzzle and the puzzle is to find a nonce such that the hash of the message concatenated with the nonce has at least z leading zeros. z is known as the difficulty of the puzzle. Since the hash function is one-way, peers have to brute-force to find a nonce. Hashing is a computationally-heavy operation so is the brute-force. While solving the puzzle is computationally expensive, it is comparatively cheap to verify the solution.\\n\\nPOW is also used as the underlying mining algorithm in Ethereum and Bitcoin blockchain. There, the goal is to contain the mining speed and allow the decentralized network to come to a consensus, or agree on things like account balances and the order of transactions.\\n\\nWhile the use of POW makes perfect sense in Ethereum / Bitcoin blockchain, it shows practical issues in heterogeneous p2p messaging systems with resource-restricted peers. Some peers won\'t be able to carry the designated computation and will be effectively excluded. Such exclusion showed to be practically an issue in applications like Status, which used to rely on POW for spam-protection, to the extent that the difficulty level had to be set close to zero.\\n\\n#### Peer Scoring\\n\\nThe peer scoring method[^6] that is utilized by libp2p is to limit the number of messages issued by a peer in connection to another peer. That is each peer monitors all the peers to which it is directly connected and adjusts their messaging quota i.e., to route or not route their messages depending on their past activities. For example, if a peer detects its neighbor is sending more than x messages per month, can drop its quota to z.x where z is less than one. The shortcoming of this solution is that scoring is based on peers\' local observations and the concept of the score is defined in relation to one single peer. This leaves room for an attack where a spammer can make connections to k peers in the system and publishes k.(x-1) messages by exploiting all of its k connections. Another attack scenario is through botnets consisting of a large number of e.g., a million bots. The attacker rents a botnet and inserts each of them as a legitimate peer to the network and each can publish x-1 messages per month[^7].\\n\\n#### Economic-Incentive Spam protection\\n\\nIs this the end of our spam-protection journey? Shall we simply give up and leave spammers be? Certainly not!\\nWaku RLN-Relay gives us a p2p spam-protection method which:\\n\\n- suits **p2p** systems and does not rely on any central entity.\\n- is **efficient** i.e., with no unreasonable computational, storage, memory, and bandwidth requirement! as such, it fits the network of **heterogeneous** peers.\\n- respects users **privacy** unlike reputation-based and centralized methods.\\n- deploys **economic-incentives** to contain spammers\' activity. Namely, there is a financial sacrifice for those who want to spam the system. How? follow along ...\\n\\nWe devise a general rule to save everyone\'s life and that is\\n\\n**No one can publish more than M messages per epoch without being financially charged!**\\n\\nWe set M to 1 for now, but this can be any arbitrary value. You may be thinking \\"This is too restrictive! Only one per epoch?\\". Don\'t worry, we set the epoch to a reasonable value so that it does not slow down the communication of innocent users but will make the life of spammers harder! Epoch here can be every second, as defined by UTC date-time +-20s.\\n\\nThe remainder of this post is all about the story of how to enforce this limit on each user\'s messaging rate as well as how to impose the financial cost when the limit gets violated. This brings us to the Rate Limiting Nullifiers and how we integrate this technique into Waku v2 (in specific the Waku Relay protocol) to protect our valuable users against spammers.\\n\\n## Technical Terms\\n\\n**Zero-knowledge proof**: Zero-knowledge proof (ZKP)[^14] allows a _prover_ to show a _verifier_ that they know something, without revealing what that something is. This means you can do the trust-minimized computation that is also privacy-preserving. As a basic example, instead of showing your ID when going to a bar you simply give them proof that you are over 18, without showing the doorman your id. In this write-up, by ZKP we essentially mean zkSNARK[^15] which is one of the many types of ZKPs.\\n\\n**Threshold Secret Sharing Scheme**: (m,n) Threshold secret-sharing is a method by which you can split a secret value s into n pieces in a way that the secret s can be reconstructed by having m pieces (m <= n). The economic-incentive spam protection utilizes a (2,n) secret sharing realized by Shamir Secret Sharing Scheme[^13].\\n\\n## Overview: Economic-Incentive Spam protection through Rate Limiting Nullifiers\\n\\n**Context**: We started the idea of economic-incentive spam protection more than a year ago and conducted a feasibility study to identify blockers and unknowns. The results are published in our prior [post](https://vac.dev/feasibility-semaphore-rate-limiting-zksnarks). Since then major progress has been made and the prior identified blockers that are listed below are now addressed. Kudos to [Barry WhiteHat](https://github.com/barryWhiteHat), [Onur Kilic](https://github.com/kilic), [Koh Wei Jie](https://github.com/weijiekoh/perpetualpowersoftau) for all of their hard work, research, and development which made this progress possible.\\n\\n- the proof time[^22] which was initially in the order of minutes ~10 mins and now is almost 0.5 seconds\\n- the prover key size[^21] which was initially ~110MB and now is ~3.9MB\\n- the lack of Shamir logic[^19] which is now implemented and part of the RLN repository[^4]\\n- the concern regarding the potential multi-party computation for the trusted setup of zkSNARKs which got resolved[^20]\\n- the lack of end-to-end integration that now we made it possible, have it implemented, and are going to present it in this post. New blockers are also sorted out during the e2e integration which we will discuss in the [Feasibility and Open Issues](#feasibility-and-open-issues) section.\\n\\nNow that you have more context, let\'s see how the final solution works. The fundamental point is to make it economically costly to send more than your share of messages and to do so in a privacy-preserving and e2e fashion. To do that we have the following components:\\n\\n- 1- **Group**: We manage all the peers inside a large group (later we can split peers into smaller groups, but for now consider only one). The group management is done via a smart contract which is devised for this purpose and is deployed on the Ethereum blockchain.\\n- 2- **Membership**: To be able to send messages and in specific for the published messages to get routed by all the peers, publishing peers have to register to the group. Membership involves setting up public and private key pairs (think of it as the username and password). The private key remains at the user side but the public key becomes a part of the group information on the contract (publicly available) and everyone has access to it. Public keys are not human-generated (like usernames) and instead they are random numbers, as such, they do not reveal any information about the owner (think of public keys as pseudonyms). Registration is mandatory for the users who want to publish a message, however, users who only want to listen to the messages are more than welcome and do not have to register in the group.\\n- **Membership fee**: Membership is not for free! each peer has to lock a certain amount of funds during the registration (this means peers have to have an Ethereum account with sufficient balance for this sake). This fund is safely stored on the contract and remains intact unless the peer attempts to break the rules and publish more than one message per epoch.\\n- **Zero-knowledge Proof of membership**: Do you want your message to get routed to its destination, fine, but you have to prove that you are a member of the group (sorry, no one can escape the registration phase!). Now, you may be thinking that should I attach my public key to my message to prove my membership? Absolutely Not! we said that our solution respects privacy! membership proofs are done in a zero-knowledge manner that is each message will carry cryptographic proof asserting that \\"the message is generated by one of the current members of the group\\", so your identity remains private and your anonymity is preserved!\\n- **Slashing through secret sharing**: Till now it does not seem like we can catch spammers, right? yes, you are right! now comes the exciting part, detecting spammers and slashing them. The core idea behind the slashing is that each publishing peer (not routing peers!) has to integrate a secret share of its private key inside the message. The secret share is deterministically computed over the private key and the current epoch. The content of this share is harmless for the peer\'s privacy (it looks random) unless the peer attempts to publish more than one message in the same epoch hence disclosing more than one secret share of its private key. Indeed two distinct shares of the private key under the same epoch are enough to reconstruct the entire private key. Then what should you do with the recovered private key? hurry up! go to the contract and withdraw the private key and claim its fund and get rich!! Are you thinking what if spammers attach junk values instead of valid secret shares? Of course, that wouldn\'t be cool! so, there is a zero-knowledge proof for this sake as well where the publishing peer has to prove that the secret shares are generated correctly.\\n\\nA high-level overview of the economic spam protection is shown in Figure 1.\\n\\n## Flow\\n\\nIn this section, we describe the flow of the economic-incentive spam detection mechanism from the viewpoint of a single peer. An overview of this flow is provided in Figure 3.\\n\\n## Setup and Registration\\n\\nA peer willing to publish a message is required to register. Registration is moderated through a smart contract deployed on the Ethereum blockchain. The state of the contract contains the list of registered members\' public keys. An overview of registration is illustrated in Figure 2.\\n\\nFor the registration, a peer creates a transaction that sends x amount of Ether to the contract. The peer who has the \\"private key\\" `sk` associated with that deposit would be able to withdraw x Ether by providing valid proof. Note that `sk` is initially only known by the owning peer however it may get exposed to other peers in case the owner attempts spamming the system i.e., sending more than one message per epoch.\\nThe following relation holds between the `sk` and `pk` i.e., `pk = H(sk)` where `H` denotes a hash function.\\n![Figure 2: Registration](/img/rln-relay/rln-relay.png)\\n\\n## Maintaining the membership Merkle Tree\\n\\nThe ZKP of membership that we mentioned before relies on the representation of the entire group as a [Merkle Tree](/#). The tree construction and maintenance is delegated to the peers (the initial idea was to keep the tree on the chain as part of the contract, however, the cost associated with member deletion and insertion was high and unreasonable, please see [Feasibility and Open Issues](#Feasibility-and-Open-Issues) for more details). As such, each peer needs to build the tree locally and sync itself with the contract updates (peer insertion and deletion) to mirror them on its tree.\\nTwo pieces of information of the tree are important as they enable peers to generate zero-knowledge proofs. One is the root of the tree and the other is the membership proof (or the authentication path). The tree root is public information whereas the membership proof is private data (or more precisely the index of the peer in the tree).\\n\\n## Publishing\\n\\nIn order to publish at a given epoch, each message must carry a proof i.e., a zero-knowledge proof signifying that the publishing peer is a registered member, and has not exceeded the messaging rate at the given epoch.\\n\\nRecall that the enforcement of the messaging rate was through associating a secret shared version of the peer\'s `sk` into the message together with a ZKP that the secret shares are constructed correctly. As for the secret sharing part, the peer generates the following data:\\n\\n1. `shareX`\\n2. `shareY`\\n3. `nullifier`\\n\\nThe pair (`shareX`, `shareY`) is the secret shared version of `sk` that are generated using Shamir secret sharing scheme. Having two such pairs for an identical `nullifier` results in full disclosure of peer\'s `sk` and hence burning the associated deposit. Note that the `nullifier` is a deterministic value derived from `sk` and `epoch` therefore any two messages issued by the same peer (i.e., using the same `sk`) for the same `epoch` are guaranteed to have identical `nullifier`s.\\n\\nFinally, the peer generates a zero-knowledge proof `zkProof` asserting the membership of the peer in the group and the correctness of the attached secret share (`shareX`, `shareY`) and the `nullifier`. In order to generate a valid proof, the peer needs to have two private inputs i.e., its `sk` and its authentication path. Other inputs are the tree root, epoch, and the content of the message.\\n\\n**Privacy Hint:** Note that the authentication path of each peer depends on the recent list of members (hence changes when new peers register or leave). As such, it is recommended (and necessary for privacy/anonymity) that the publisher updates her authentication path based on the latest status of the group and attempts the proof using the updated version.\\n\\nAn overview of the publishing procedure is provided in Figure 3.\\n\\n## Routing\\n\\nUpon the receipt of a message, the routing peer needs to decide whether to route it or not. This decision relies on the following factors:\\n\\n1. If the epoch value attached to the message has a non-reasonable gap with the routing peer\'s current epoch then the message must be dropped (this is to prevent a newly registered peer spamming the system by messaging for all the past epochs).\\n2. The message MUST contain valid proof that gets verified by the routing peer.\\n   If the preceding checks are passed successfully, then the message is relayed. In case of an invalid proof, the message is dropped. If spamming is detected, the publishing peer gets slashed (see [Spam Detection and Slashing](#Spam-Detection-and-Slashing)).\\n\\nAn overview of the routing procedure is provided in Figure 3.\\n\\n### Spam Detection and Slashing\\n\\nIn order to enable local spam detection and slashing, routing peers MUST record the `nullifier`, `shareX`, and `shareY` of any incoming message conditioned that it is not spam and has valid proof. To do so, the peer should follow the following steps.\\n\\n1. The routing peer first verifies the `zkProof` and drops the message if not verified.\\n2. Otherwise, it checks whether a message with an identical `nullifier` has already been relayed.\\n   - a) If such message exists and its `shareX` and `shareY` components are different from the incoming message, then slashing takes place (if the `shareX` and `shareY` fields of the previously relayed message is identical to the incoming message, then the message is a duplicate and shall be dropped).\\n   - b) If none found, then the message gets relayed.\\n\\nAn overview of the slashing procedure is provided in Figure 3.\\n![Figure 3: Publishing, Routing and Slashing workflow.](/img/rln-relay/rln-message-verification.png)\\n\\n## Feasibility and Open Issues\\n\\nWe\'ve come a long way since a year ago, blockers resolved, now we have implemented it end-to-end. We learned lot and could identify further issues and unknowns some of which are blocking getting to production. The summary of the identified issues are presented below.\\n\\n## Storage overhead per peer\\n\\nCurrently, peers are supposed to maintain the entire tree locally and it imposes storage overhead which is linear in the size of the group (see this [issue](https://github.com/vacp2p/research/issues/57)[^11] for more details). One way to cope with this is to use the light-node and full-node paradigm in which only a subset of peers who are more resourceful retain the tree whereas the light nodes obtain the necessary information by interacting with the full nodes. Another way to approach this problem is through a more storage efficient method (as described in this research issue[^12]) where peers store a partial view of the tree instead of the entire tree. Keeping the partial view lowers the storage complexity to O(log(N)) where N is the size of the group. There are still unknown unknowns to this solution, as such, it must be studied further to become fully functional.\\n\\n## Cost-effective way of member insertion and deletion\\n\\nCurrently, the cost associated with RLN-Relay membership is around 30 USD[^10]. We aim at finding a more cost-effective approach. Please feel free to share with us your solution ideas in this regard in this [issue](https://github.com/vacp2p/research/issues/56).\\n\\n## Exceeding the messaging rate via multiple registrations\\n\\nWhile the economic-incentive solution has an economic incentive to discourage spamming, we should note that there is still **expensive attack(s)**[^23] that a spammer can launch to break the messaging rate limit. That is, the attacker can pay for multiple legit registrations e.g., k, hence being able to publish k messages per epoch. We believe that the higher the membership fee is, the less probable would be such an attack, hence a stronger level of spam-protection can be achieved. Following this argument, the high fee associated with the membership (which we listed above as an open problem) can indeed be contributing to a better protection level.\\n\\n## Conclusion and Future Steps\\n\\nAs discussed in this post, Waku RLN Relay can achieve a privacy-preserving economic spam protection through rate-limiting nullifiers. The idea is to financially discourage peers from publishing more than one message per epoch. In specific, exceeding the messaging rate results in a financial charge. Those who violate this rule are called spammers and their messages are spam. The identification of spammers does not rely on any central entity. Also, the financial punishment of spammers is cryptographically guaranteed.\\nIn this solution, privacy is guaranteed since: 1) Peers do not have to disclose any piece of personally identifiable information in any phase i.e., neither in the registration nor in the messaging phase 2) Peers can prove that they have not exceeded the messaging rate in a zero-knowledge manner and without leaving any trace to their membership accounts.\\nFurthermore, all the computations are light hence this solution fits the heterogenous p2p messaging system. Note that the zero-knowledge proof parts are handled through zkSNARKs and the benchmarking result can be found in the RLN benchmark report[^5].\\n\\n**Future steps**:\\n\\nWe are still at the PoC level, and the development is in progress. As our future steps,\\n\\n- we would like to evaluate the running time associated with the Merkle tree operations. Indeed, the need to locally store Merkle tree on each peer was one of the unknowns discovered during this PoC and yet the concrete benchmarking result in this regard is not available.\\n- We would also like to pursue our storage-efficient Merkle Tree maintenance solution in order to lower the storage overhead of peers.\\n- In line with the storage optimization, the full-node light-node structure is another path to follow.\\n- Another possible improvement is to replace the membership contract with a distributed group management scheme e.g., through distributed hash tables. This is to address possible performance issues that the interaction with the Ethereum blockchain may cause. For example, the registration transactions are subject to delay as they have to be mined before being visible in the state of the membership contract. This means peers have to wait for some time before being able to publish any message.\\n\\n## Acknowledgement\\n\\nThanks to Onur K\u0131l\u0131\xe7 for his explanation and pointers and for assisting with development and runtime issues. Also thanks to Barry Whitehat for his time and insightful comments. Special thanks to Oskar Thoren for his constructive comments and his guides during the development of this PoC and the writeup of this post.\\n\\n## References\\n\\n[^1]: Waku v2: https://rfc.vac.dev/waku/standards/core/10/waku2\\n[^2]: RLN-Relay specification: https://rfc.vac.dev/waku/standards/core/17/rln-relay\\n[^3]: RLN documentation: [https://hackmd.io/tMTLMYmTR5eynw2lwK9n1w?both](https://hackmd.io/tMTLMYmTR5eynw2lwK9n1w?both)\\n[^4]: RLN repositories: [https://github.com/kilic/RLN](https://github.com/kilic/RLN) and [https://github.com/kilic/rlnapp](https://github.com/kilic/rlnapp)\\n[^5]: RLN Benchmark: [https://hackmd.io/tMTLMYmTR5eynw2lwK9n1w?view#Benchmarks](https://hackmd.io/tMTLMYmTR5eynw2lwK9n1w?view#Benchmarks)\\n[^6]: Peer Scoring: [https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.1.md#peer-scoring](https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.1.md#peer-scoring)\\n[^7]: Peer scoring security issues: [https://github.com/vacp2p/research/issues/44](https://github.com/vacp2p/research/issues/44)\\n[^8]: Proof of work: [http://www.infosecon.net/workshop/downloads/2004/pdf/clayton.pdf](http://www.infosecon.net/workshop/downloads/2004/pdf/clayton.pdf) and [https://link.springer.com/content/pdf/10.1007/3-540-48071-4_10.pdf](https://link.springer.com/content/pdf/10.1007/3-540-48071-4_10.pdf)\\n[^9]: EIP-627 Whisper: https://eips.ethereum.org/EIPS/eip-627\\n[^10]: Cost-effective way of member insertion and deletion: [https://github.com/vacp2p/research/issues/56](https://github.com/vacp2p/research/issues/56)\\n[^11]: Storage overhead per peer: [https://github.com/vacp2p/research/issues/57](https://github.com/vacp2p/research/issues/57)\\n[^12]: Storage-efficient Merkle Tree maintenance: [https://github.com/vacp2p/research/pull/54](https://github.com/vacp2p/research/pull/54)\\n[^13]: Shamir Secret Sharing Scheme: [https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing](https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing)\\n[^14]: Zero Knowledge Proof: [https://dl.acm.org/doi/abs/10.1145/3335741.3335750](https://dl.acm.org/doi/abs/10.1145/3335741.3335750) and [https://en.wikipedia.org/wiki/Zero-knowledge_proof](https://en.wikipedia.org/wiki/Zero-knowledge_proof)\\n[^15]: zkSNARKs: [https://link.springer.com/chapter/10.1007/978-3-662-49896-5_11](https://link.springer.com/chapter/10.1007/978-3-662-49896-5_11) and [https://coinpare.io/whitepaper/zcash.pdf](https://coinpare.io/whitepaper/zcash.pdf)\\n[^16]: GossipSub: [https://docs.libp2p.io/concepts/publish-subscribe/](https://docs.libp2p.io/concepts/publish-subscribe/)\\n[^17]: Waku Relay: https://rfc.vac.dev/waku/standards/core/11/relay\\n[^18]: Prior blockers of RLN-Relay: [https://vac.dev/feasibility-semaphore-rate-limiting-zksnarks](https://vac.dev/feasibility-semaphore-rate-limiting-zksnarks)\\n[^19]: The lack of Shamir secret sharing in zkSNARKs: [https://github.com/vacp2p/research/issues/10](https://github.com/vacp2p/research/issues/10)\\n[^20]: The MPC required for zkSNARKs trusted setup: [https://github.com/vacp2p/research/issues/9](https://github.com/vacp2p/research/issues/9)\\n[^21]: Prover key size: [https://github.com/vacp2p/research/issues/8](https://github.com/vacp2p/research/issues/8)\\n[^22]: zkSNARKs proof time: [https://github.com/vacp2p/research/issues/7](https://github.com/vacp2p/research/issues/7)\\n[^23]: Attack on the messaging rate: [https://github.com/vacp2p/specs/issues/251](https://github.com/vacp2p/specs/issues/251)"},{"id":"waku-v2-ethereum-messaging","metadata":{"permalink":"/rlog/waku-v2-ethereum-messaging","source":"@site/rlog/2020-11-10-waku-v2-ethereum-messaging.mdx","title":"[Talk] Vac, Waku v2 and Ethereum Messaging","description":"Talk from Taipei Ethereum Meetup. Read on to find out about our journey from Whisper to Waku v2, as well as how Waku v2 can be useful for Etherum Messaging.","date":"2020-11-10T12:00:00.000Z","formattedDate":"November 10, 2020","tags":[],"readingTime":9.51,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"[Talk] Vac, Waku v2 and Ethereum Messaging","title":"[Talk] Vac, Waku v2 and Ethereum Messaging","date":"2020-11-10T12:00:00.000Z","authors":"oskarth","published":true,"slug":"waku-v2-ethereum-messaging","categories":"research","image":"/img/taipei_ethereum_meetup_slide.png","discuss":"https://forum.vac.dev/t/discussion-talk-vac-waku-v2-and-ethereum-messaging/60"},"prevItem":{"title":"Privacy-preserving p2p economic spam protection in Waku v2","permalink":"/rlog/rln-relay"},"nextItem":{"title":"Waku v2 Update","permalink":"/rlog/waku-v2-update"}},"content":"Talk from Taipei Ethereum Meetup. Read on to find out about our journey from Whisper to Waku v2, as well as how Waku v2 can be useful for Etherum Messaging.\\n\\n\x3c!--truncate--\x3e\\n\\n_The following post is a transcript of the talk given at the [Taipei Ethereum meetup, November 5](https://www.meetup.com/Taipei-Ethereum-Meetup/events/274033344/). There is also a [video recording](https://www.youtube.com/watch?v=lUDy1MoeYnI)._\\n\\n---\\n\\n## 0. Introduction\\n\\nHi! My name is Oskar and I\'m the protocol research lead at Vac. This talk will be divided into two parts. First I\'ll talk about the journey from Whisper, to Waku v1 and now to Waku v2. Then I\'ll talk about messaging in Ethereum. After this talk, you should have an idea of what Waku v2 is, the problems it is trying to solve, as well as where it can be useful for messaging in Ethereum.\\n\\n## PART 1 - VAC AND THE JOURNEY FROM WHISPER TO WAKU V1 TO WAKU V2\\n\\n## 1. Vac intro\\n\\nFirst, what is Vac? Vac grew out of our efforts Status to create a window on to Ethereum and secure messenger. Vac is modular protocol stack for p2p secure messaging, paying special attention to resource restricted devices, privacy and censorship resistance.\\n\\nToday we are going to talk mainly about Waku v2, which is the transport privacy / routing aspect of the Vac protocol stack. It sits \\"above\\" the p2p overlay, such as libp2p dealing with transports etc, and below a conversational security layer dealing with messaging encryption, such as using Double Ratchet etc.\\n\\n## 2. Whisper to Waku v1\\n\\nIn the beginning, there was Whisper. Whisper was part of the holy trinity of Ethereum. You had Ethereum for consensus/computation, Whisper for messaging, and Swarm for storage.\\n\\nHowever, for various reasons, Whisper didn\'t get the attention it deserved. Development dwindled, it promised too much and it suffered from many issues, such as being extremely inefficient and not being suitable for running on e.g. mobile phone. Despite this, Status used it in its app from around 2017 to 2019. As far as I know, it was one of very few, if not the only, production uses of Whisper.\\n\\nIn an effort to solve some of its immediate problems, we forked Whisper into Waku and formalized it with a proper specification. This solved immediate bandwidth issues for light nodes, introduced rate limiting for better spam protection, improved historical message support, etc.\\n\\nIf you are interested in this journey, checkout the [EthCC talk Dean and I gave in Paris earlier this year](https://www.youtube.com/watch?v=6lLT33tsJjs).\\n\\nStatus upgraded to Waku v1 early 2020. What next?\\n\\n## 3. Waku v1 to v2\\n\\nWe were far from done. The changes we had made were quite incremental and done in order to get tangible improvements as quickly as possible. This meant we couldn\'t address more fundamental issues related to full node routing scalability, running with libp2p for more transports, better security, better spam protection and incentivization.\\n\\nThis kickstarted Waku v2 efforts, which is what we\'ve been working on since July. This work was and is initally centered around a few pieces:\\n\\n(a) Moving to libp2p\\n\\n(b) Better routing\\n\\n(c) Accounting and user-run nodes\\n\\nThe general theme was: making the Waku network more scalable and robust.\\n\\nWe also did a scalability study to show at what point the network would run into issues, due to the inherent lack of routing that Whisper and Waku v1 provided.\\n\\nYou can read more about this [here](https://vac.dev/waku-v2-plan).\\n\\n## 3.5 Waku v2 - Design goals\\n\\nTaking a step back, what problem does Waku v2 attempt to solve compared to all the other solutions that exists out there? What type of applications should use it and why? We have the following design goals:\\n\\n1. **Generalized messaging**. Many applications requires some form of messaging protocol to communicate between different subsystems or different nodes. This messaging can be human-to-human or machine-to-machine or a mix.\\n\\n2. **Peer-to-peer**. These applications sometimes have requirements that make them suitable for peer-to-peer solutions.\\n\\n3. **Resource restricted**. These applications often run in constrained environments, where resources or the environment is restricted in some fashion. E.g.:\\n\\n   - limited bandwidth, CPU, memory, disk, battery, etc\\n   - not being publicly connectable\\n   - only being intermittently connected; mostly-offline\\n\\n4. **Privacy**. These applications have a desire for some privacy guarantees, such as pseudonymity, metadata protection in transit, etc.\\n\\nAs well as to do so in a modular fashion. Meaning you can find a reasonable trade-off depending on your exact requirements. For example, you usually have to trade off some bandwidth to get metadata protection, and vice versa.\\n\\nThe concept of designing for resource restricted devices also leads to the concept of adaptive nodes, where you have more of a continuum between full nodes and light nodes. For example, if you switch your phone from mobile data to WiFi you might be able to handle more bandwidth, and so on.\\n\\n## 4. Waku v2 - Breakdown\\n\\nWhere is Waku v2 at now, and how is it structured?\\n\\nIt is running over libp2p and we had our second internal testnet last week or so. As a side note, we name our testnets after subway stations in Taipei, the first one being Nangang, and the most recent one being Dingpu.\\n\\nThe main implementation is written in Nim using nim-libp2p, which is also powering Nimbus, an Ethereum 2 client. There is also a PoC for running Waku v2 in the browser. On a spec level, we have the following specifications that corresponds to the components that make up Waku v2:\\n\\n- Waku v2 - this is the main spec that explains the goals of providing generalized messaging, in a p2p context, with a focus on privacy and running on resources restricted devices.\\n- Relay - this is the main PubSub spec that provides better routing. It builds on top of GossipSub, which is what Eth2 heavily relies on as well.\\n- Store - this is a 1-1 protocol for light nodes to get historical messages, if they are mostly-offline.\\n- Filter - this is a 1-1 protocol for light nodes that are bandwidth restricted to only (or mostly) get messages they care about.\\n- Message - this explains the payload, to get some basic encryption and content topics. It corresponds roughly to envelopes in Whisper/Waku v1.\\n- Bridge - this explains how to do bridging between Waku v1 and Waku v2 for compatibility.\\n\\nRight now, all protocols, with the exception of bridge, are in draft mode, meaning they have been implemented but are not yet being relied upon in production.\\n\\nYou can read more about the breakdown in this [update](https://vac.dev/waku-v2-update) though some progress has been made since then, as well was in the [main Waku v2 spec](https://rfc.vac.dev/waku/standards/core/10/waku2).\\n\\n## 5. Waku v2 - Upcoming\\n\\nWhat\'s coming up next? There are a few things.\\n\\nFor Status to use it in production, it needs to be integrated into the main app using the Nim Node API. The bridge also needs to be implemented and tested.\\n\\nFor other users, we are currently overhauling the API to allow usage from a browser, e.g. To make this experience great, there are also a few underlying infrastructure things that we need in nim-libp2p, such as a more secure HTTP server in Nim, Websockets and WebRTC support.\\n\\nThere are also some changes we made to at what level content encryption happens, and this needs to be made easier to use in the API. This means you can use a node without giving your keys to it, which is useful in some environments.\\n\\nMore generally, beyond getting to production-ready use, there are a few bigger pieces that we are working on or will work on soon. These are things like:\\n\\n- Better scaling, by using topic sharding.\\n- Accounting and user-run nodes, to account for and incentives full nodes.\\n- Stronger and more rigorous privacy guarantees, e.g. through study of GossipSub, unlinkable packet formats, etc.\\n- Rate Limit Nullifier for privacy preserving spam protection, a la what Barry Whitehat has presented before.\\n\\nAs well as better support for Ethereum M2M Messaging. Which is what I\'ll talk about next.\\n\\n## PART 2 - ETHEREUM MESSAGING\\n\\nA lot of what follows is inspired by exploratory work that John Lea has done at Status, previously Head of UX Architecture at Ubuntu.\\n\\n## 6. Ethereum Messaging - Why?\\n\\nIt is easy to think that Waku v2 is only for human to human messaging, since that\'s how Waku is currently primarily used in the Status app. However, the goal is to be useful for generalized messaging, which includes other type of information as well as machine to machine messaging.\\n\\nWhat is Ethereum M2M messaging? Going back to the Holy Trinity of Ethereum/Whisper/Swarm, the messaging component was seen as something that could facilitate messages between dapps and acts as a building block. This can help with things such as:\\n\\n- Reducing on-chain transactions\\n- Reduce latency for operations\\n- Decentralize centrally coordinated services (like WalletConnect)\\n- Improve UX of dapps\\n- Broadcast live information\\n- A message transport layer for state channels\\n\\nAnd so on.\\n\\n## 7. Ethereum Messaging - Why? (Cont)\\n\\nWhat are some examples of practical things Waku as used for Ethereum Messaging could solve?\\n\\n- Multisig transfers only needing one on chain transaction\\n- DAO votes only needing one one chain transaction\\n- Giving dapps ability to direct push notifications to users\\n- Giving users ability to directly respond to requests from daps\\n- Decentralized Wallet Connect\\n\\nEtc.\\n\\n## 8. What\'s needed to deliver this?\\n\\nWe can break it down into our actors:\\n\\n- Decentralized M2M messaging system (Waku)\\n- Native wallets (Argent, Metamask, Status, etc)\\n- Dapps that benefit from M2M messaging\\n- Users whose problems are being solved\\n\\nEach of these has a bunch of requirements in turn. The messaging system needs to be decentralized, scalable, robust, etc. Wallets need support for messaging layer, dapps need to integrate this, etc.\\n\\nThis is a lot! Growing adoption is a challenge. There is a catch 22 in terms of justifying development efforts for wallets, when no dapps need it, and likewise for dapps when no wallets support Waku. In addition to this, there must be proven usage of Waku before it can be relied on, etc. How can we break this up into smaller pieces of work?\\n\\n## 9. Breaking up the problem and a high level roadmap\\n\\nWe can start small. It doesn\'t and need to be used for critical features first. A more hybrid approach can be taken where it acts more as nice-to-haves.\\n\\n1.  Forking Whisper and solving scalablity, spam etc issues with it.\\n    This is a work in progress. What we talked about in part 1.\\n2.  Expose messaging API for Dapp developers.\\n3.  Implement decentralized version of WalletConnect.\\n    Currently wallets connect ot dapps with centralized service. Great UX.\\n4.  Solve DAO/Multi-Sig coordination problem.\\n    E.g. send message to wallet-derived key when it is time to sign a transaction.\\n5.  Extend dapp-to-user and user-to-dapp communication to more dapps.\\n    Use lessons learned and examples to drive adoptation for wallets/dapps.\\n\\nAnd then build up from there.\\n\\n## 10. We are hiring!\\n\\nA lot of this will happen in Javascript and browsers, since that\'s the primarily environment for a lot of wallets and dapps. We are currently hiring for a Waku JS Wallet integration lead to help push this effort further.\\n\\nCome talk to me after or [apply here](https://status.im/our_team/open_positions.html?gh_jid=2385338).\\n\\nThat\'s it! You can find us on Status, Telegram, vac.dev. I\'m on twitter [here](https://twitter.com/oskarth).\\n\\nQuestions?\\n\\n---"},{"id":"waku-v2-update","metadata":{"permalink":"/rlog/waku-v2-update","source":"@site/rlog/2020-09-28-waku-v2-update.mdx","title":"Waku v2 Update","description":"A research log. Read on to find out what is going on with Waku v2, a messaging protocol. What has been happening? What is coming up next?","date":"2020-09-28T12:00:00.000Z","formattedDate":"September 28, 2020","tags":[],"readingTime":7.435,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"Waku v2 Update","title":"Waku v2 Update","date":"2020-09-28T12:00:00.000Z","authors":"oskarth","published":true,"slug":"waku-v2-update","categories":"research","discuss":"https://forum.vac.dev/t/discussion-waku-v2-update/56"},"prevItem":{"title":"[Talk] Vac, Waku v2 and Ethereum Messaging","permalink":"/rlog/waku-v2-ethereum-messaging"},"nextItem":{"title":"What\'s the Plan for Waku v2?","permalink":"/rlog/waku-v2-plan"}},"content":"A research log. Read on to find out what is going on with Waku v2, a messaging protocol. What has been happening? What is coming up next?\\n\\n\x3c!--truncate--\x3e\\n\\nIt has been a while since the last post. It is time for an update on Waku v2. Aside from getting more familiar with libp2p (specifically nim-libp2p) and some vacation, what have we been up to? In this post we\'ll talk about what we\'ve gotten done since last time, and briefly talk about immediate next steps and future. But first, a recap.\\n\\n## Recap\\n\\nIn the last post ([Waku v2 plan](https://vac.dev/waku-v2-plan)) we explained the rationale of Waku v2 - the current Waku network is fragile and doesn\'t scale. To solve this, Waku v2 aims to reduce amplification factors and get more user run nodes. We broke the work down into three separate tracks.\\n\\n1. Track 1 - Move to libp2p\\n2. Track 2 - Better routing\\n3. Track 3 - Accounting and user-run nodes\\n\\nAs well as various rough components for each track. The primary initial focus is track 1. This means things like: moving to FloodSub, simplify the protocol, core integration, topic interest behavior, historical message caching, and Waku v1<\\\\>v2 bridge.\\n\\n## Current state\\n\\nLet\'s talk about the state of specs and our main implementation nim-waku. Then we\'ll go over our recent testnet, Nangang, and finish off with a Web PoC.\\n\\n## Specs\\n\\nAfter some back and forth on how to best structure things, we ended up breaking down the specs into a few pieces. While Waku v2 is best thought of as a cohesive whole in terms of its capabilities, it is made up of several protocols. Here\'s a list of the current specs and their status:\\n\\n- [Main spec](https://rfc.vac.dev/waku/standards/core/10/waku2) (draft)\\n- [Relay protocol spec](https://rfc.vac.dev/waku/standards/core/11/relay) (draft)\\n- [Filter protocol spec](https://rfc.vac.dev/waku/standards/core/12/filter) (raw)\\n- [Store protocol spec](https://rfc.vac.dev/waku/standards/core/13/store) (raw)\\n- [Bridge spec](https://rfc.vac.dev/waku/standards/core/15/bridge) (raw)\\n\\nRaw means there is not yet an implementation that corresponds fully to the spec, and draft means there is an implementation that corresponds to the spec. In the interest of space, we won\'t go into too much detail on the specs here except to note a few things:\\n\\n- The relay spec is essentially a thin wrapper on top of PubSub/FloodSub/GossipSub\\n- The filter protocol corresponds to previous light client mode in Waku v1\\n- The store protocol corresponds to the previous mailserver construct in Waku v1\\n\\nThe filter and store protocol allow for adaptive nodes, i.e. nodes that have various capabilities. For example, a node being mostly offline, or having limited bandwidth capacity. The bridge spec outlines how to bridge the Waku v1 and v2 networks.\\n\\n## Implementation\\n\\nThe main implementation we are working on is [nim-waku](https://github.com/status-im/nim-waku/). This builds on top of libraries such as [nim-libp2p](https://github.com/status-im/nim-libp2p) and others that the [Nimbus team](https://nimbus.team/) have been working on as part of their Ethereum 2.0 client.\\n\\nCurrently nim-waku implements the relay protocol, and is close to implementing filter and store protocol. It also exposes a [Nim Node API](https://github.com/status-im/nim-waku/blob/master/docs/api/v2/node.md) that allows libraries such as [nim-status](https://github.com/status-im/status-nim) to use it. Additionally, there is also a rudimentary JSON RPC API for command line scripting.\\n\\n## Nangang testnet\\n\\nLast week we launched a very rudimentary internal testnet called Nangang. The goal was to test basic connectivity and make sure things work end to end. It didn\'t have things like: client integration, encryption, bridging, multiple clients, store/filter protocol, or even a real interface. What it did do is allow Waku developers to \\"chat\\" via RPC calls and looking in the log output. Doing this meant we exposed and fixed a few blockers, such as connection issues, deployment, topic subscription management, protocol and node integration, and basic scripting/API usage. After this, we felt confident enough to upgrade the main and relay spec to \\"draft\\" status.\\n\\n## Waku Web PoC\\n\\nAs a bonus, we wanted to see what it\'d take to get Waku running in a browser. This is a very powerful capability that enables a lot of use cases, and something that libp2p enables with its multiple transport support.\\n\\nUsing the current stack, with nim-waku, would require quite a lot of ground work with WASM, WebRTC, Websockets support etc. Instead, we decided to take a shortcut and hack together a JS implementation called [Waku Web Chat](https://github.com/vacp2p/waku-web-chat/). This quick hack wouldn\'t be possible without the people behind [js-libp2p-examples](https://github.com/libp2p/js-libp2p-examples/) and [js-libp2p](https://github.com/libp2p/js-libp2p) and all its libraries. These are people like Jacob Heun, Vasco Santos, and Cayman Nava. Thanks!\\n\\nIt consists of a brower implementation, a NodeJS implementation and a bootstrap server that acts as a signaling server for WebRTC. It is largely a bastardized version of GossipSub, and while it isn\'t completely to spec, it does allow messages originating from a browser to eventually end up at a nim-waku node, and vice versa. Which is pretty cool.\\n\\n## Coming up\\n\\nNow that we know what the current state is, what is still missing? what are the next steps?\\n\\n## Things that are missing\\n\\nWhile we are getting closer to closing out work for track 1, there are still a few things missing from the initial scope:\\n\\n1. Store and filter protocols need to be finished. This means basic spec, implementation, API integration and proven to work in a testnet. All of these are work in progress and expected to be done very soon. Once the store protocol is done in a basic form, it needs further improvements to make it production ready, at least on a spec/basic implementation level.\\n\\n2. Core integration was mentioned in scope for track 1 initially. This work has stalled a bit, largely due to organizational bandwidth and priorities. While there is a Nim Node API that in theory is ready to be used, having it be used in e.g. Status desktop or mobile app is a different matter. The team responsible for this at Status ([status-nim](https://github.com/status-im/status-nim) has been making progress on getting nim-waku v1 integrated, and is expected to look into nim-waku v2 integration soon. One thing that makes this a especially tricky is the difference in interface between Waku v1 and v2, which brings\\n   us too...\\n\\n3. Companion spec for encryption. As part of simplifying the protocol, the routing is decoupled from the encryption in v2 ([1](https://github.com/vacp2p/specs/issues/158), [2](https://github.com/vacp2p/specs/issues/181)). There are multiple layers of encryption at play here, and we need to figure out a design that makes sense for various use cases (dapps using Waku on their own, Status app, etc).\\n\\n4. Bridge implementation. The spec is done and we know how it should work, but it needs to be implemented.\\n\\n5. General tightening up of specs and implementation.\\n\\nWhile this might seem like a lot, a lot has been done already, and the majority of the remaining tasks are more amendable to be pursued in parallel with other efforts. It is also worth mentioning that part of track 2 and 3 have been started, in the form of moving to GossipSub (amplification factors) and basics of adaptive nodes (multiple protocols). This is in addition to things like Waku Web which were not part of the initial scope.\\n\\n## Upcoming\\n\\nAside from the things mentioned above, what is coming up next? There are a few areas of interest, mentioned in no particular order. For track 2 and 3, see previous post for more details.\\n\\n1. Better routing (track 2). While we are already building on top of GossipSub, we still need to explore things like topic sharding in more detail to further reduce amplification factors.\\n\\n2. Accounting and user-run nodes (track 3). With store and filter protocol getting ready, we can start to implement accounting and light connection game for incentivization in a bottom up and iterative manner.\\n\\n3. Privacy research. Study better and more rigorous privacy guarantees. E.g. how FloodSub/GossipSub behaves for common threat models, and how custom packet\\n   format can improve things like unlinkability.\\n\\n4. zkSnarks RLN for spam protection and incentivization. We studied this [last year](https://vac.dev/feasibility-semaphore-rate-limiting-zksnarks) and recent developments have made this relevant to study again. Create an [experimental spec/PoC](https://github.com/vacp2p/specs/issues/189) as an extension to the relay protocol. Kudos to Barry Whitehat and others like Kobi Gurkan and Koh Wei Jie for pushing this!\\n\\n5. Ethereum M2M messaging. Being able to run in the browser opens up a lot of doors, and there is an opportunity here to enable things like a decentralized WalletConnect, multi-sig transactions, voting and similar use cases. This was the original goal of Whisper, and we\'d like to deliver on that.\\n\\nAs you can tell, quite a lot of thing! Luckily, we have two people joining as protocol engineers soon, which will bring much needed support for the current team of ~2-2.5 people. More details to come in further updates.\\n\\n---\\n\\nIf you are feeling adventurous and want to use early stage alpha software, check out the [docs](https://github.com/status-im/nim-waku/tree/master/docs). If you want to read the specs, head over to [Waku spec](https://rfc.vac.dev/spec/10/). If you want to talk with us, join us on [Status](https://get.status.im/chat/public/vac) or on [Telegram](https://t.me/vacp2p) (they are bridged)."},{"id":"waku-v2-plan","metadata":{"permalink":"/rlog/waku-v2-plan","source":"@site/rlog/2020-07-01-waku-v2-pitch.mdx","title":"What\'s the Plan for Waku v2?","description":"Read about our plans for Waku v2, moving to libp2p, better routing, adaptive nodes and accounting!","date":"2020-07-01T12:00:00.000Z","formattedDate":"July 1, 2020","tags":[],"readingTime":13.7,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"What\'s the Plan for Waku v2?","title":"What\'s the Plan for Waku v2?","date":"2020-07-01T12:00:00.000Z","authors":"oskarth","published":true,"slug":"waku-v2-plan","categories":"research","image":"/img/status_scaling_model_fig4.png","discuss":"https://forum.vac.dev/t/waku-version-2-pitch/52"},"prevItem":{"title":"Waku v2 Update","permalink":"/rlog/waku-v2-update"},"nextItem":{"title":"Feasibility Study: Discv5","permalink":"/rlog/feasibility-discv5"}},"content":"Read about our plans for Waku v2, moving to libp2p, better routing, adaptive nodes and accounting!\\n\\n\x3c!--truncate--\x3e\\n\\n**tldr: The Waku network is fragile and doesn\'t scale. Here\'s how to solve it.**\\n\\n_NOTE: This post was originally written with Status as a primary use case in mind, which reflects how we talk about some problems here. However, Waku v2 is a general-purpose private p2p messaging protocol, especially for people running in resource restricted environments._\\n\\n## Problem\\n\\nThe Waku network is fragile and doesn\'t scale.\\n\\nAs [Status](https://status.im) is moving into a user-acquisition phase and is improving retention rates for users they need the infrastructure to keep up, specifically when it comes to messaging.\\n\\nBased on user acquisition models, the initial goal is to support 100k DAU in September, with demand growing from there.\\n\\nWith the Status Scaling Model we have studied the current bottlenecks as a function of concurrent users (CCU) and daily active users (DAU). Here are the conclusions.\\n\\n\\\\***\\\\*1. Connection limits\\\\*\\\\***. With 100 full nodes we reach ~10k CCU based on connection limits. This can primarily be addressed by increasing the number of nodes (cluster or user operated). This assumes node discovery works. It is also worth investigating the limitations of max number of connections, though this is likely to be less relevant for user-operated nodes. For a user-operated network, this means 1% of users have to run a full node. See Fig 1-2.\\n\\n\\\\***\\\\*2. Bandwidth as a bottleneck\\\\*\\\\***. We notice that memory usage appears to not be\\nthe primary bottleneck for full nodes, and the bottleneck is still bandwidth. To support 10k DAU, and full nodes with an amplification factor of 25 the required Internet speed is ~50 Mbps, which is a fast home Internet connection. For ~100k DAU only cloud-operated nodes can keep up (500 Mbps). See Fig 3-5.\\n\\n\\\\***\\\\*3. Amplification factors\\\\*\\\\***. Reducing amplification factors with better routing, would have a high impact, but it is likely we\'d need additional measures as well, such as topic sharding or similar. See Fig 8-13.\\n\\nFigure 1-5:\\n\\n![](/img/status_scaling_model_fig1.png)\\n![](/img/status_scaling_model_fig2.png)\\n![](/img/status_scaling_model_fig3.png)\\n![](/img/status_scaling_model_fig4.png)\\n![](/img/status_scaling_model_fig5.png)\\n\\nSee <https://colab.research.google.com/drive/1Fz-oxRxxAFPpM1Cowpnb0nT52V1-yeRu#scrollTo=Yc3417FUJJ_0> for the full report.\\n\\nWhat we need to do is:\\n\\n1.  Reduce amplification factors\\n2.  Get more user-run full nodes\\n\\nDoing this means the Waku network will be able to scale, and doing so in the right way, in a robust fashion. What would a fragile way of scaling be? Increasing our reliance on a Status Pte Ltd operated cluster which would paint us in a corner where we:\\n\\n- keep increasing requirements for Internet speed for full nodes\\n- are vulnerable to censorship and attacks\\n- have to control the topology in an artifical manner to keep up with load\\n- basically re-invent a traditional centralized client-server app with extra steps\\n- deliberately ignore most of our principles\\n- risk the network being shut down when we run out of cash\\n\\n## Appetite\\n\\nOur initial risk appetite for this is 6 weeks for a small team.\\n\\nThe idea is that we want to make tangible progress towards the goal in a limited period of time, as opposed to getting bogged down in trying to find a theoretically perfect generalized solution. Fixed time, variable scope.\\n\\nIt is likely some elements of a complete solution will be done separately. See later sections for that.\\n\\n## Solution\\n\\nThere are two main parts of the solution. One is to reduce amplification factors, and the other is incentivization to get more user run full nodes with desktop, etc.\\n\\nWhat does a full node provide? It provides connectivity to the network, can act as a bandwidth \\"barrier\\" and be high or reasonably high availability. What this means right now is essentially topic interest and storing historical messages.\\n\\nThe goal is here to improve the status quo, not get a perfect solution from the get go. All of this can be iterated on further, for stronger guarantees, as well as replaced by other new modules.\\n\\nLet\'s first look at the baseline, and then go into some of the tracks and their phases. Track 1 is best done first, after which track 2 and 3 can be executed in parallel. Track 1 gives us more options for track 2 and 3. The work in track 1 is currently more well-defined, so it is likely the specifics of track 2 and 3 will get refined at a later stage.\\n\\n## Baseline\\n\\nHere\'s where we are at now. In reality, the amplification factor are likely even worse than this (15 in the graph below), up to 20-30. Especially with an open network, where we can\'t easily control connectivity and availability of nodes. Left unchecked, with a full mesh, it could even go as high x100, though this is likely excessive and can be dialed down. See scaling model for more details.\\n\\n![](/img/waku_v1_routing_small.png)\\n\\n## Track 1 - Move to libp2p\\n\\nMoving to PubSub over libp2p wouldn\'t improve amplification per se, but it would be stepping stone. Why? It paves the way for GossipSub, and would be a checkpoint on this journey. Additionally, FloodSub and GossipSub are compatible, and very likely other future forms of PubSub such as GossipSub 1.1 (hardened/more secure), EpiSub, forwarding Kademlia / PubSub over Kademlia, etc. Not to mention security This would also give us access to the larger libp2p ecosystem (multiple protocols, better encryption, quic, running in the browser, security audits, etc, etc), as well as be a joint piece of infrastructured used for Eth2 in Nimbus. More wood behind fewer arrows.\\n\\nSee more on libp2p PubSub here: <https://docs.libp2p.io/concepts/publish-subscribe/>\\n\\nAs part of this move, there are a few individual pieces that are needed.\\n\\n### 1. FloodSub\\n\\nThis is essentially what Waku over libp2p would look like in its most basic form.\\n\\nOne difference that is worth noting is that the app topics would **not** be the same as Waku topics. Why? In Waku we currently don\'t use topics for routing between full nodes, but only for edge/light nodes in the form of topic interest. In FloodSub, these topics are used for routing.\\n\\nWhy can\'t we use Waku topics for routing directly? PubSub over libp2p isn\'t built for rare and ephemeral topics, and nodes have to explicitly subscribe to a topic. See topic sharding section for more on this.\\n\\n![](/img/waku_v2_routing_flood_small.png)\\n\\nMoving to FloodSub over libp2p would also be an opportunity to clean up and simplify some components that are no longer needed in the Waku v1 protocol, see point below.\\n\\nVery experimental and incomplete libp2p support can be found in the nim-waku repo under v2: <https://github.com/status-im/nim-waku>\\n\\n### 2. Simplify the protocol\\n\\nDue to Waku\'s origins in Whisper, devp2p and as a standalone protocol, there are a lot of stuff that has accumulated (<https://rfc.vac.dev/waku/standards/legacy/6/waku1/>). Not all of it serves it purpose anymore. For example, do we still need RLP here when we have Protobuf messages? What about extremely low PoW when we have peer scoring? What about key management / encryption when have encryption at libp2p and Status protocol level?\\n\\nNot everything has to be done in one go, but being minimalist at this stage will the protocol lean and make us more adaptable.\\n\\nThe essential characteristic that has to be maintained is that we don\'t need to change the upper layers, i.e. we still deal with (Waku) topics and some envelope like data unit.\\n\\n### 3. Core integration\\n\\nAs early as possible we want to integrate with Core via Stimbus in order to mitigate risk and catch integration issues early in the process. What this looks like in practice is some set of APIs, similar to how Whisper and Waku were working in parallel, and experimental feature behind a toggle in core/desktop.\\n\\n### 4. Topic interest behavior\\n\\nWhile we target full node traffic here, we want to make sure we maintain the existing bandwidth requirements for light nodes that Waku v1 addressed (<https://vac.dev/fixing-whisper-with-waku>). This means implementing topic-interest in the form of Waku topics. Note that this would be separate from app topics notes above.\\n\\n### 5. Historical message caching\\n\\nBasically what mailservers are currently doing. This likely looks slightly different in a libp2p world. This is another opportunity to simplify things with a basic REQ-RESP architecture, as opposed to the roundabout way things are now. Again, not everything has to be done in one go but there\'s no reason to reimplement a poor API if we don\'t have to.\\n\\nAlso see section below on adaptive nodes and capabilities.\\n\\n### 6. Waku v1 <\\\\> Libp2p bridge\\n\\nTo make the transition complete, there has to a be bridge mode between current Waku and libp2p. This is similar to what was done for Whisper and Waku, and allows any nodes in the network to upgrade to Waku v2 at their leisure. For example, this would likely look different for Core, Desktop, Research and developers.\\n\\n## Track 2 - Better routing\\n\\nThis is where we improve the amplification factors.\\n\\n### 1. GossipSub\\n\\nThis is a subprotocol of FloodSub in the libp2p world. Moving to GossipSub would allow traffic between full nodes to go from an amplification factor of ~25 to ~6. This basically creates a mesh of stable bidirectional connections, together with some gossiping capabilities outside of this view.\\n\\nExplaining how GossipSub works is out of scope of this document. It is implemented in nim-libp2p and used by Nimbus as part of Eth2. You can read the specs here in more detail if you are interested: <https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.0.md> and <https://github.com/libp2p/specs/blob/master/pubsub/gossipsub/gossipsub-v1.1.md>\\n\\n![](/img/waku_v2_routing_gossip_small.png)\\n\\n![](/img/status_scaling_model_fig8.png)\\n![](/img/status_scaling_model_fig9.png)\\n![](/img/status_scaling_model_fig10.png)\\n![](/img/status_scaling_model_fig11.png)\\n\\nWhile we technically could implement this over existing Waku, we\'d have to re-implement it, and we\'d lose out on all the other benefits libp2p would provide, as well as the ecosystem of people and projects working on improving the scalability and security of these protocols.\\n\\n### 2. Topic sharding\\n\\nThis one is slightly more speculative in terms of its ultimate impact. The basic idea is to split the application topic into N shards, say 10, and then each full node can choose which shards to listen to. This can reduce amplification factors by another factor of 10.\\n\\n![](/img/waku_v2_routing_sharding_small.png)\\n\\n![](/img/status_scaling_model_fig12.png)\\n![](/img/status_scaling_model_fig13.png)\\n\\nNote that this means a light node that listens to several topics would have to be connected to more full nodes to get connectivity. For a more exotic version of this, see <https://forum.vac.dev/t/rfc-topic-propagation-extension-to-libp2p-pubsub/47>\\n\\nThis is orthogonal from the choice of FloodSub or GossipSub, but due to GossipSub\'s more dynamic nature it is likely best combined with it.\\n\\n### 3. Other factors\\n\\nNot a primary focus, but worth a look. Looking at the scaling model, there might be other easy wins to improve overall bandwidth consumption between full nodes. For example, can we reduce envelope size by a significant factor?\\n\\n## Track 3 - Accounting and user-run nodes\\n\\nThis is where we make sure the network isn\'t fragile, become a true p2p app, get our users excited and engaged, and allow us to scale the network without creating an even bigger cluster.\\n\\nTo work in practice, this has a soft dependency on node discovery such as DNS based discovery (<https://eips.ethereum.org/EIPS/eip-1459>) or Discovery v5 (<https://vac.dev/feasibility-discv5>).\\n\\n### 1. Adaptive nodes and capabilities\\n\\nWe want to make the gradation between light nodes, full nodes, storing (partial set of) historical messages, only acting for a specific shard, etc more flexible and explicit. This is required to identify and discover the nodes you want. See <https://github.com/vacp2p/specs/issues/87>\\n\\nDepending on how the other tracks come together, this design should allow for a desktop node to identify as a full relaying node for some some app topic shard, but also express waku topic interest and retrieve historical messages itself.\\n\\nE.g. Disc v5 can be used to supply node properties through ENR.\\n\\n### 2. Accounting\\n\\nThis is based on a few principles:\\n\\n1.  Some nodes contribute a lot more than other nodes in the network\\n2.  We can account for the difference in contribution in some fashion\\n3.  We want to incentivize nodes to tell the true, and be incentivized not to lie\\n\\nAccounting here is a stepping stone, where accounting is the raw data upon which some settlement later occurs. It can have various forms of granularity. See <https://forum.vac.dev/t/accounting-for-resources-in-waku-and-beyond/31> for discussion.\\n\\nWe also note that in GossipSub, the mesh is bidrectional. Additionally, it doesn\'t appears to be a high priority issue in terms of nodes misreporting. What is an issue is having people run full nodes in the first place. There are a few points to that. It has to be possible in the end-user UX, nodes have to be discovered, and it has to be profitable/visible that you are contributing. UX and discovery are out of scope for this work, whereas visibility/accounting is part of this scope. Settlement is a stretch goal here.\\n\\nThe general shape of the solution is inspired by the Swarm model, where we do accounting separate from settlement. It doesn\'t require any specific proofs, but nodes are incentivized to tell the truth in the following way:\\n\\n1.  Both full node and light node do accounting in a pairwise, local fashion\\n2.  If a light node doesn\'t ultimately pay or lie about reporting, they get disconnected (e.g.)\\n3.  If a full node doesn\'t provide its service the light node may pick another full node (e.g.)\\n\\nWhile accounting for individual resource usage is useful, for the ultimate end user experience we can ideally account for other things such as:\\n\\n- end to end delivery\\n- online time\\n- completeness of storage\\n\\nThis can be gradually enhanced and strengthened, for example with proofs, consistency checks, Quality of Service, reputation systems. See <https://discuss.status.im/t/network-incentivisation-first-draft/1037> for one attempt to provide stronger guarantees with periodic consistency checks and a shared fund mechanism. And <https://forum.vac.dev/t/incentivized-messaging-using-validity-proofs/51> for using validity proofs and removing liveness requirement for settlement.\\n\\nAll of this is optional at this stage, because our goal here is to improve the status quo for user run nodes. Accounting at this stage should be visible and correspond to the net benefit a node provides to another.\\n\\nAs a concrete example: a light node has some topic interest and cares about historical messages on some topic. A full node communicates envelopes as they come in, communicates their high availability (online time) and stores/forward stored messages. Both nodes have this information, and if they agree settlement (initially just a mock message) can be sending a payment to an address at some time interval / over some defined volume. See future sections for how this can be improved upon.\\n\\nAlso see below in section 4, using constructs such as eigentrust as a local reputation mechanism.\\n\\n### 3. Relax high availability requirement\\n\\nIf we want desktop nodes to participate in the storing of historical messages, high availability is a problem. It is a problem for any node, especially if they lie about it, but assuming they are honest it is still an issue.\\n\\nBy being connected to multiple nodes, we can get an overlapping online window. Then these can be combined together to get consistency. This is obviously experimental and would need to be tested before being deployed, but if it works it\'d be very useful.\\n\\nAdditionally or alternatively, instead of putting a high requirement on message availability, focus on detection of missing information. This likely requires re-thinking how we do data sync / replication.\\n\\n### 4. Incentivize light and full nodes to tell the truth (policy, etc)\\n\\nIn accounting phase it is largely assumed nodes are honest. What happens when they lie, and how do we incentivize them to be honest? In the case of Bittorrent this is done with tit-for-tat, however this is a different kind of relationship. What follows are some examples of how this can be done.\\n\\nFor light nodes:\\n\\n- if they don\'t, they get disconnected\\n- prepayment (especially to \\"high value\\" nodes)\\n\\nFor full nodes:\\n\\n- multiple nodes reporting to agree, where truth becomes a shelling point\\n- use eigentrust\\n- staking for discovery visibility with slashing\\n\\n### 5. Settlement PoC\\n\\nCan be done after phase 2 if so desired. Basically integrate payments based on accounting and policy.\\n\\n## Out of scope\\n\\n1.  We assume the Status Base model requirements are accurate.\\n2.  We assume Core will improve retention rates.\\n3.  We assume the Stimbus production team will enable integration of nim-waku.\\n4.  We assume Discovery mechanisms such as DNS and Discovery v5 will be worked on separately.\\n5.  We assume Core will, at some point, provide an UX for integrating payment of services.\\n6.  We assume the desktop client is sufficiently usable.\\n7.  We assume Core and Infra will investigate ways of improving MaxPeers."},{"id":"feasibility-discv5","metadata":{"permalink":"/rlog/feasibility-discv5","source":"@site/rlog/2020-04-27-feasibility-discv5.mdx","title":"Feasibility Study: Discv5","description":"Looking at discv5 and the theoretical numbers behind finding peers.","date":"2020-04-27T12:00:00.000Z","formattedDate":"April 27, 2020","tags":[],"readingTime":5.655,"hasTruncateMarker":true,"authors":[{"name":"Dean","twitter":"DeanEigenmann","github":"decanus","website":"https://dean.eigenmann.me","key":"dean"}],"frontMatter":{"layout":"post","name":"Feasibility Study: Discv5","title":"Feasibility Study: Discv5","date":"2020-04-27T12:00:00.000Z","authors":"dean","published":true,"slug":"feasibility-discv5","categories":"research","discuss":"https://discuss.status.im/t/discv5-feasibility-study/1632"},"prevItem":{"title":"What\'s the Plan for Waku v2?","permalink":"/rlog/waku-v2-plan"},"nextItem":{"title":"What Would a WeChat Replacement Need?","permalink":"/rlog/wechat-replacement-need"}},"content":"Looking at discv5 and the theoretical numbers behind finding peers.\\n\\n\x3c!--truncate--\x3e\\n\\n> Disclaimer: some of the numbers found in this write-up could be inaccurate. They are based on the current understanding of theoretical parts of the protocol itself by the author and are meant to provide a rough overview rather than bindable numbers.\\n\\nThis post serves as a more authoritative overview of the discv5 study, for a discussionary post providing more context make sure to check out the corresponding [discuss post](https://discuss.status.im/t/discv5-feasibility-study/1632). Additionally, if you are unfamiliar with discv5, check out my previous write-up: [\\"From Kademlia to Discv5\\"](https://vac.dev/kademlia-to-discv5).\\n\\n## Motivating Problem\\n\\nThe discovery method currently used by [Status](https://status.im), is made up of various components and grew over time to solve a mix of problems. We want to simplify this while maintaining some of the properties we currently have.\\n\\nNamely, we want to ensure censorship resistance to state-level adversaries. One of the issues Status had which caused us them add to their discovery method was the fact that addresses from providers like AWS and GCP were blocked both in Russia and China. Additionally, one of the main factors required is the ability to function on resource restricted devices.\\n\\nConsidering we are talking about resource restricted devices, let\'s look at the implications and what we need to consider:\\n\\n- **Battery consumption** - constant connections like websockets consume a lot of battery life.\\n- **CPU usage** - certain discovery methods may be CPU incentive, slowing an app down and making it unusable.\\n- **Bandwidth consumption** - a lot of users will be using data plans, the discovery method needs to be efficient in order to accommodate those users without using up significant portions of their data plans.\\n- **Short connection windows** - the discovery algorithm needs to be low latency, that means it needs to return results fast. This is because many users will only have the app open for a short amount of time.\\n- **Not publicly connectable** - There is a good chance that most resource restricted devices are not publicly connectable.\\n\\nFor a node to be able to participate as both a provider, and a consumer in the discovery method. Meaning a node both reads from other nodes\' stored DHTs and hosts the DHT for other nodes to read from, it needs to be publically connectable. This means another node must be able to connect to some public IP of the given node.\\n\\nWith devices that are behind a NAT, this is easier said than done. Especially mobile devices, that when connected to 4G LTE networks are often stuck behind a symmetric NAT, drastically reducing the the succeess rate of NAT traversal. Keeping this in mind, it becomes obvious that most resource restricted devices will be consumers rather than providers due to this technical limitation.\\n\\nIn order to answer our questions, we formulated the problem with a simple method for testing. The \\"needle in a haystack\\" problem was formulated to figure out how easily a specific node can be found within a given network. This issue was fully formulated in [vacp2p/research#15](https://github.com/vacp2p/research/issues/15).\\n\\n## Overview\\n\\nThe main things we wanted to investigate was the overhead on finding a peer. This means we wanted to look at both the bandwidth, latency and effectiveness of this. There are 2 methods which we can use to find a peer:\\n\\n- We can find a peer with a specific ID, using normal lookup methods as documented by Kademlia.\\n- We can find a peer that advertises a capability, this is possible using either capabilities advertised in the ENR or through [topic tables](https://github.com/ethereum/devp2p/blob/master/discv5/discv5-theory.md#topic-advertisement).\\n\\n## Feasbility\\n\\nTo be able to investigate the feasibility of discv5, we used various methods including rough calculations which can be found in the [notebook](https://vac.dev/discv5-notebook/), and a simulation isolated in [vacp2p/research#19](https://github.com/vacp2p/research/pull/19).\\n\\n### CPU & Memory Usage\\n\\nThe experimental discv5 has already been used within Status, however what was noticed was that the CPU and memory usage was rather high. It therefore should be investiaged if this is still the case, and if it is, it should be isolated where this stems from. Additionally it is worth looking at whether or not this is the case with both the go and nim implementation.\\n\\nSee details: [vacp2p/research#31](https://github.com/vacp2p/research/issues/31)\\n\\n### NAT on Cellular Data\\n\\nIf a peer is not publically connectable it can not participate in the DHT both ways. A lot of mobile phones are behind symmetric NATs which UDP hole-punching close to impossible. It should be investigated whether or not mobile phones will be able to participate both ways and if there are good methods for doing hole-punching.\\n\\nSee details: [vacp2p/research#29](https://github.com/vacp2p/research/issues/29)\\n\\n### Topic Tables\\n\\nTopic Tables allow us the ability to efficiently find nodes given a specific topic. However, they are not implemented in the [status-im/nim-eth](https://github.com/status-im/nim-eth/) implementation nor are they fully finalized in the spec. These are important if the network grows past a size where the concentration of specific nodes is relatively low making them hard to find.\\n\\nSee details: [vacp2p/research#26](https://github.com/vacp2p/research/issues/26)\\n\\n### Finding a node\\n\\nIt is important to note, that given a network is relatively small sized, eg 100-500 nodes, then finding a node given a specific address is relatively managable. Additionally, if the concentration of a specific capability in a network is reasonable, then finding a node advertising its capabilities using an ENR rather than the topic table is also managable. A reasonable concentration for example would be 10%, which would give us an 80% chance of getting a node with that capability in the first lookup request. This can be explored more using our [discv5 notebook](https://vac.dev/discv5-notebook/#Needle-in-a-haystack-with-ENR-records-indicating-capabilities).\\n\\n## Results\\n\\nResearch has shown that finding a node in the DHT has a relatively low effect on bandwidth, both inbound and outbound. For example when trying to find a node in a network of 100 nodes, it would take roughly 5668 bytes total. Additionally if we assume 100ms latency per request it would range at \u2248 300ms latency, translating to 3 requests to find a specific node.\\n\\n## General Thoughts\\n\\nOne of the main blockers right now is figuring out what the CPU and memory usage of discv5 is on mobile phones, this is a large blocker as it affects one of the core problems for us. We need to consider whether discv5 is an upgrade as it allows us to simplify our current discovery process or if it is too much of an overhead for resource restricted devices. The topic table feature could largely enhance discovery however it is not yet implemented. Given that CPU and memory isn\'t too high, discv5 could probably be used as the other issues are more \\"features\\" than large scale issues. Implementing it would already reduce the ability for state level adversaries to censor our nodes.\\n\\n## Acknowledgements\\n\\n- Oskar Thoren\\n- Dmitry Shmatko\\n- Kim De Mey\\n- Corey Petty"},{"id":"wechat-replacement-need","metadata":{"permalink":"/rlog/wechat-replacement-need","source":"@site/rlog/2020-04-16-wechat-replacement-need.mdx","title":"What Would a WeChat Replacement Need?","description":"What would a self-sovereign, private, censorship-resistant and open alternative to WeChat look like?","date":"2020-04-16T12:00:00.000Z","formattedDate":"April 16, 2020","tags":[],"readingTime":25.24,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"What Would a WeChat Replacement Need?","title":"What Would a WeChat Replacement Need?","date":"2020-04-16T12:00:00.000Z","authors":"oskarth","published":true,"slug":"wechat-replacement-need","categories":"research","image":"/img/tianstatue.jpg","discuss":"https://forum.vac.dev/t/discussion-what-would-a-wechat-replacement-need/42"},"prevItem":{"title":"Feasibility Study: Discv5","permalink":"/rlog/feasibility-discv5"},"nextItem":{"title":"From Kademlia to Discv5","permalink":"/rlog/kademlia-to-discv5"}},"content":"What would a self-sovereign, private, censorship-resistant and open alternative to WeChat look like?\\n\\n\x3c!--truncate--\x3e\\n\\nWhat would it take to replace WeChat? More specifically, what would a self-sovereign, private, censorship-resistant and open alternative look like? One that allows people to communicate, coordinate and transact freely.\\n\\n## Background\\n\\n### What WeChat provides to the end-user\\n\\nLet\'s first look at some of the things that WeChat providers. It is a lot:\\n\\n- **Messaging:** 1:1 and group chat. Text, as well as voice and video. Post gifs. Share location.\\n- **Group chat:** Limited to 500 people; above 100 people people need to verify with a bank account. Also has group video chat and QR code to join a group.\\n- **Timeline/Moments:** Post comments with attachments and have people like/comment on it.\\n- **Location Discovery:** See WeChat users that are nearby.\\n- **Profile:** Nickname and profile picture; can alias people.\\n- **\\"Broadcast\\" messages:** Send one message to many contacts, up to 200 people (spam limited).\\n- **Contacts:** Max 5000 contacts (people get around it with multiple accounts and sim cards).\\n- **App reach:** Many diferent web apps, extensions, native apps, etc. Scan QR code to access web app from phone.\\n- **Selective posting:** Decide who can view your posts and who can view your comments on other people\'s post.\\n- **Transact:** Send money gifts through red envelopes.\\n- **Transact:** Use WeChat pay to transfer money to friends and businesses; linked account with Alipay that is connected to your bank account.\\n- **Services:** Find taxis and get notifications; book flights, train tickets, hotels etc.\\n- **Mini apps:** API for all kinds of apps that allow you to provide services etc.\\n- **Picture in picture:** allowing you to have a video call while using the app.\\n\\nAnd much more. Not going to through it all in detail, and there are probably many things I don\'t know about WeChat since I\'m not a heavy user living in mainland China.\\n\\n### How WeChat works - a toy model\\n\\nThis is an overly simplistic model of how WeChat works, but it is sufficient for our purposes. This general design applies to most traditional client-server apps today.\\n\\nTo sign up for account you need a phone number or equivalent. To get access to some features you need to verify your identity further, for example with official ID and/or bank account.\\n\\nWhen you signup this creates an entry in the WeChat server, from now on treated as a black box. You authenticate with that box, and thats where you get your messages from. If you go online the app asks that box for messages you have received while you were offline. If you login from a different app your contacts and conversations are synced from that box.\\n\\nThe box gives you an account, it deals with routing to your contacts, it stores messages and attachments and gives access to mini apps that people have uploaded. For transacting money, there is a partnership with a different company that has a different box which talks to your bank account.\\n\\nThis is done in a such a way that they can support a billion users with the features above, no sweat.\\n\\nWhoever controls that box can sees who you are talking with and what the content of those messages are. There is no end to end encryption. If WeChat/Tencent disagrees with you for some reason they can ban you. This means you can\'t interact with the box under that name anymore.\\n\\n## What do we want?\\n\\nWe want something that is self-sovereign, private, censorship-resistant and open that allows individuals and groups of people to communicate and transact freely. To explore what this means in more detail, without getting lost in the weeds, we provide the following list of properties. A lot of these are tied together, and some fall out of the other requirements. Some of them stand in slight opposition to each other.\\n\\n**Self-sovereignity identity**. Exercises authority within your own sphere. If you aren\'t harming anyone, you should be able to have an account and communicate with other people.\\n\\n**Pseudonymity, and ideally total anonymity**. Not having your identity tied to your real name (e.g. through phone number, bank account, ID, etc). This allows people to act more freely without being overly worried about censorship and coercion in the real world. While total anonymity is even more desirable - especially to break multiple hops to a true-name action - real-world constraints sometimes makes this more challenging.\\n\\n**Private and secure communication**. Your communication and who you transact with should be for your eyes only. This includes transactions (transfer of value) as a form of communication.\\n\\n**Censorship-resistance**. Not being able to easily censor individuals on the platform. Both at an individual, group and collective level. Not having single points of failure that allow service to be disrupted.\\n\\n**Decentralization**. Partly falls out of censorship-resistance and other properties. If infrastructure isn\'t decentralized it means there\'s a single point of failure that can be disrupted. This is more of a tool than a goal on its own, but it is an important tool.\\n\\n**Built for mass adoption**. Includes scalabiltiy, UX (latency, reliability, bandwidth consumption, UI etc), and allowing for people to stick around. One way of doing this is to allow users to discover people they want to talk to.\\n\\n**Scalability**. Infrastructure needs to support a lot of users to be a viabile alternative. Like, a billion of them (eventually).\\n\\n**Fundamentals in place to support great user experience**. To be a viable alternative, aside from good UI and distribution, fundamentals such as latency, bandwidth usage, consistency etc must support great UX to be a viable alternative.\\n\\n**Works for resource restricted devices, including smartphones**. Most people will use a smartphone to use this. This means it has to work well on them and similar devices, without becoming a second-class citizen where we ignore properties such as censorship-resistance and privacy. Some concession to reality will be necessary due to additional constraints, which leads us to...\\n\\n**Adaptive nodes**. Nodes will have different capabilities, and perhaps at different times. To maintain a lot of the properties described here it is desirable if as many participants as possible are first-class citizens. If a phone is switching from a limited data plan to a WiFi network or from battery to AC power it can do more useful work, and so on. Likewise for a laptop with a lot of free disk space and spare compute power, etc.\\n\\n**Sustainable**. If there\'s no centralized, top down ad-driven model, this means all the infrastructure has to be sustainable somehow. Since these are individual entitites, this means it has to be paid for. While altruistic modes and similar can be used, this likely requires some form of incentivization scheme for useful services provided in the network. Related: free rider problem.\\n\\n**Spam resistant**. Relates to sustainability, scalability and built for mass adoption. Made more difficult by pseudonymous identity due to whitewashing attacks.\\n\\n**Trust-minimized**. To know that properties are provided for and aren\'t compromised, various ways of minimizing trust requirements are useful. This also related to mass adoption and social cohesion. Examples include: open and audited protocols, open source, reproducible builds, etc. This also relates to how mini apps are provided for, since we may not know their source but want to be able to use them anyway.\\n\\n**Open source**. Related to above, where we must be able to inspect the software to know that it functions as advertised and hasn\'t been compromised, e.g. by uploading private data to a third party.\\n\\nSome of these are graded and a bit subtle, i.e.:\\n\\n- Censorship resistance would ideally be able to absorb Internet shutdowns. This would require an extensive MANET/meshnet infrastructure, which while desirable, requires a lot of challenges to be overcome to be feasible.\\n- Privacy would ideally make all actions (optionally) totally anoymous, though this may incur undue costs on bandwidth and latency, which impacts user experience.\\n- Decentralization, certain topologies, such as DHTs, are efficient and quite decentralized but still have some centralized aspects, which makes it attackable in various ways. Ditto for blockchains compared with bearer instruments which requires some coordinating infrastructure, compared with naturally occuring assets such as precious metals.\\n- \\"Discover people\\" and striving for \\"total anonymity\\" might initially seem incompatible. The idea is to provide for sane defaults, and then allow people to decide how much information they want to disclose. This is the essence of privacy.\\n- Users often want _some_ form of moderation to get a good user experience, which can be seen as a form of censorship. The idea to raise the bar on the basics, the fundamental infrastructure. If individuals or specific communities want certain moderation mechanisms, that is still a compatible requirement.\\n\\n### Counterpoint 1\\n\\nWe could refute the above by saying that the design goals are undesirable. We want a system where people can censor others, and where everyone is tied to their real identity. Or we could say something like, freedom of speech is a general concept, and it doesn\'t apply to Internet companies, even if they provide a vital service. You can survive without it and you should\'ve read the terms of service. This roughly charactericizes the mainstream view.\\n\\nAdditional factor here is the idea that a group of people know more about what\'s good for you then you do, so they are protecting you.\\n\\n### Counterpoint 2\\n\\nWe could agree with all these design goals, but think they are too extreme in terms of their requirements. For example, we could operate as a non profit, take donations and volunteers, and then host the whole infrastructure ourselves. We could say we are in a friendly legislation, so we won\'t be a single point of failure. Since we are working on this and maybe even our designs are open, you can trust us and we\'ll provide service and infrastructure that gives you what you want without having to pay for it or solve all these complex decentralized computation and so on problems. If you don\'t trust us for some reason, you shouldn\'t use us regardless. Also, this is better than status quo. And we are more likely to survive by doing this, either by taking shortcuts or by being less ambituous in terms of scope.\\n\\n## Principal components\\n\\nThere are many ways to skin a cat, but this is one way of breaking down the problem. We have a general direction with the properties listed above, together with some understanding of how WeChat works for the everday user. Now the question is, what infrastructure do we need to support this? How do we achieve the above properties, or at least get closer to them? We want to figure out the necessary building blocks, and one of doing this is to map out likely necessary components.\\n\\n### Background: Ethereum and Web3 stack\\n\\nIt is worth noting that a lot of the required infrastructure has been developed, at least as concepts, in the original Ethereum / Web3 vision. In it there is Ethereum for consensus/compute/transact, storage through Swarm, and communication through Whisper. That said, the main focus has been on the Ethereum blockchain itself, and a lot of things have happened in the last 5y+ with respect to technology around privacy and scalabilty. It is worth revisiting things from a fresh point of view, with the WeChat alternative in mind as a clear use case.\\n\\n### Account - self-sovereign identity and the perils of phone numbers\\n\\nStarting from the most basic: what is an account and how do you get one? With most internet services today, WeChat and almost all popular messaging apps included, you need to signup with some centralized authority. Usually you also have to verify this with some data that ties this account to you as an individual. E.g. by requiring a phone number, which in most jurisdictions [^1] means giving out your real ID. This also means you can be banned from using the service by a somewhat arbitrary process, with no due process.\\n\\nNow, we could argue these app providers can do what they want. And they are right, in a very narrow sense. As apps like WeChat (and Google) become general-purpose platforms, they become more and more ingrained in our everyday lives. They start to provide utilities that we absolutely require to work to go about our day, such as paying for food or transportation. This means we need higher standard than this.\\n\\nJustifications for requiring phone numbers are usually centered around three claims:\\n\\n1. Avoiding spam\\n2. Tying your account to your real name, for various reasons\\n3. Using as a commonly shared identifier as a social network discovery mechanism\\n\\nOf course, many services require more than phone numbers. E.g. email, other forms of personal data such as voice recording, linking a bank account, and so on.\\n\\nIn contrast, a self-sovereign system would allow you to \\"create an account\\" completely on your own. This can easily be done with public key cryptograpy, and it also paves the way for end-to-end encryption to make your messages private.\\n\\nThe main issue with this that you need to get more creative about avoiding spam (e.g. through white washing attacks), and ideally there is some other form of social discovery mechanism.\\n\\nJust having a public key as an account isn\'t enough though. If it goes through a central server, then nothing is stopping that server from arbitrarly blocking requests related to that public key. Of course, this also depends on how transparent such requests are. Fundamentally, lest we rely completely on goodwill, there needs to be multiple actors by which you can use the service. This naturally points to decentralization as a requirement. See counterpoint.\\n\\nEven so, if the system is closed source we don\'t know what it is doing. Perhaps the app communicating is also uploading data to another place, or somehow making it possible to see who is who and act accordingly.\\n\\nYou might notice that just one simple property, self-sovereign identity, leads to a slew of other requirements and properties. You might also notice that WeChat is far from alone in this, even if their identity requirements might be a bit stringent than, say, Telegram. Their control aspects are also a bit more extreme, at least for someone with western sensibilities [^2].\\n\\nMost user facing applications have similar issues, Google Apps/FB/Twitter etc. For popular tools that have this built in, we can look at git - which is truly decentralized and have keypair at the bottom. It is for a very specific technical domain, and even then people rely on Github. Key management is fairly difficult even for technical people, and for normal people even more so. Banks are generally far behind on this tech, relying on arcane procedures and special purpose hardware for 2FA. That\'s another big issue.\\n\\nLet\'s shift gears a bit and talk about some other functional requirements.\\n\\n### Routing - packets from A to B\\n\\nIn order to get a lot of the features WeChat provides, we need the ability to do three things: communicate, store data, and transact with people. We need a bit more than that, but let\'s focus on this for now.\\n\\nTo communicate with people, in the base case, we need to go from one phone to another phone that is separated by a large distance. This requires some form of routing. The most natural platform to build this on is the existing Internet, though not the only one. Most phones are resource restricted, and are only \\"on\\" for brief periods of time. This is needed to preserve battery and bandwidth. Additionally, Internet uses IPs as endpoints, which change as a phones move through space. NAT punching etc isn\'t always perfect either. This means we need a way to get a message from one public key to another, and through some intermediate nodes. We can think of these nodes as a form of service network. Similar to how a power grid works, or phone lines, or collection of ISPs.\\n\\nOne important property here is to ensure we don\'t end up in a situation like the centralized capture scenario above, something we\'ve seen with centralized ISPs [^3] [^4] where they can choose which traffic is good and which is bad. We want to allow the use of different service nodes, just like if a restaurant gives you food poisioning you can go to the one next door and then the first one goes out of business after a while. And the circle of life continues.\\n\\nWe shouldn\'t be naive though, and think that this is something nodes are likely to do for free. They need to be adequately compensated for their services, in some of incentivization scheme. That can either be monetary, or as in the case of Bittorrent, more of a barter situation where you use game theory to coordinate with strangers [^5], and some form of reputation attached to it (for private trackers).\\n\\nThere are many ways of doing routing, and we won\'t go into too much technical detail here. Suffice to say is that you likely want both a structured and unstructured alternative, and that these comes with several trade-offs when it comes to efficiency, metadata protection, ability to incentivize, compatibility with existing topologies, and suitability for mobilephones (mostly offline, bandwidth restricted, not directly connectable). Expect more on this in a future article.\\n\\nSome of these considerations naturally leads us into the storage and transaction components.\\n\\n### Storage - available and persistant for later\\n\\nIf mobile phones are mostly offline, we need some way to store these messages so they can be retrieved when online again. The same goes for various kinds attachments as well, and for when people are switching devices. A user might control their timeline, but in the WeChat case that timeline is stored on Tencent\'s servers, and queried from there as well. This naturally needs to happen by some other service nodes. In the WeChat case, and for most IMs, the way these servers are paid for is through some indirect ad mechanism. The entity controlling these ads and so on is the same one as the one operating the servers for storage. A more direct model with different entities would see these services being compensated for their work.\\n\\nWe also need storage for attachments, mini-apps, as well as a way of understanding the current state of consensus when it comes to the compute/transact module. In the WeChat case, this state is completely handled by the bank institution or one of their partners, such as Alibaba. When it comes to bearer instruments like cash, no state needs to be kept as that\'s a direct exchange in the physical world. This isn\'t directly compatible with transfering value over a distance.\\n\\nAll of this state requires availability and persistance. It should be done in a trust minimized fashion and decentralized, which requires some form of incentivization for keeping data around. If it isn\'t, you are relying on social cohesion which breaks down at very large scales.\\n\\nSince data will be spread out across multiple nodes, you need a way to sync data and transfer it in the network. As well as being able to add and query data from it. All of this requires a routing component.\\n\\nTo make it more censorship resistant it might be better to keep it as a general-purpose store, i.e. individuals don\'t need to know what they storing. Otherwise, you naturally end up in a situation where individual nodes can be pressured to not store certain content.\\n\\n### Messaging - from me to you to all of us (not them)\\n\\nThis builds on top of routing, but it has a slightly different focus. The goal is to allow for individuals and groups to communicate in a private, secure and censorship-resistant manner.\\n\\nIt also needs to provide a decent interface to the end user, in terms of dealing seamlessly with offline messages, providing reliable and timely messaging.\\n\\nIn order to get closer to the ideal of total anonymity, it is useful to be able to hide metadata of who is talking to whom. This applies to both normal communication as well as for transactions. Ideally, no one but the parties involved can see who is taking part in a conversation. This can be achieved through various techniques such as mixnets, anonymous credentials, private information retrieval, and so on. Many of these techniques have a fundamental trade-off with latency and bandwidth, something that is a big concern for mobilephones. Being able to do some form of tuning, in an adaptive node manner, depending on your threat model and current capabilities is useful here.\\n\\nThe baseline here is pseudonymity, and having tools to allow individuals to \\"cut off\\" ties to their real world identity and transactions. People act different in different circles in the real world, and this should be mimicked online as well. Your company, family or government shouldn\'t be able to know what exactly you use your paycheck for, and who you are talking to.\\n\\n### Compute - transact, contract and settle\\n\\nThe most immediate need here is transaction from A to B. Direct exchange. There is also a more indirect need for private lawmaking and contracting.\\n\\nWe talked about routing and storage and how they likely need to be incentivized to work properly. How are they going to be compensated? While this could in theory work via existing banking system and so on, this would be rather heavy. It\'d also very likely require tying your identifier to your legal name, something that goes against what we want to achieve. What we want is something that acts more as right-to-access, similar to the way cash functions in a society [^6]. I pay for a fruit with something that is valuable to you and then I\'m on my way.\\n\\nWhile there might be other candidates, such as pre-paid debit cards and so on, this transaction mode pretty much requires a cryptocurrency component. The alternative is to do it on a reputation basis, which might work for small communities, due to social cohesion, but quickly detoriates for large ones [^7]. Ad hoc models like private Bittorrent trackers are centralized and easy to censor.\\n\\nNow, none of the existing cryptocurrency models are ideal. They also all suffer from lack of widespread use, and it is difficult to get onboarded to them in the first place. Transactions in Bitcoin are slow. Ethereum is faster and has more capabilities, but it still suffers from linking payments over time, which makes the privacy part of this more difficult. Zcash, Monero and similar are interesting, but also require more use. For Zcash, shielded transactions appear to only account for less than 2% of all transactions in 2019 [^8] [^9].\\n\\nAnother dimension is what sets general purpose cryptocurrencies like Ethereum apart. Aside from just paying from A to B, you can encode rules about when something should be paid out and not. This is very useful for doing a form of private lawmaking, contracting, for setting up service agreements with these nodes. If there\'s no trivial recourse as in the meatspace world, where you know someone\'s name and you can sue them, you need a different kind of model.\\n\\nWhat makes something like Zcash interesting is that it works more like digital cash. Instead of leaving a public trail for everyone, where someone can see where you got the initial money from and then trace you across various usage, for Zcash every hop is privacy preserving.\\n\\nTo fulfill the general goals of being censorship resistance and secure, it is also vital that the system being used stays online and can\'t be easily disrupted. That points to disintermediation, as opposed to using gateways and exchanges. This is a case where something like cash, or gold, is more direct, since no one can censor this transaction without being physically present where this direct exchange is taking place. However, like before, this doesn\'t work over distance.\\n\\n### Secure chat - just our business\\n\\nSimilar to the messaging module above. The distinction here is that we assume the network part has already taken place. Here we are interested in keeping the contents of messages private, so that means confidentiality/end-to-end encryption, integrity, authentication, as well as forward secrecy and plausible deniability. This means that even if there\'s some actor that gets some private key material, or confiscated your phone, there is some level of...ephemerality to your conversations. Another issue here in terms of scalable private group chat.\\n\\n### Extensible mini apps\\n\\nThis relates to the compute and storage module above. Essentially we want to provide mini apps as in WeChat, but to do so in a way that is compatible with what we want to achieve more generally. This allows individuals and small businesses to create small tools for various purposes, and coordinate with strangers. E.g. booking a cab or getting an insurance, and so on.\\n\\nThis has a higher dependency on the contracting/general computation aspect. I.e. often it isn\'t only a transaction, but you might want to encode some specific rules here that strangers can abide by without having too high trust requirements. As a simple example: escrows.\\n\\nThis also needs an open API that anyone can use. It should be properly secured, so using one doesn\'t compromise the rest of the system it is operating in. To be censorship resistant it requires the routing and storage component to work properly.\\n\\n## Where are we now?\\n\\nLet\'s look back at some of desirable properties we set out in the beginning and see how close we are to building out the necessary components. Is it realistic at all or just a pipe dream? We\'ll see that there are many building blocks in place, and there\'s reason for hope.\\n\\n**Self-sovereignity identity**. Public key crypto and web of trust like constructs makes this possible.\\n\\n**Pseudonymity, and ideally total anonymity**. Pseudonymity can largely be achieved with public key crypto and open systems that allow for permissionless participation. For transactions, pseudonymity exists in most cryptocurrencies. The challenge is linkage across time, especially when interfacing with other \\"legacy\\" system. There are stronger constructs that are actively being worked on and are promising here, such as mixnets (Nym), mixers (Wasabi Wallet, Tornado.Cash) and zero knowledge proofs (Zcash, Ethereum, Starkware). This area of applied research has exploded over the last few years.\\n\\n**Private and secure communication**. Signal has pioneered a lot of this, following OTR. Double Ratchet, X3DH. E2EE is minimum these days, and properties like PFS and PD are getting better. For metadata protection, you have Tor, with its faults, and more active research on mixnets and private information retrieval, etc.\\n\\n**Censorship-resistance**. This covers a lot of ground across the spectrum. You have technologies like Bittorrent, Bitcoin/Ethereum, Tor obfuscated transports, E2EE by default, partial mesh networks in production, abilit to move/replicate host machines more quickly have all made this more of a reality than it used to be. this easier. Of course, techniques such as deep packet inspection and internet shutdowns have increased.\\n\\n**Decentralization**. Cryptocurrencies, projects like libp2p and IPFS. Need to be mindful here of many projects that claim decentralization but are still vulnerable to single points of failures, such as relying on gateways.\\n\\n**Built for mass adoption**. This one is more subjective. There\'s definitely a lot of work to be done here, both when it comes to fundamental performance, key management and things like social discoverability. Directionally these things are improving and becoming easier for the average person but there is a lot ot be done here.\\n\\n**Scalability**. With projects like Ethereum 2.0 and IPFS more and more resources are a being put into this, both at the consensus/compute layer as well as networking (gossip, scalable Kademlia) layer. Also various layer 2 solutions for transactions.\\n\\n**Fundamentals in place to support great user experience**. Similar to built for mass adoption. As scalability becomes more important, more applied research is being done in the p2p area to improve things like latency, bandwidth.\\n\\n**Works for resource restricted devices, including smartphones**. Work in progress and not enough focus here, generally an after thought. Also have stateless clients etc.\\n\\n**Adaptive nodes**. See above. With subprotocols and capabilities in Ethereum and libp2p, this is getting easier.\\n\\n**Sustainable**. Token economics is a thing. While a lot of it won\'t stay around, there are many more projects working on making themselves dispensable. Being open source, having an engaged community and enabling users run their own infrastructure. Users as stakeholders.\\n\\n**Spam resistant**. Tricky problem if you want to be pseudonymous, but some signs of hope with incentivization mechanisms, zero knowledge based signaling, etc. Together with various forms of rate limiting and better controlling of topology and network amplification. And just generally being battle-tested by real world attacks, such as historical Ethereum DDoS attacks.\\n\\n**Trust minimized**. Bitcoin. Zero knowledge provable computation. Open source. Reproducible builds. Signed binaries. Incentive compatible structures. Independent audits. Still a lot of work, but getting better.\\n\\n**Open source**. Big and only getting bigger. Including mainstream companies.\\n\\n## What\'s next?\\n\\nWe\'ve look at what WeChat provides and what we\'d like an alternative to look like. We\'ve also seen a few principal modules that are necessary to achieve those goals. To achieve all of this is a daunting task, and one might call it overly ambitiuous. We\'ve also seen how far we\'ve come with some of the goals, and how a lot of the pieces are there, in one form or another. Then it is a question of putting them all together in the right mix.\\n\\nThe good news is that a lot of people are working all these building blocks and thinking about these problems. Compared to a few years ago we\'ve come quite far when it comes to p2p infrastructure, privacy, security, scalability, and general developer mass and mindshare. If you want to join us in building some of these building blocks, and assembling them, check out our forum.\\n\\nPS. We are [hiring protocol engineers](https://status.im/our_team/open_positions.html). DS\\n\\n## Acknowledgements\\n\\nCorey, Dean, Jacek.\\n\\n## References\\n\\n[^1]: Mandatory SIM card registration laws: https://privacyinternational.org/long-read/3018/timeline-sim-card-registration-laws\\n[^2]: On WeChat keyword censorship: https://citizenlab.ca/2016/11/wechat-china-censorship-one-app-two-systems/\\n[^3]: Net Neutrality: https://www.eff.org/issues/net-neutrality\\n[^4]: ISP centralization: https://ilsr.org/repealing-net-neutrality-puts-177-million-americans-at-risk/\\n[^5]: Incentives Build Robustness in BitTorrent bittorrent.org/bittorrentecon.pdf\\n[^6]: The Case for Electronic Cash: https://coincenter.org/files/2019-02/the-case-for-electronic-cash-coin-center.pdf\\n[^7]: Money, blockchains, and social scalability: http://unenumerated.blogspot.com/2017/02/money-blockchains-and-social-scalability.html\\n[^8]: Zcash private transactions (partial paywall): https://www.theblockcrypto.com/genesis/48413/an-analysis-of-zcashs-private-transactions\\n[^9]: Shielded transactions usage (stats page 404s): https://z.cash/support/faq/"},{"id":"kademlia-to-discv5","metadata":{"permalink":"/rlog/kademlia-to-discv5","source":"@site/rlog/2020-04-9-kademlia-to-discv5.mdx","title":"From Kademlia to Discv5","description":"A quick history of discovery in peer-to-peer networks, along with a look into discv4 and discv5, detailing what they are, how they work and where they differ.","date":"2020-04-09T16:00:00.000Z","formattedDate":"April 9, 2020","tags":[],"readingTime":8.045,"hasTruncateMarker":true,"authors":[{"name":"Dean","twitter":"DeanEigenmann","github":"decanus","website":"https://dean.eigenmann.me","key":"dean"}],"frontMatter":{"layout":"post","name":"From Kademlia to Discv5","title":"From Kademlia to Discv5","date":"2020-04-09T16:00:00.000Z","authors":"dean","published":true,"slug":"kademlia-to-discv5","categories":"research"},"prevItem":{"title":"What Would a WeChat Replacement Need?","permalink":"/rlog/wechat-replacement-need"},"nextItem":{"title":"Waku Update","permalink":"/rlog/waku-update"}},"content":"A quick history of discovery in peer-to-peer networks, along with a look into discv4 and discv5, detailing what they are, how they work and where they differ.\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'ve been working on Ethereum or adjacent technologies you\'ve probably heard of [discv4](https://github.com/ethereum/devp2p/blob/master/discv4.md) or [discv5](https://github.com/ethereum/devp2p/blob/master/discv5/discv5.md). But what are they actually? How do they work and what makes them different? To answer these questions, we need to start at the beginning, so this post will assume that there is little knowledge on the subject so the post should be accessible for anyone.\\n\\n## The Beginning\\n\\nLet\'s start right at the beginning: the problem of discovery and organization of nodes in peer-to-peer networks.\\n\\nEarly P2P file sharing technologies, such as Napster, would share information about who holds what file using a single server. A node would connect to the central server and give it a list of the files it owns. Another node would then connect to that central server, find a node that has the file it is looking for and contact that node. This however was a flawed system -- it was vulnerable to attacks and left a single party open to lawsuits.\\n\\nIt became clear that another solution was needed, and after years of research and experimentation, we were given the distributed hash table or DHT.\\n\\n## Distributed Hash Tables\\n\\nIn 2001 4 new protocols for such DHTs were conceived, [Tapestry](https://pdos.csail.mit.edu/~strib/docs/tapestry/tapestry_jsac03.pdf), [Chord](https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf), [CAN](https://people.eecs.berkeley.edu/~sylvia/papers/cans.pdf) and [Pastry](http://rowstron.azurewebsites.net/PAST/pastry.pdf), all of which made various trade-offs and changes in their core functionality, giving them unique characteristics.\\n\\nBut as said, they\'re all DHTs. So what is a DHT?\\n\\nA distributed hash table (DHT) is essentially a distributed key-value list. Nodes participating in the DHT can easily retrieve the value for a key.\\n\\nIf we have a network with 9 key-value pairs and 3 nodes, ideally each node would store 3 (optimally 6 for redundancy) of those key-value pairs, meaning that if a key-value pair were to be updated, only part of the network would responsible for ensuring that it is. The idea is that any node in the network would know where to find the specific key-value pair it is looking for based on how things are distributed amongst the nodes.\\n\\n## Kademlia\\n\\nSo now that we know what DHTs are, let\'s get to Kademlia, the predecessor of discv4. Kademlia was created by Petar Maymounkov and David Mazi\xe8res in 2002. I will naively say that this is probably one of the most popular and most used DHT protocols. It\'s quite simple in how it works, so let\'s look at it.\\n\\nIn Kademlia, nodes and values are arranged by distance (in a very mathematical definition). This distance is not a geographical one, but rather based on identifiers. It is calculated how far 2 identifiers are from eachother using some distance function.\\n\\nKademlia uses an `XOR` as its distance function. An `XOR` is a function that outputs `true` only when inputs differ. Here is an example with some binary identifiers:\\n\\n```\\nXOR 10011001\\n    00110010\\n    --------\\n    10101011\\n```\\n\\nThe top in decimal numbers means that the distance between `153` and `50` is `171`.\\n\\nThere are several reasons why `XOR` was taken:\\n\\n1.  The distance from one ID to itself will be `0`.\\n2.  Distance is symmetric, A to B is the same as B to A.\\n3.  Follows triangle inequality, if `A`, `B` and `C` are points on a triangle then the distance `A` to `B` is closer or equal to that of `A` to `C` plus the one from `B` to `C`.\\n\\nIn summary, this distance function allows a node to decide what is \\"close\\" to it and make decisions based on that \\"closeness\\".\\n\\nKademlia nodes store a routing table. This table contains multiple lists. Each subsequent list contains nodes which are a little further distanced than the ones included in the previous list. Nodes maintain detailed knowledge about nodes closest to them, and the further away a node is, the less knowledge the node maintains about it.\\n\\nSo let\'s say I want to find a specific node. What I would do is go to any node which I already know and ask them for all their neighbours closest to my target. I repeat this process for the returned neighbours until I find my target.\\n\\nThe same thing happens for values. Values have a certain distance from nodes and their IDs are structured the same way so we can calculate this distance. If I want to find a value, I simply look for the neighbours closest to that value\'s key until I find the one storing said value.\\n\\nFor Kademlia nodes to support these functions, there are several messages with which the protocol communicates.\\n\\n- `PING` - Used to check whether a node is still running.\\n- `STORE` - Stores a value with a given key on a node.\\n- `FINDNODE` - Returns the closest nodes requested to a given ID.\\n- `FINDVALUE` - The same as `FINDNODE`, except if a node stores the specific value it will return it directly.\\n\\n_This is a **very** simplified explanation of Kademlia and skips various important details. For the full description, make sure to check out the [paper](https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf) or a more in-depth [design specification](http://xlattice.sourceforge.net/components/protocol/kademlia/specs.html)_\\n\\n## Discv4\\n\\nNow after that history lesson, we finally get to discv4 (which stands for discovery v4), Ethereum\'s current node discovery protocol. The protocol itself is essentially based off of Kademlia, however it does away with certain aspects of it. For example, it does away with any usage of the value part of the DHT.\\n\\nKademlia is mainly used for the organisation of the network, so we only use the routing table to locate other nodes. Due to the fact that discv4 doesn\'t use the value portion of the DHT at all, we can throw away the `FINDVALUE` and `STORE` commands described by Kademlia.\\n\\nThe lookup method previously described by Kademlia describes how a node gets its peers. A node contacts some node and asks it for the nodes closest to itself. It does so until it can no longer find any new nodes.\\n\\nAdditionally, discv4 adds mutual endpoint verification. This is meant to ensure that a peer calling `FINDNODE` also participates in the discovery protocol.\\n\\nFinally, all discv4 nodes are expected to maintain up-to-date ENR records. These contain information about a node. They can be requested from any node using a discv4-specific packet called `ENRRequest`.\\n\\n_If you want some more details on ENRs, check out one of my posts [\\"Network Addresses in Ethereum\\"](https://dean.eigenmann.me/blog/2020/01/21/network-addresses-in-ethereum/)_\\n\\nDiscv4 comes with its own range of problems however. Let\'s look at a few of them.\\n\\nFirstly, the way discv4 works right now, there is no way to differentiate between node sub-protocols. This means for example that an Ethereum node could add an Ethereum Classic Node, Swarm or Whisper node to its DHT without realizing that it is invalid until more communication has happened. This inability to differentiate sub-protocols makes it harder to find specific nodes, such as Ethereum nodes with light-client support.\\n\\nNext, in order to prevent replay attacks, discv4 uses timestamps. This however can lead to various issues when a host\'s clock is wrong. For more details, see the [\\"Known Issues\\"](https://github.com/ethereum/devp2p/blob/master/discv4.md#known-issues-in-the-current-version) section of the discv4 specification.\\n\\nFinally, we have an issue with the way mutual endpoint verification works. Messages can get dropped and there is no way to tell if both peers have verified eachother. This means that we could consider our peer verified while it does not consider us so making them drop the `FINDNODE` packet.\\n\\n## Discv5\\n\\nFinally, let\'s look at discv5. The next iteration of discv4 and the discovery protocol which will be used by Eth 2.0. It aims at fixing various issues present in discv4.\\n\\nThe first change is the way `FINDNODE` works. In traditional Kademlia as well as in discv5, we pass an identifier. However, in discv5 we instead pass the logarithmic distance, meaning that a `FINDNODE` request gets a response containing all nodes at the specified logarithmic distance from the called node.\\n\\nLogarithmic distance means we first calculate the distance and then run it through our log base 2 function. See:\\n\\n```\\nlog2(A xor B)\\n```\\n\\nAnd the second, more important change, is that discv5 aims at solving one of the biggest issues of discv4: the differentiation of sub-protocols. It does this by adding topic tables. Topic tables are [first in first out](<https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)>) lists that contain nodes which have advertised that they provide a specific service. Nodes get themselves added to this list by registering `ads` on their peers.\\n\\nAs of writing, there is still an issue with this proposal. There is currently no efficient way for a node to place `ads` on multiple peers, since it would require separate requests for every peer which is inefficient in a large-scale network.\\n\\nAdditionally, it is unclear how many peers a node should place these `ads` on and exactly which peers to place them on. For more details, check out the issue [devp2p#136](https://github.com/ethereum/devp2p/issues/136).\\n\\nThere are a bunch more smaller changes to the protocol, but they are less important hence they were ommitted from this summary.\\n\\nNevertheless, discv5 still does not resolve a couple issues present in discv4, such as unreliable endpoint verification. As of writing this post, there is currently no new method in discv5 to improve the endpoint verification process.\\n\\nAs you can see discv5 is still a work in progress and has a few large challenges to overcome. However if it does, it will most likely be a large improvement to a more naive Kademlia implementations.\\n\\n---\\n\\nHopefully this article helped explain what these discovery protocols are and how they work. If you\'re interested in their full specifications you can find them on [github](https://github.com/ethereum/devp2p)."},{"id":"waku-update","metadata":{"permalink":"/rlog/waku-update","source":"@site/rlog/2020-02-14-waku-update.mdx","title":"Waku Update","description":"A research log. What\'s the current state of Waku? How many users does it support? What are the bottlenecks? What\'s next?","date":"2020-02-14T12:00:00.000Z","formattedDate":"February 14, 2020","tags":[],"readingTime":5.63,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"Waku Update","title":"Waku Update","date":"2020-02-14T12:00:00.000Z","authors":"oskarth","published":true,"slug":"waku-update","categories":"research","image":"/img/waku_infrastructure_sky.jpg","discuss":"https://forum.vac.dev/t/waku-update-where-are-we-at/34"},"prevItem":{"title":"From Kademlia to Discv5","permalink":"/rlog/kademlia-to-discv5"},"nextItem":{"title":"DNS Based Discovery","permalink":"/rlog/dns-based-discovery"}},"content":"A research log. What\'s the current state of Waku? How many users does it support? What are the bottlenecks? What\'s next?\\n\\n\x3c!--truncate--\x3e\\n\\nWaku is our fork of Whisper where we address the shortcomings of Whisper in an iterative manner. We\'ve seen a in [previous post](https://vac.dev/fixing-whisper-with-waku) that Whisper doesn\'t scale, and why. In this post we\'ll talk about what the current state of Waku is, how many users it can support, and future plans.\\n\\n## Current state\\n\\n**Specs:**\\n\\nWe released [Waku spec v0.3](https://rfc.vac.dev/waku/standards/legacy/6/waku1) this week! You can see the full changelog [here](https://rfc.vac.dev/waku/standards/legacy/6/waku1/#changelog).\\n\\nThe main change from 0.2 is making the handshake more flexible. This enables us to communicate topic interest immediately without ambiguity. We also did the following:\\n\\n- added recommendation for DNS based discovery\\n- added an upgradability and compatibility policy\\n- cut the spec up into several components\\n\\nWe cut the spec up in several components to make Vac as modular as possible. The components right now are:\\n\\n- Waku (main spec), currently in [version 0.3.0](https://rfc.vac.dev/waku/standards/legacy/6/waku1)\\n- Waku envelope data field, currently in [version 0.1.0](https://rfc.vac.dev/waku/standards/legacy/7/data)\\n- Waku mailserver, currently in [version 0.2.0](https://rfc.vac.dev/waku/standards/legacy/8/mail)\\n\\nWe can probably factor these out further as the main spec is getting quite big, but this is good enough for now.\\n\\n**Clients:**\\n\\nThere are currently two clients that implement Waku v0.3, these are [Nimbus (Update: now nim-waku)](https://github.com/status-im/nim-waku) in Nim and [status-go](https://github.com/status-im/status-go) in Go.\\n\\nFor more details on what each client support and don\'t, you can follow the [work in progress checklist](https://github.com/vacp2p/pm/issues/7).\\n\\nWork is currently in progress to integrate it into the [Status core app](https://github.com/status-im/status-react/pull/9949). Waku is expected to be part of their upcoming 1.1 release (see [Status app roadmap (link deprecated)](https://trello.com/b/DkxQd1ww/status-app-roadmap)).\\n\\n**Simulation:**\\n\\nWe have a [simulation](https://github.com/status-im/nim-waku/blob/master/waku/v1/node/quicksim.nim) that verifies - or rather, fails to falsify - our [scalability model](https://vac.dev/fixing-whisper-with-waku). More on the simulation and what it shows below.\\n\\n## How many users does Waku support?\\n\\nThis is our current understanding of how many users a network running Waku can support. Specifically in the context of the Status chat app, since that\'s the most immediate consumer of Waku. It should generalize fairly well to most deployments.\\n\\n**tl;dr (for Status app):**\\n\\n- beta: 100 DAU\\n- v1: 1k DAU\\n- v1.1 (waku only): 10k DAU (up to x10 with deployment hotfixes)\\n- v1.2 (waku+dns): 100k DAU (can optionally be folded into v1.1)\\n\\n_Assuming 10 concurrent users = 100 DAU. Estimate uncertainty increases for each order of magnitude until real-world data is observed._\\n\\nAs far as we know right now, these are the bottlenecks we have:\\n\\n- Immediate bottleneck - Receive bandwidth for end user clients (aka \u2018Fixing Whisper with Waku\u2019)\\n- Very likely bottleneck - Nodes and cluster capacity (aka \u2018DNS based node discovery\u2019)\\n- Conjecture but not unlikely to appear- Full node traffic (aka \u2018the routing / partition problem\u2019)\\n\\nWe\'ve already seen the first bottleneck being discussed in the initial post. Dean wrote a post on [DNS based discovery](https://vac.dev/dns-based-discovery) which explains how we will address the likely second bottleneck. More on the third one in future posts.\\n\\nFor more details on these bottlenecks, see [Scalability estimate: How many users can Waku and the Status app support?](https://discuss.status.im/t/scalability-estimate-how-many-users-can-waku-and-the-status-app-support/1514).\\n\\n## Simulation\\n\\nThe ultimate test is real-world usage. Until then, we have a simulation thanks to Kim De Mey from the Nimbus team!\\n\\n![](/img/waku_simulation.jpeg)\\n\\nWe have two network topologies, Star and full mesh. Both networks have 6 full nodes, one traditional light node with bloom filter, and one Waku light node.\\n\\nOne of the full nodes sends 1 envelope over 1 of the 100 topics that the two light nodes subscribe to. After that, it sends 10000 envelopes over random topics.\\n\\nFor light node, bloom filter is set to almost 10% false positive (bloom filter: n=100, k=3, m=512). It shows the number of valid and invalid envelopes received for the different nodes.\\n\\n**Star network:**\\n\\n| Description     | Peers | Valid | Invalid |\\n| --------------- | ----- | ----- | ------- |\\n| Master node     | 7     | 10001 | 0       |\\n| Full node 1     | 3     | 10001 | 0       |\\n| Full node 2     | 1     | 10001 | 0       |\\n| Full node 3     | 1     | 10001 | 0       |\\n| Full node 4     | 1     | 10001 | 0       |\\n| Full node 5     | 1     | 10001 | 0       |\\n| Light node      | 2     | 815   | 0       |\\n| Waku light node | 2     | 1     | 0       |\\n\\n**Full mesh:**\\n\\n| Description     | Peers | Valid | Invalid |\\n| --------------- | ----- | ----- | ------- |\\n| Full node 0     | 7     | 10001 | 20676   |\\n| Full node 1     | 7     | 10001 | 9554    |\\n| Full node 2     | 5     | 10001 | 23304   |\\n| Full node 3     | 5     | 10001 | 11983   |\\n| Full node 4     | 5     | 10001 | 24425   |\\n| Full node 5     | 5     | 10001 | 23472   |\\n| Light node      | 2     | 803   | 803     |\\n| Waku light node | 2     | 1     | 1       |\\n\\nThings to note:\\n\\n- Whisper light node with ~10% false positive gets ~10% of total traffic\\n- Waku light node gets ~1000x less envelopes than Whisper light node\\n- Full mesh results in a lot more duplicate messages, expect for Waku light node\\n\\nRun the simulation yourself [here](https://github.com/status-im/nim-waku/blob/master/waku/v1/node/quicksim.nim). The parameters are configurable, and it is integrated with Prometheus and Grafana.\\n\\n## Difference between Waku and Whisper\\n\\nSummary of main differences between Waku v0 spec and Whisper v6, as described in [EIP-627](https://eips.ethereum.org/EIPS/eip-627):\\n\\n- Handshake/Status message not compatible with shh/6 nodes; specifying options as association list\\n- Include topic-interest in Status handshake\\n- Upgradability policy\\n- `topic-interest` packet code\\n- RLPx subprotocol is changed from shh/6 to waku/0.\\n- Light node capability is added.\\n- Optional rate limiting is added.\\n- Status packet has following additional parameters: light-node, confirmations-enabled and rate-limits\\n- Mail Server and Mail Client functionality is now part of the specification.\\n- P2P Message packet contains a list of envelopes instead of a single envelope.\\n\\n## Next steps and future plans\\n\\nSeveral challenges remain to make Waku a robust and suitable base\\ncommunication protocol. Here we outline a few challenges that we are addressing and will continue to work on:\\n\\n- scalability of the network\\n- incentived infrastructure and spam-resistance\\n- build with resource restricted devices in mind, including nodes being mostly offline\\n\\nFor the third bottleneck, a likely candidate for fixing this is Kademlia routing. This is similar to what is done in [Swarm\'s](https://www.ethswarm.org/) PSS. We are in the early stages of experimenting with this over libp2p in [nim-libp2p](https://github.com/status-im/nim-libp2p). More on this in a future post!\\n\\n## Acknowledgements\\n\\n_Image from \\"caged sky\\" by mh.xbhd.org is licensed under CC BY 2.0 (https://ccsearch.creativecommons.org/photos/a9168311-78de-4cb7-a6ad-f92be8361d0e)_"},{"id":"dns-based-discovery","metadata":{"permalink":"/rlog/dns-based-discovery","source":"@site/rlog/2020-02-7-dns-based-discovery.mdx","title":"DNS Based Discovery","description":"A look at EIP-1459 and the benefits of DNS based discovery.","date":"2020-02-07T12:00:00.000Z","formattedDate":"February 7, 2020","tags":[],"readingTime":5.635,"hasTruncateMarker":true,"authors":[{"name":"Dean","twitter":"DeanEigenmann","github":"decanus","website":"https://dean.eigenmann.me","key":"dean"}],"frontMatter":{"layout":"post","name":"DNS Based Discovery","title":"DNS Based Discovery","date":"2020-02-07T12:00:00.000Z","authors":"dean","published":true,"slug":"dns-based-discovery","categories":"research"},"prevItem":{"title":"Waku Update","permalink":"/rlog/waku-update"},"nextItem":{"title":"Fixing Whisper with Waku","permalink":"/rlog/fixing-whisper-with-waku"}},"content":"A look at EIP-1459 and the benefits of DNS based discovery.\\n\\n\x3c!--truncate--\x3e\\n\\nDiscovery in p2p networks is the process of how nodes find each other and specific resources they are looking for. Popular discovery protocols, such as [Kademlia](https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf) which utilizes a [distributed hash table](https://en.wikipedia.org/wiki/Distributed_hash_table) or DHT, are highly inefficient for resource restricted devices. These methods use short connection windows, and it is quite battery intensive to keep establishing connections. Additionally, we cannot expect a mobile phone for example to synchronize an entire DHT using cellular data.\\n\\nAnother issue is how we do the initial bootstrapping. In other words, how does a client find its first node to then discover the rest of the network? In most applications, including Status right now, this is done with a [static list of nodes](https://rfc.vac.dev/status/deprecated/client/#bootstrapping) that a client can connect to.\\n\\nIn summary, we have a static list that provides us with nodes we can connect to which then allows us to discover the rest of the network using something like Kademlia. But what we need is something that can easily be mutated, guarantees a certain amount of security, and is efficient for resource restricted devices. Ideally our solution would also be robust and scalable.\\n\\nHow do we do this?\\n\\n[EIP 1459: Node Discovery via DNS](https://eips.ethereum.org/EIPS/eip-1459), which is one of the strategies we are using for discovering waku nodes. [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459) is a DNS-based discovery protocol that stores [merkle trees](https://en.wikipedia.org/wiki/Merkle_tree) in DNS records which contain connection information for nodes.\\n\\n_Waku is our fork of Whisper. Oskar recently wrote an [entire post](https://vac.dev/fixing-whisper-with-waku) explaining it. In short, Waku is our method of fixing the shortcomings of Whisper in a more iterative fashion. You can find the specification [here](https://rfc.vac.dev/waku/standards/legacy/6/waku1/)_\\n\\nDNS-based methods for bootstrapping p2p networks are quite popular. Even Bitcoin uses it, but it uses a concept called DNS seeds, which are just DNS servers that are configured to return a list of randomly selected nodes from the network upon being queried. This means that although these seeds are hardcoded in the client, the IP addresses of actual nodes do not have to be.\\n\\n```console\\n> dig dnsseed.bluematt.me +short\\n129.226.73.12\\n107.180.78.111\\n169.255.56.123\\n91.216.149.28\\n85.209.240.91\\n66.232.124.232\\n207.55.53.96\\n86.149.241.168\\n193.219.38.57\\n190.198.210.139\\n74.213.232.234\\n158.181.226.33\\n176.99.2.207\\n202.55.87.45\\n37.205.10.3\\n90.133.4.73\\n176.191.182.3\\n109.207.166.232\\n45.5.117.59\\n178.211.170.2\\n160.16.0.30\\n```\\n\\nThe above displays the result of querying on of these DNS seeds. All the nodes are stored as [`A` records](https://simpledns.plus/help/a-records) for the given domain name. This is quite a simple solution which Bitcoin almost soley relies on since removing the [IRC bootstrapping method in v0.8.2](https://en.bitcoin.it/wiki/Network#IRC).\\n\\nWhat makes this DNS based discovery useful? It allows us to have a mutable list of bootstrap nodes without needing to ship a new version of the client every time a list is mutated. It also allows for a more lightweight method of discovering nodes, something very important for resource restricted devices.\\n\\nAdditionally, DNS provides us with a robust and scalable infrastructure. This is due to its hierarchical architecture. This hierarchical architecture also already makes it distributed such that the failure of one DNS server does not result in us no longer being able to resolve our name.\\n\\nAs with every solution though, there is a trade-off. By storing the list in DNS name an adversary would simply need to censor the DNS records for a specific name. This would prevent any new client trying to join the network from being able to do so.\\n\\nOne thing you notice when looking at [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459) is that it is a lot more technically complex than Bitcoin\'s way of doing this. So if Bitcoin uses this simple method and has proven that it works, why did we need a new method?\\n\\nThere are multiple reasons, but the main one is **security**. In the Bitcoin example, an attacker could create a new list and no one querying would be able to tell. This is however mitigated in [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459) where we can verify the integrity of the entire returned list by storing an entire merkle tree in the DNS records.\\n\\nLet\'s dive into this. Firstly, a client that is using these DNS records for discovery must know the public key corresponding to the private key controlled by the entity creating the list. This is because the entire list is signed using a secp256k1 private key, giving the client the ability to authenticate the list and know that it has not been tampered with by some external party.\\n\\nSo that already makes this a lot safer than the method Bitcoin uses. But how are these lists even stored? As previously stated they are stored using **merkle trees** as follows:\\n\\n- The root of the tree is stored in a [`TXT` record](https://simpledns.plus/help/txt-records), this record contains the tree\'s root hash, a sequence number which is incremented every time the tree is updated and a signature as stated above.\\n\\n  Additionally, there is also a root hash to a second tree called a **link tree**, it contains the information to different lists. This link tree allows us to delegate trust and build a graph of multiple merkle trees stored across multiple DNS names.\\n\\n  The sequence number ensures that an attacker cannot replace a tree with an older version because when a client reads the tree, they should ensure that the sequence number is greater than the last synchronized version.\\n\\n- Using the root hash for the tree, we can find the merkle tree\'s first branch, the branch is also stored in a `TXT` record. The branch record contains all the hashes of the branch\'s leafs.\\n\\n- Once a client starts reading all the leafs, they can find one of two things: either a new branch record leading them further down the tree or an Ethereum Name Records (ENR) which means they now have the address of a node to connect to! To learn more about ethereum node records you can have a look at [EIP-778](https://eips.ethereum.org/EIPS/eip-778), or read a short blog post I wrote explaining them [here](https://dean.eigenmann.me/blog/2020/01/21/network-addresses-in-ethereum/#enr).\\n\\nBelow is the zone file taken from the [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459), displaying how this looks in practice.\\n\\n```\\n; name                        ttl     class type  content\\n@                             60      IN    TXT   enrtree-root:v1 e=JWXYDBPXYWG6FX3GMDIBFA6CJ4 l=C7HRFPF3BLGF3YR4DY5KX3SMBE seq=1 sig=o908WmNp7LibOfPsr4btQwatZJ5URBr2ZAuxvK4UWHlsB9sUOTJQaGAlLPVAhM__XJesCHxLISo94z5Z2a463gA\\nC7HRFPF3BLGF3YR4DY5KX3SMBE    86900   IN    TXT   enrtree://AM5FCQLWIZX2QFPNJAP7VUERCCRNGRHWZG3YYHIUV7BVDQ5FDPRT2@morenodes.example.org\\nJWXYDBPXYWG6FX3GMDIBFA6CJ4    86900   IN    TXT   enrtree-branch:2XS2367YHAXJFGLZHVAWLQD4ZY,H4FHT4B454P6UXFD7JCYQ5PWDY,MHTDO6TMUBRIA2XWG5LUDACK24\\n2XS2367YHAXJFGLZHVAWLQD4ZY    86900   IN    TXT   enr:-HW4QOFzoVLaFJnNhbgMoDXPnOvcdVuj7pDpqRvh6BRDO68aVi5ZcjB3vzQRZH2IcLBGHzo8uUN3snqmgTiE56CH3AMBgmlkgnY0iXNlY3AyNTZrMaECC2_24YYkYHEgdzxlSNKQEnHhuNAbNlMlWJxrJxbAFvA\\nH4FHT4B454P6UXFD7JCYQ5PWDY    86900   IN    TXT   enr:-HW4QAggRauloj2SDLtIHN1XBkvhFZ1vtf1raYQp9TBW2RD5EEawDzbtSmlXUfnaHcvwOizhVYLtr7e6vw7NAf6mTuoCgmlkgnY0iXNlY3AyNTZrMaECjrXI8TLNXU0f8cthpAMxEshUyQlK-AM0PW2wfrnacNI\\nMHTDO6TMUBRIA2XWG5LUDACK24    86900   IN    TXT   enr:-HW4QLAYqmrwllBEnzWWs7I5Ev2IAs7x_dZlbYdRdMUx5EyKHDXp7AV5CkuPGUPdvbv1_Ms1CPfhcGCvSElSosZmyoqAgmlkgnY0iXNlY3AyNTZrMaECriawHKWdDRk2xeZkrOXBQ0dfMFLHY4eENZwdufn1S1o\\n```\\n\\nAll of this has already been introduced into go-ethereum with the pull request [#20094](https://github.com/ethereum/go-ethereum/pull/20094), created by Felix Lange. There\'s a lot of tooling around it that already exists too which is really cool. So if your project is written in Golang and wants to use this, it\'s relatively simple! Additionally, here\'s a proof of concept that shows what this might look like with libp2p on [github](https://github.com/decanus/dns-discovery).\\n\\nI hope this was a helpful explainer into DNS based discovery, and shows [EIP-1459](https://eips.ethereum.org/EIPS/eip-1459)\'s benefits over more traditional DNS-based discovery schemes."},{"id":"fixing-whisper-with-waku","metadata":{"permalink":"/rlog/fixing-whisper-with-waku","source":"@site/rlog/2019-12-03-fixing-whisper-with-waku.mdx","title":"Fixing Whisper with Waku","description":"A research log. Why Whisper doesn\'t scale and how to fix it.","date":"2019-12-03T12:00:00.000Z","formattedDate":"December 3, 2019","tags":[],"readingTime":9.995,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"Fixing Whisper with Waku","title":"Fixing Whisper with Waku","date":"2019-12-03T12:00:00.000Z","authors":"oskarth","published":true,"slug":"fixing-whisper-with-waku","categories":"research","image":"/img/whisper_scalability.png","discuss":"https://forum.vac.dev/t/discussion-fixing-whisper-with-waku/27"},"prevItem":{"title":"DNS Based Discovery","permalink":"/rlog/dns-based-discovery"},"nextItem":{"title":"Feasibility Study: Semaphore rate limiting through zkSNARKs","permalink":"/rlog/feasibility-semaphore-rate-limiting-zksnarks"}},"content":"A research log. Why Whisper doesn\'t scale and how to fix it.\\n\\n\x3c!--truncate--\x3e\\n\\nThis post will introduce Waku. Waku is a fork of Whisper that attempts to\\naddresses some of Whisper\'s shortcomings in an iterative fashion. We will also\\nintroduce a theoretical scaling model for Whisper that shows why it doesn\'t\\nscale, and what can be done about it.\\n\\n## Introduction\\n\\nWhisper is a gossip-based communication protocol or an ephemeral key-value store\\ndepending on which way you look at it. Historically speaking, it is the\\nmessaging pilllar of [Web3](http://gavwood.com/dappsweb3.html), together with\\nEthereum for consensus and Swarm for storage.\\n\\nWhisper, being a somewhat esoteric protocol and with some fundamental issues,\\nhasn\'t seen a lot of usage. However, applications such as Status are using it,\\nand have been making minor ad hoc modifications to it to make it run on mobile\\ndevices.\\n\\nWhat are these fundamental issues? In short:\\n\\n1. scalability, most immediately when it comes to bandwidth usage\\n2. spam-resistance, proof of work is a poor mechanism for heterogeneous nodes\\n3. no incentivized infrastructure, leading to centralized choke points\\n4. lack of formal and unambiguous specification makes it hard to analyze and implement\\n5. running over devp2p, which limits where it can run and how\\n\\nIn this post, we\'ll focus on the first problem, which is scalability through bandwidth usage.\\n\\n## Whisper theoretical scalability model\\n\\n_(Feel free to skip this section if you want to get right to the results)._\\n\\nThere\'s widespread implicit knowledge that Whisper \\"doesn\'t scale\\", but it is less understood exactly why. This theoretical model attempts to encode some characteristics of it. Specifically for use case such as one by Status (see [Status Whisper usage\\nspec](https://specs.status.im/spec/3)).\\n\\n### Caveats\\n\\nFirst, some caveats: this model likely contains bugs, has wrong assumptions, or completely misses certain dimensions. However, it acts as a form of existence proof for unscalability, with clear reasons.\\n\\nIf certain assumptions are wrong, then we can challenge them and reason about them in isolation. It doesn\u2019t mean things will definitely work as the model predicts, and that there aren\u2019t unknown unknowns.\\n\\nThe model also only deals with receiving bandwidth for end nodes, uses mostly static assumptions of averages, and doesn\u2019t deal with spam resistance, privacy guarantees, accounting, intermediate node or network wide failures.\\n\\n### Goals\\n\\n1. Ensure network scales by being user or usage bound, as opposed to bandwidth growing in proportion to network size.\\n2. Staying with in a reasonable bandwidth limit for limited data plans.\\n3. Do the above without materially impacting existing nodes.\\n\\nIt proceeds through various case with clear assumptions behind them, starting from the most naive assumptions. It shows results for 100 users, 10k users and 1m users.\\n\\n### Model\\n\\n```\\nCase 1. Only receiving messages meant for you [naive case]\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A4. Only receiving messages meant for you.\\n\\nFor 100 users, receiving bandwidth is 1000.0KB/day\\nFor 10k users, receiving bandwidth is 1000.0KB/day\\nFor  1m users, receiving bandwidth is 1000.0KB/day\\n\\n------------------------------------------------------------\\n\\nCase 2. Receiving messages for everyone [naive case]\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A5. Received messages for everyone.\\n\\nFor 100 users, receiving bandwidth is   97.7MB/day\\nFor 10k users, receiving bandwidth is    9.5GB/day\\nFor  1m users, receiving bandwidth is  953.7GB/day\\n\\n------------------------------------------------------------\\n\\nCase 3. All private messages go over one discovery topic [naive case]\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A6. Proportion of private messages (static): 0.5\\n- A7. Public messages only received by relevant recipients (static).\\n- A8. All private messages are received by everyone (same topic) (static).\\n\\nFor 100 users, receiving bandwidth is   49.3MB/day\\nFor 10k users, receiving bandwidth is    4.8GB/day\\nFor  1m users, receiving bandwidth is  476.8GB/day\\n\\n------------------------------------------------------------\\n\\nCase 4. All private messages are partitioned into shards [naive case]\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A6. Proportion of private messages (static): 0.5\\n- A7. Public messages only received by relevant recipients (static).\\n- A9. Private messages partitioned across partition shards (static), n=5000\\n\\nFor 100 users, receiving bandwidth is 1000.0KB/day\\nFor 10k users, receiving bandwidth is    1.5MB/day\\nFor  1m users, receiving bandwidth is   98.1MB/day\\n\\n------------------------------------------------------------\\n\\nCase 5. 4 + Bloom filter with false positive rate\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A6. Proportion of private messages (static): 0.5\\n- A7. Public messages only received by relevant recipients (static).\\n- A9. Private messages partitioned across partition shards (static), n=5000\\n- A10. Bloom filter size (m) (static): 512\\n- A11. Bloom filter hash functions (k) (static): 3\\n- A12. Bloom filter elements, i.e. topics, (n) (static): 100\\n- A13. Bloom filter assuming optimal k choice (sensitive to m, n).\\n- A14. Bloom filter false positive proportion of full traffic, p=0.1\\n\\nFor 100 users, receiving bandwidth is   10.7MB/day\\nFor 10k users, receiving bandwidth is  978.0MB/day\\nFor  1m users, receiving bandwidth is   95.5GB/day\\n\\nNOTE: Traffic extremely sensitive to bloom false positives\\nThis completely dominates network traffic at scale.\\nWith p=1% we get 10k users ~100MB/day and 1m users ~10gb/day)\\n\\n------------------------------------------------------------\\n\\nCase 6. Case 5 + Benign duplicate receives\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A6. Proportion of private messages (static): 0.5\\n- A7. Public messages only received by relevant recipients (static).\\n- A9. Private messages partitioned across partition shards (static), n=5000\\n- A10. Bloom filter size (m) (static): 512\\n- A11. Bloom filter hash functions (k) (static): 3\\n- A12. Bloom filter elements, i.e. topics, (n) (static): 100\\n- A13. Bloom filter assuming optimal k choice (sensitive to m, n).\\n- A14. Bloom filter false positive proportion of full traffic, p=0.1\\n- A15. Benign duplicate receives factor (static): 2\\n- A16. No bad envelopes, bad PoW, expired, etc (static).\\n\\nFor 100 users, receiving bandwidth is   21.5MB/day\\nFor 10k users, receiving bandwidth is    1.9GB/day\\nFor  1m users, receiving bandwidth is  190.9GB/day\\n\\n------------------------------------------------------------\\n\\nCase 7. 6 + Mailserver under good conditions; small bloom fp; mostly offline\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A6. Proportion of private messages (static): 0.5\\n- A7. Public messages only received by relevant recipients (static).\\n- A9. Private messages partitioned across partition shards (static), n=5000\\n- A10. Bloom filter size (m) (static): 512\\n- A11. Bloom filter hash functions (k) (static): 3\\n- A12. Bloom filter elements, i.e. topics, (n) (static): 100\\n- A13. Bloom filter assuming optimal k choice (sensitive to m, n).\\n- A14. Bloom filter false positive proportion of full traffic, p=0.1\\n- A15. Benign duplicate receives factor (static): 2\\n- A16. No bad envelopes, bad PoW, expired, etc (static).\\n- A17. User is offline p% of the time (static) p=0.9\\n- A18. No bad request, dup messages for mailservers; overlap perfect (static).\\n- A19. Mailserver requests can change false positive rate to be p=0.01\\n\\nFor 100 users, receiving bandwidth is    3.9MB/day\\nFor 10k users, receiving bandwidth is  284.8MB/day\\nFor  1m users, receiving bandwidth is   27.8GB/day\\n\\n------------------------------------------------------------\\n\\nCase 8. No metadata protection w bloom filter; 1 node connected; static shard\\n\\nAka waku mode.\\n\\nNext step up is to either only use contact code, or shard more aggressively.\\nNote that this requires change of other nodes behavior, not just local node.\\n\\nAssumptions:\\n- A1. Envelope size (static): 1024kb\\n- A2. Envelopes / message (static): 10\\n- A3. Received messages / day (static): 100\\n- A6. Proportion of private messages (static): 0.5\\n- A7. Public messages only received by relevant recipients (static).\\n- A9. Private messages partitioned across partition shards (static), n=5000\\n\\nFor 100 users, receiving bandwidth is 1000.0KB/day\\nFor 10k users, receiving bandwidth is    1.5MB/day\\nFor  1m users, receiving bandwidth is   98.1MB/day\\n\\n------------------------------------------------------------\\n```\\n\\nSee [source](https://github.com/vacp2p/research/tree/master/whisper_scalability)\\nfor more detail on the model and its assumptions.\\n\\n### Takeaways\\n\\n1. Whisper as it currently works doesn\u2019t scale, and we quickly run into unacceptable bandwidth usage.\\n2. There are a few factors of this, but largely it boils down to noisy topics usage and use of bloom filters. Duplicate (e.g. see [Whisper vs PSS](https://our.status.im/whisper-pss-comparison/)) and bad envelopes are also factors, but this depends a bit more on specific deployment configurations.\\n3. Waku mode (case 8) is an additional capability that doesn\u2019t require other nodes to change, for nodes that put a premium on performance.\\n4. The next bottleneck after this is the partitioned topics (app/network specific), which either needs to gracefully (and potentially quickly) grow, or an alternative way of consuming those messages needs to be deviced.\\n\\n![](/img/whisper_scalability.png)\\n\\nThe results are summarized in the graph above. Notice the log-log scale. The\\ncolored backgrounds correspond to the following bandwidth usage:\\n\\n- Blue: <10mb/d (<~300mb/month)\\n- Green: <30mb/d (<~1gb/month)\\n- Yellow: <100mb/d (<~3gb/month)\\n- Red: >100mb/d (>3gb/month)\\n\\nThese ranges are somewhat arbitrary, but are based on [user\\nrequirements](https://github.com/status-im/status-react/issues/9081) for users\\non a limited data plan, with comparable usage for other messaging apps.\\n\\n## Introducing Waku\\n\\n### Motivation for a new protocol\\n\\nApps such as Status will likely use something like Whisper for the forseeable\\nfuture, and we want to enable them to use it with more users on mobile devices\\nwithout bandwidth exploding with minimal changes.\\n\\nAdditionally, there\'s not a clear cut alternative that maps cleanly to the\\ndesired use cases (p2p, multicast, privacy-preserving, open, etc).\\n\\nWe are actively researching, developing and collaborating with more greenfield\\napproaches. It is likely that Waku will either converge to those, or Waku will\\nlay the groundwork (clear specs, common issues/components) necessary to make\\nswitching to another protocol easier. In this project we want to emphasize\\niterative work with results on the order of weeks.\\n\\n### Briefly on Waku mode\\n\\n- Doesn\u2019t impact existing clients, it\u2019s just a separate node and capability.\\n- Other nodes can still use Whisper as is, like a full node.\\n- Sacrifices metadata protection and incurs higher connectivity/availability requirements for scalbility\\n\\n**Requirements:**\\n\\n- Exposes API to get messages from a set of list of topics (no bloom filter)\\n- Way of being identified as a Waku node (e.g. through version string)\\n- Option to statically encode this node in app, e.g. similar to custom bootnodes/mailserver\\n- Only node that needs to be connected to, possibly as Whisper relay / mailserver hybrid\\n\\n**Provides:**\\n\\n- likely provides scalability of up to 10k users and beyond\\n- with some enhancements to partition topic logic, can possibly scale up to 1m users (app/network specific)\\n\\n**Caveats:**\\n\\n- hasn\u2019t been tested in a large-scale simulation\\n- other network and intermediate node bottlenecks might become apparent (e.g. full bloom filter and private cluster capacity; can likely be dealt with in isolation using known techniques, e.g. load balancing) (deployment specific)\\n\\n### Progress so far\\n\\nIn short, we have a [Waku version 0 spec up](https://rfc.vac.dev/waku/deprecated/5/waku0) as well as a [PoC](https://github.com/status-im/nim-eth/pull/120) for backwards compatibility. In the coming weeks, we are going to solidify the specs, get a more fully featured PoC for [Waku mode](https://github.com/status-im/nim-eth/pull/114). See [rough roadmap](https://github.com/vacp2p/pm/issues/5), project board [link deprecated] and progress thread on the [Vac forum](https://forum.vac.dev/t/waku-project-and-progress/24).\\n\\nThe spec has been rewrittten for clarity, with ABNF grammar and less ambiguous language. The spec also incorporates several previously [ad hoc implemented features](https://rfc.vac.dev/waku/standards/legacy/6/waku1/#additional-capabilities), such as light nodes and mailserver/client support. This has already caught a few incompatibilities between the `geth` (Go), `status/whisper` (Go) and `nim-eth` (Nim) versions, specifically around light node usage and the handshake.\\n\\nIf you are interested in this effort, please check out [our forum](https://forum.vac.dev/) for questions, comments and proposals. We already have some discussion for better [spam protection](https://forum.vac.dev/t/stake-priority-based-queuing/26) (see [previous post](https://vac.dev/feasibility-semaphore-rate-limiting-zksnarks) for a more complex but privacy-preserving proposal), something that is likely going to be addressed in future versions of Waku, along with many other fixes and enhancement."},{"id":"feasibility-semaphore-rate-limiting-zksnarks","metadata":{"permalink":"/rlog/feasibility-semaphore-rate-limiting-zksnarks","source":"@site/rlog/2019-11-08-feasibility-semaphore-rate-limiting-zksnarks.mdx","title":"Feasibility Study: Semaphore rate limiting through zkSNARKs","description":"A research log. Zero knowledge signaling as a rate limiting mechanism to prevent spam in p2p networks.","date":"2019-11-08T12:00:00.000Z","formattedDate":"November 8, 2019","tags":[],"readingTime":7.54,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"Feasibility Study: Semaphore rate limiting through zkSNARKs","title":"Feasibility Study: Semaphore rate limiting through zkSNARKs","date":"2019-11-08T12:00:00.000Z","authors":"oskarth","published":true,"slug":"feasibility-semaphore-rate-limiting-zksnarks","categories":"research","image":"/img/peacock-signaling.jpg","discuss":"https://forum.vac.dev/t/discussion-feasibility-study-semaphore-rate-limiting-through-zksnarks/21","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Fixing Whisper with Waku","permalink":"/rlog/fixing-whisper-with-waku"},"nextItem":{"title":"P2P Data Sync with a Remote Log","permalink":"/rlog/remote-log"}},"content":"A research log. Zero knowledge signaling as a rate limiting mechanism to prevent spam in p2p networks.\\n\\n\x3c!--truncate--\x3e\\n\\n**tldr: Moon math promising for solving spam in Whisper, but to get there we need to invest more in performance work and technical upskilling.**\\n\\n## Motivating problem\\n\\nIn open p2p networks for messaging, one big problem is spam-resistance. Existing solutions, such as Whisper\'s proof of work, are insufficient, especially for heterogeneous nodes. Other reputation-based approaches might not be desirable, due to issues around arbitrary exclusion and privacy.\\n\\nOne possible solution is to use a right-to-access staking-based method, where a node is only able to send a message, signal, at a certain rate, and otherwise they can be slashed. One problem with this is in terms of privacy-preservation, where we specifically don\'t want a user to be tied to a specific payment or unique fingerprint.\\n\\n### Related problems\\n\\nIn addition to above, there are a lot of related problems that share similarities in terms of their structure and proposed solution.\\n\\n- Private transactions ([Zcash](https://z.cash/), [AZTEC](https://www.aztecprotocol.com/))\\n- Private voting ([Semaphore](https://github.com/kobigurk/semaphore))\\n- Private group membership (Semaphore)\\n- Layer 2 scaling, poss layer 1 ([ZK Rollup](https://ethresear.ch/t/on-chain-scaling-to-potentially-500-tx-sec-through-mass-tx-validation/3477); StarkWare/Eth2-3)\\n\\n## Overview\\n\\n## Basic terminology\\n\\nA _zero-knowledge proof_ allows a _prover_ to show a _verifier_ that they know something, without revealing what that something is. This means you can do trust-minimized computation that is also privacy preserving. As a basic example, instead of showing your ID when going to a bar you simply give them a proof that you are over 18, without showing the doorman your id.\\n\\n_zkSNARKs_ is a form of zero-knowledge proofs. There are many types of zero-knowledge proofs, and the field is evolving rapidly. They come with various trade-offs in terms of things such as: trusted setup, cryptographic assumptions, proof/verification key size, proof/verification time, proof size, etc. See section below for more.\\n\\n_Semaphore_ is a framework/library/construct on top of zkSNARks. It allows for zero-knowledge signaling, specifically on top of Ethereum. This means an approved user can broadcast some arbitrary string without revealing their identity, given some specific constraints. An approved user is someone who has been added to a certain merkle tree. See [current Github home](https://github.com/kobigurk/semaphore) for more.\\n\\n_Circom_ is a DSL for writing arithmetic circuits that can be used in zkSNARKs, similar to how you might write a NAND gate. See [Github](https://github.com/iden3/circom) for more.\\n\\n## Basic flow\\n\\nWe start with a private voting example, and then extend it to the slashable rate limiting example.\\n\\n1. A user registers an identity (arbitrary keypair), along with a small fee, to a smart contract. This adds them to a merkle tree and allows them to prove that they are member of that group, without revealing who they are.\\n\\n2. When a user wants to send a message, they compute a zero-knowledge proof. This ensures certain invariants, have some _public outputs_, and can be verified by anyone (including a smart contract).\\n3. Any node can verify the proof, including smart contracts on chain (as of Byzantinum HF). Additionally, a node can have rules for the public output. In the case of voting, one such rule is that a specific output hash has to be equal to some predefined value, such as \\"2020-01-01 vote on Foo Bar for president\\".\\n4. Because of how the proof is constructed, and the rules around output values, this ensures that: a user is part of the approved set of voters and that a user can only vote once.\\n5. As a consequence of above, we have a system where registered users can only vote once, no one can see who voted for what, and this can all be proven and verified.\\n\\n### Rate limiting example\\n\\nIn the case of rate limiting, we do want nodes to send multiple messages. This changes step 3-5 above somewhat.\\n\\n_NOTE: It is a bit more involved than this, and if we precompute proofs the flow might look a bit different. But the general idea is the same_.\\n\\n1. Instead of having a rule that you can only vote once, we have a rule that you can only send a message per epoch. Epoch here can be every second, as defined by UTC date time +-20s.\\n2. Additionally, if a users sends more than one message per epoch, one of the public outputs is a random share of a private key. Using Shamir\'s Secret Sharing (similar to a multisig) and 2/3 key share as an example threshold: in the normal case only 1/3 private keys is revealed, which is insufficient to have access. In the case where two messages are sent in an epoch, probabilistically 2/3 shares is sufficient to have access to the key (unless you get the same random share of the key).\\n3. This means any untrusted user who detects a spamming user, can use it to access their private key corresponding to funds in the contract, and thus slash them.\\n\\n4. As a consequence of above, we have a system where registered users can only messages X times per epoch, and no one can see who is sending what messages. Additionally, if a user is violating the above rate limit, they can be punished and any user can profit from it.\\n\\n### Briefly on scope of \'approved users\'\\n\\nIn the case of an application like Status, this construct can either be a global StatusNetwork group, or one per chat, or network, etc. It can be applied both at the network and user level. There are no specific limitations on where or who deploys this, and it is thus more of a UX consideration.\\n\\n## Technical details\\n\\nFor a fairly self-contained set of examples above, see exploration in [Vac research repo](https://github.com/vacp2p/research/blob/master/zksnarks/semaphore/src/hello.js). Note that the Shamir secret sharing is not inside the SNARK, but out-of-band for now.\\n\\nThe [current version](https://github.com/kobigurk/semaphore) of Semaphore is using NodeJS and [Circom](https://github.com/iden3/circom) from Iden3 for Snarks.\\n\\nFor more on rate limiting idea, see [ethresearch post](https://ethresear.ch/t/semaphore-rln-rate-limiting-nullifier-for-spam-prevention-in-anonymous-p2p-setting/5009/).\\n\\n## Feasibility\\n\\nThe above repo was used to exercise the basic paths and to gain intution of feasibility. Based on it and related reading we outline a few blockers and things that require further study.\\n\\n### Technical feasibility\\n\\n#### Proof time\\n\\nProve time for Semaphore (<https://github.com/kobigurk/semaphore>) zKSNARKs using circom, groth and snarkjs is currently way too long. It takes on the order of ~10m to generate a proof. With Websnark, it is likely to take 30s, which might still be too long. We should experiment with native code on mobile here.\\n\\nSee [details](https://github.com/vacp2p/research/issues/7).\\n\\n#### Proving key size\\n\\nProver key size is ~110mb for Semaphore. Assuming this is embedded on mobile device, it bloats the APK a lot. Current APK size is ~30mb and even that might be high for people with limited bandwidth.\\n\\nSee [details](https://github.com/vacp2p/research/issues/8).\\n\\n#### Trusted setup\\n\\nUsing zkSNARKs a trusted setup is required to generate prover and verifier keys. As part of this setup, a toxic parameter lambda is generated. If a party gets access to this lambda, they can prove anything. This means people using zKSNARKs usually have an elaborate MPC ceremony to ensure this parameter doesn\'t get discovered.\\n\\nSee [details](https://github.com/vacp2p/research/issues/9).\\n\\n#### Shamir logic in SNARK\\n\\nFor [Semaphore RLN](https://ethresear.ch/t/semaphore-rln-rate-limiting-nullifier-for-spam-prevention-in-anonymous-p2p-setting/5009) we need to embed the Shamir logic inside the SNARK in order to do slashing for spam. Currently the [implementation](https://github.com/vacp2p/research/blob/master/zksnarks/semaphore/src/hello.js#L450) is trusted and very hacky.\\n\\nSee [details](https://github.com/vacp2p/research/issues/10).\\n\\n#### End to end integation\\n\\n[Currently](https://github.com/vacp2p/research/blob/master/zksnarks/semaphore/src/hello.js) is standalone and doesn\'t touch multiple users, deployed contract with merkle tree and verification, actual transactions, a mocked network, add/remove members, etc. There are bound to be edge cases and unknown unknowns here.\\n\\nSee [details](https://github.com/vacp2p/research/issues/11).\\n\\n#### Licensing issues\\n\\nCurrently Circom [uses a GPL license](https://github.com/iden3/circom/blob/master/COPYING), which can get tricky when it comes to the App Store etc.\\n\\nSee [details](https://github.com/vacp2p/research/issues/12).\\n\\n#### Alternative ZKPs?\\n\\nSome of the isolated blockers for zKSNARKs ([#7](https://github.com/vacp2p/research/issues/7), [#8](https://github.com/vacp2p/research/issues/8), [#9](https://github.com/vacp2p/research/issues/9)) might be mitigated by the use of other ZKP technology. However, they likely have their own issues.\\n\\nSee [details](https://github.com/vacp2p/research/issues/13).\\n\\n### Social feasibility\\n\\n#### Technical skill\\n\\nzkSNARKs and related technologies are quite new. To learn how they work and get an intuition for them requires individuals to dedicate a lot of time to studying them. This means we must make getting competence in these technologies if we wish to use them to our advantage.\\n\\n#### Time and resources\\n\\nIn order for this and related projects (such as private transaction) to get anywhere, it must be made an explicit area of focus for an extend period of time.\\n\\n## General thoughts\\n\\nSimilar to Whisper, and in line with moving towards protocol and infrastructure, we need to upskill and invest resources into this. This doesn\'t mean developing all of the technologies ourselves, but gaining enough competence to leverage and extend existing solutions by the growing ZKP community.\\n\\nFor example, this might also include leveraging largely ready made solutions such as AZTEC for private transaction; more fundamental research into ZK rollup and similar; using Semaphore for private group membership and private voting; Nim based wrapper aronud Bellman, etc.\\n\\n## Acknowledgement\\n\\nThanks to Barry Whitehat for patient explanation and pointers. Thanks to WJ for helping with runtime issues.\\n\\n_Peacock header image from [Tonos](<https://en.wikipedia.org/wiki/File:Flickr_-_lo.tangelini_-_Tonos_(1).jpg>).\\\\_"},{"id":"remote-log","metadata":{"permalink":"/rlog/remote-log","source":"@site/rlog/2019-10-04-remote-log.mdx","title":"P2P Data Sync with a Remote Log","description":"A research log. Asynchronous P2P messaging? Remote logs to the rescue!","date":"2019-10-04T12:00:00.000Z","formattedDate":"October 4, 2019","tags":[],"readingTime":4.515,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"P2P Data Sync with a Remote Log","title":"P2P Data Sync with a Remote Log","date":"2019-10-04T12:00:00.000Z","authors":"oskarth","published":true,"slug":"remote-log","categories":"research","summary":null,"image":"/img/remote-log.png"},"prevItem":{"title":"Feasibility Study: Semaphore rate limiting through zkSNARKs","permalink":"/rlog/feasibility-semaphore-rate-limiting-zksnarks"},"nextItem":{"title":"Vac - A Rough Overview","permalink":"/rlog/vac-overview"}},"content":"A research log. Asynchronous P2P messaging? Remote logs to the rescue!\\n\\n\x3c!--truncate--\x3e\\n\\nA big problem when doing end-to-end data sync between mobile nodes is that most devices are offline most of the time. With a naive approach, you quickly run into issues of \'ping-pong\' behavior, where messages have to be constantly retransmitted. We saw some basic calculations of what this bandwidth multiplier looks like in a [previous post](https://vac.dev/p2p-data-sync-for-mobile).\\n\\nWhile you could do some background processing, this is really battery-draining, and on iOS these capabilities are limited. A better approach instead is to loosen the constraint that two nodes need to be online at the same time. How do we do this? There are two main approaches, one is the _store and forward model_, and the other is a _remote log_.\\n\\nIn the _store and forward_ model, we use an intermediate node that forward messages on behalf of the recipient. In the _remote log_ model, you instead replicate the data onto some decentralized storage, and have a mutable reference to the latest state, similar to DNS. While both work, the latter is somewhat more elegant and \\"pure\\", as it has less strict requirements of an individual node\'s uptime. Both act as a highly-available cache to smoothen over non-overlapping connection windows between endpoints.\\n\\nIn this post we are going to describe how such a remote log schema could work. Specifically, how it enhances p2p data sync and takes care of the [following requirements](https://vac.dev/p2p-data-sync-for-mobile):\\n\\n> 3. MUST allow for mobile-friendly usage. By mobile-friendly we mean devices\\n>    that are resource restricted, mostly-offline and often changing network.\\n\\n> 4. MAY use helper services in order to be more mobile-friendly. Examples of\\n>    helper services are decentralized file storage solutions such as IPFS and\\n>    Swarm. These help with availability and latency of data for mostly-offline\\n>    devices.\\n\\n## Remote log\\n\\nA remote log is a replication of a local log. This means a node can read data from a node that is offline.\\n\\nThe spec is in an early draft stage and can be found [here](https://github.com/vacp2p/specs/pull/16). A very basic [spike](<https://en.wikipedia.org/wiki/Spike_(software_development)>) / proof-of-concept can be found [here](https://github.com/vacp2p/research/tree/master/remote_log).\\n\\n### Definitions\\n\\n| Term       | Definition                                                                |\\n| ---------- | ------------------------------------------------------------------------- |\\n| CAS        | Content-addressed storage. Stores data that can be addressed by its hash. |\\n| NS         | Name system. Associates mutable data to a name.                           |\\n| Remote log | Replication of a local log at a different location.                       |\\n\\n### Roles\\n\\nThere are four fundamental roles:\\n\\n1. Alice\\n2. Bob\\n3. Name system (NS)\\n4. Content-addressed storage (CAS)\\n\\nThe _remote log_ is the data format of what is stored in the name system.\\n\\n\\"Bob\\" can represent anything from 0 to N participants. Unlike Alice, Bob only needs read-only access to NS and CAS.\\n\\n### Flow\\n\\n![Figure 1: Remote log data synchronization.](/img/remote-log.png)\\n\\n### Data format\\n\\nThe remote log lets receiving nodes know what data they are missing. Depending on the specific requirements and capabilities of the nodes and name system, the information can be referred to differently. We distinguish between three rough modes:\\n\\n1. Fully replicated log\\n2. Normal sized page with CAS mapping\\n3. \\"Linked list\\" mode - minimally sized page with CAS mapping\\n\\nA remote log is simply a mapping from message identifiers to their corresponding address in a CAS:\\n\\n| Message Identifier (H1) | CAS Hash (H2) |\\n| ----------------------- | ------------- |\\n| H1_3                    | H2_3          |\\n| H1_2                    | H2_2          |\\n| H1_1                    | H2_1          |\\n|                         |               |\\n| _address to next page_  |\\n\\nThe numbers here corresponds to messages. Optionally, the content itself can be included, just like it normally would be sent over the wire. This bypasses the need for a dedicated CAS and additional round-trips, with a trade-off in bandwidth usage.\\n\\n| Message Identifier (H1) | Content |\\n| ----------------------- | ------- |\\n| H1_3                    | C3      |\\n| H1_2                    | C2      |\\n| H1_1                    | C1      |\\n|                         |         |\\n| _address to next page_  |\\n\\nBoth patterns can be used in parallel, e,g. by storing the last `k` messages directly and use CAS pointers for the rest. Together with the `next_page` page semantics, this gives users flexibility in terms of bandwidth and latency/indirection, all the way from a simple linked list to a fully replicated log. The latter is useful for things like backups on durable storage.\\n\\n### Interaction with MVDS\\n\\n[vac.mvds.Message](https://rfc.vac.dev/vac/2/mvds/#payloads) payloads are the only payloads that MUST be uploaded. Other messages types MAY be uploaded, depending on the implementation.\\n\\n## Future work\\n\\nThe spec is still in an early draft stage, so it is expected to change. Same with the proof of concept. More work is needed on getting a fully featured proof of concept with specific CAS and NAS instances. E.g. Swarm and Swarm Feeds, or IPFS and IPNS, or something else.\\n\\nFor data sync in general:\\n\\n- Make consistency guarantees more explicit for app developers with support for sequence numbers and DAGs, as well as the ability to send non-synced messages. E.g. ephemeral typing notifications, linear/sequential history and casual consistency/DAG history\\n- Better semantics and scalability for multi-user sync contexts, e.g. CRDTs and joining multiple logs together\\n- Better usability in terms of application layer usage (data sync clients) and supporting more transports\\n\\n---\\n\\nPS1. Thanks everyone who submitted great [logo proposals](https://explorer.bounties.network/bounty/3389) for Vac!\\n\\nPPS2. Next week on October 10th decanus and I will be presenting Vac at [Devcon](https://devcon.org/agenda), come say hi :)"},{"id":"vac-overview","metadata":{"permalink":"/rlog/vac-overview","source":"@site/rlog/2019-08-02-vac-overview.mdx","title":"Vac - A Rough Overview","description":"Vac is a modular peer-to-peer messaging stack, with a focus on secure messaging. Overview of terms, stack and open problems.","date":"2019-08-02T12:00:00.000Z","formattedDate":"August 2, 2019","tags":[],"readingTime":5.535,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"layout":"post","name":"Vac - A Rough Overview","title":"Vac - A Rough Overview","date":"2019-08-02T12:00:00.000Z","authors":"oskarth","published":true,"slug":"vac-overview","categories":"research"},"prevItem":{"title":"P2P Data Sync with a Remote Log","permalink":"/rlog/remote-log"},"nextItem":{"title":"P2P Data Sync for Mobile","permalink":"/rlog/p2p-data-sync-for-mobile"}},"content":"Vac is a modular peer-to-peer messaging stack, with a focus on secure messaging. Overview of terms, stack and open problems.\\n\\n\x3c!--truncate--\x3e\\n\\nVac is a **modular peer-to-peer messaging stack, with a focus on secure messaging**. What does that mean? Let\'s unpack it a bit.\\n\\n## Basic terms\\n\\n_messaging stack_. While the initial focus is on [data sync](https://vac.dev/p2p-data-sync-for-mobile), we are concerned with all layers in the stack. That means all the way from underlying transports, p2p overlays and routing, to initial trust establishment and semantics for things like group chat. The ultimate goal is to give application developers the tools they need to provide secure messaging for their users, so they can focus on their domain expertise.\\n\\n_modular_. Unlike many other secure messaging applications, our goal is not to have a tightly coupled set of protocols, nor is it to reinvent the wheel. Instead, we aim to provide options at each layer in the stack, and build on the shoulders of giants, putting a premimum on interoperability. It\'s similar in philosophy to projects such as [libp2p](https://libp2p.io/) or [Substrate](https://www.parity.io/substrate/) in that regard. Each choice comes with different trade-offs, and these look different for different applications.\\n\\n_peer-to-peer_. The protocols we work on are pure p2p, and aim to minimize centralization. This too is in opposition to many initiatives in the secure messaging space.\\n\\n_messaging_. By messaging we mean messaging in a generalized sense. This includes both human to human communication, as well machine to machine communication. By messaging we also mean something more fundamental than text messages, we also include things like transactions (state channels, etc) under this moniker.\\n\\n_secure messaging_. Outside of traditional notions of secure messaging, such as ensuring end to end encryption, forward secrecy, avoiding MITM-attacks, etc, we are also concerned with two other forms of secure messaging. We call these _private messaging_ and _censorship-resistance_. Private messaging means viewing privacy as a security property, with all that entails. Censorship resistance ties into being p2p, but also in terms of allowing for transports and overlays that can\'t easily be censored by port blocking, traffic analysis, and similar.\\n\\n_V\u0101c_. Is a Vedic goddess of speech. It also hints at being a vaccine.\\n\\n## Protocol stack\\n\\nWhat does this stack look like? We take inspiration from [core](https://tools.ietf.org/html/rfc793) [internet architecture](https://www.ietf.org/rfc/rfc1122.txt), existing [survey work](https://css.csail.mit.edu/6.858/2020/readings/secure-messaging.pdf) and other [efforts](https://code.briarproject.org/briar/briar/wikis/A-Quick-Overview-of-the-Protocol-Stack) that have been done to decompose the problem into orthogonal pieces. Each layer provides their own set of properties and only interact with the layers it is adjacent to. Note that this is a rough sketch.\\n\\n| Layer / Protocol    | Purpose                           | Examples             |\\n| ------------------- | --------------------------------- | -------------------- |\\n| Application layer   | End user semantics                | 1:1 chat, group chat |\\n| Data Sync           | Data consistency                  | MVDS, BSP            |\\n| Secure Transport    | Confidentiality, PFS, etc         | Double Ratchet, MLS  |\\n| Transport Privacy   | Transport and metadata protection | Whisper, Tor, Mixnet |\\n| P2P Overlay         | Overlay routing, NAT traversal    | devp2p, libp2p       |\\n|                     |                                   |\\n| Trust Establishment | Establishing end-to-end trust     | TOFU, web of trust   |\\n\\nAs an example, end user semantics such as group chat or moderation capabilities can largely work regardless of specific choices further down the stack. Similarly, using a mesh network or Tor doesn\'t impact the use of Double Ratchet at the Secure Transport layer.\\n\\nData Sync plays a similar role to what TCP does at the transport layer in a traditional Internet architecture, and for some applications something more like UDP is likely to be desirable.\\n\\nIn terms of specific properties and trade-offs at each layer, we\'ll go deeper down into them as we study them. For now, this is best treated as a rough sketch or mental map.\\n\\n## Problems and rough priorities\\n\\nWith all the pieces involved, this is quite an undertaking. Luckily, a lot of pieces are already in place and can be either incorporated as-is or iterated on. In terms of medium and long term, here\'s a rough sketch of priorities and open problems.\\n\\n1. **Better data sync.** While the current [MVDS](https://rfc.vac.dev/vac/2/mvds/) works, it is lacking in a few areas:\\n\\n- Lack of remote log for mostly-offline offline devices\\n- Better scalability for multi-user chat contexts\\n- Better usability in terms of application layer usage and supporting more transports\\n\\n2. **Better transport layer support.** Currently MVDS runs primarily over Whisper, which has a few issues:\\n\\n- scalability, being able to run with many nodes\\n- spam-resistance, proof of work is a poor mechanism for heterogeneous devices\\n- no incentivized infrastructure, leading to centralized choke points\\n\\nIn addition to these most immediate concerns, there are other open problems. Some of these are overlapping with the above.\\n\\n3. **Adaptive nodes.** Better support for resource restricted devices and nodes of varying capabilities. Light connection strategy for resources and guarantees. Security games to outsource processing with guarantees.\\n\\n4. **Incentivized and spam-resistant messaging.** Reasons to run infrastructure and not relying on altruistic nodes. For spam resistance, in p2p multicast spam is a big attack vector due to amplification. There are a few interesting directions here, such as EigenTrust, proof of burn with micropayments, and leveraging zero-knowledge proofs.\\n\\n5. **Strong privacy guarantees at transport privacy layer**. More rigorous privacy guarantees and explicit trade-offs for metadata protection. Includes Mixnet.\\n6. **Censorship-resistant and robust P2P overlay**. NAT traversal; running in the browser; mesh networks; pluggable transports for traffic obfuscation.\\n\\n7. **Scalable and decentralized secure conversational security.** Strong security guarantees such as forward secrecy, post compromise security, for large group chats. Includes projects such MLS and extending Double Ratchet.\\n\\n8. **Better trust establishment and key handling**. Avoiding MITM attacks while still enabling a good user experience. Protecting against ghost users in group chat and providing better ways to do key handling.\\n\\nThere is also a set of more general problems, that touch multiple layers:\\n\\n9. **Ensuring modularity and interoperability**. Providing interfaces that allow for existing and new protocols to be at each layer of the stack.\\n\\n10. **Better specifications**. Machine-readable and formally verified specifications. More rigorous analysis of exact guarantees and behaviors. Exposing work in such a way that it can be analyzed by academics.\\n\\n11. **Better simulations**. Providing infrastructure and tooling to be able to test protocols in adverse environments and at scale.\\n\\n12. **Enabling excellent user experience**. A big reason for the lack of widespread adoption of secure messaging is the fact that more centralized, insecure methods provide a better user experience. Given that incentives can align better for users interested in secure messaging, providing an even better user experience should be doable.\\n\\n---\\n\\nWe got some work to do. Come help us if you want. See you in the next update!"},{"id":"p2p-data-sync-for-mobile","metadata":{"permalink":"/rlog/p2p-data-sync-for-mobile","source":"@site/rlog/2019-07-19-p2p-data-sync-for-mobile.mdx","title":"P2P Data Sync for Mobile","description":"A research log. Reliable and decentralized, pick two.","date":"2019-07-19T12:00:00.000Z","formattedDate":"July 19, 2019","tags":[],"readingTime":11.01,"hasTruncateMarker":true,"authors":[{"name":"Oskar","twitter":"oskarth","github":"oskarth","key":"oskarth"}],"frontMatter":{"title":"P2P Data Sync for Mobile","date":"2019-07-19T12:00:00.000Z","authors":"oskarth","published":true,"slug":"p2p-data-sync-for-mobile","categories":"research","image":"/img/mvds_interactive.png","toc_min_heading_level":2,"toc_max_heading_level":5},"prevItem":{"title":"Vac - A Rough Overview","permalink":"/rlog/vac-overview"}},"content":"A research log. Reliable and decentralized, pick two.\\n\\n\x3c!--truncate--\x3e\\n\\nTogether with decanus, I\'ve been working on the problem of data sync lately.\\n\\nIn building p2p messaging systems, one problem you quickly come across is the problem of reliably transmitting data. If there\'s no central server with high availability guarantees, you can\'t meaningfully guarantee that data has been transmitted. One way of solving this problem is through a synchronization protocol.\\n\\nThere are many synchronization protocols out there and I won\'t go into detail of how they differ with our approach here. Some common examples are Git and Bittorrent, but there are also projects like IPFS, Swarm, Dispersy, Matrix, Briar, SSB, etc.\\n\\n## Problem motivation\\n\\nWhy do we want to do p2p sync for mobilephones in the first place? There are three components to that question. One is on the value of decentralization and peer-to-peer, the second is on why we\'d want to reliably sync data at all, and finally why mobilephones and other resource restricted devices.\\n\\n### Why p2p?\\n\\nFor decentralization and p2p, there are both technical and social/philosophical reasons. Technically, having a user-run network means it can scale with the number of users. Data locality is also improved if you query data that\'s close to you, similar to distributed CDNs. The throughput is also improved if there are more places to get data from.\\n\\nSocially and philosophically, there are several ways to think about it. Open and decentralized networks also relate to the idea of open standards, i.e. compare the longevity of AOL with IRC or Bittorrent. One is run by a company and is shut down as soon as it stops being profitable, the others live on. Additionally increasingly control of data and infrastructure is becoming a liability. By having a network with no one in control, everyone is. It\'s ultimately a form of democratization, more similar to organic social structures pre Big Internet companies. This leads to properties such as censorship resistance and coercion resistance, where we limit the impact a 3rd party might have a voluntary interaction between individuals or a group of people. Examples of this are plentiful in the world of Facebook, Youtube, Twitter and WeChat.\\n\\n### Why reliably sync data?\\n\\nAt risk of stating the obvious, reliably syncing data is a requirement for many problem domains. You don\'t get this by default in a p2p world, as it is unreliable with nodes permissionslessly join and leave the network. In some cases you can get away with only ephemeral data, but usually you want some kind of guarantees. This is a must for reliable group chat experience, for example, where messages are expected to arrive in a timely fashion and in some reasonable order. The same is true for messages there represent financial transactions, and so on.\\n\\n### Why mobilephones?\\n\\nMost devices people use daily are mobile phones. It\'s important to provide the same or at least similar guarantees to more traditional p2p nodes that might run on a desktop computer or computer. The alternative is to rely on gateways, which shares many of the drawbacks of centralized control and prone to censorship, control and surveillence.\\n\\nMore generally, resource restricted devices can differ in their capabilities. One example is smartphones, but others are: desktop, routers, Raspberry PIs, POS systems, and so on. The number and diversity of devices are exploding, and it\'s useful to be able to leverage this for various types of infrastructure. The alternative is to centralize on big cloud providers, which also lends itself to lack of democratization and censorship, etc.\\n\\n## Minimal Requirements\\n\\nFor requirements or design goals for a solution, here\'s what we came up with.\\n\\n1. MUST sync data reliably between devices. By reliably we mean having the ability to deal with messages being out of order, dropped, duplicated, or delayed.\\n\\n2. MUST NOT rely on any centralized services for reliability. By centralized services we mean any single point of failure that isn\u2019t one of the endpoint devices.\\n\\n3. MUST allow for mobile-friendly usage. By mobile-friendly we mean devices that are resource restricted, mostly-offline and often changing network.\\n\\n4. MAY use helper services in order to be more mobile-friendly. Examples of helper services are decentralized file storage solutions such as IPFS and Swarm. These help with availability and latency of data for mostly-offline devices.\\n\\n5. MUST have the ability to provide casual consistency. By casual consistency we mean the commonly accepted definition in distributed systems literature. This means messages that are casually related can achieve a partial ordering.\\n\\n6. MUST support ephemeral messages that don\u2019t need replication. That is, allow for messages that don\u2019t need to be reliabily transmitted but still needs to be transmitted between devices.\\n\\n7. MUST allow for privacy-preserving messages and extreme data loss. By privacy-preserving we mean things such as exploding messages (self-destructing messages). By extreme data loss we mean the ability for two trusted devices to recover from a, deliberate or accidental, removal of data.\\n\\n8. MUST be agnostic to whatever transport it is running on. It should not rely on specific semantics of the transport it is running on, nor be tightly coupled with it. This means a transport can be swapped out without loss of reliability between devices.\\n\\n## MVDS - a minimium viable version\\n\\nThe first minimum viable version is in an alpha stage, and it has a [specification](https://rfc.vac.dev/vac/2/mvds/), [implementation](https://github.com/vacp2p/mvds) and we have deployed it in a [console client](https://github.com/status-im/status-console-client) for end to end functionality. It\'s heavily inspired by [Bramble Sync Protocol](https://code.briarproject.org/briar/briar-spec/blob/master/protocols/BSP.md).\\n\\nThe spec is fairly minimal. You have nodes that exchange records over some secure transport. These records are of different types, such as `OFFER`, `MESSAGE`, `REQUEST`, and `ACK`. A peer keep tracks of the state of message for each node it is interacting with. There\'s also logic for message retransmission with exponential delay. The positive ACK and retransmission model is quite similar to how TCP is designed.\\n\\nThere are two different modes of syncing, interactive and batch mode. See sequence diagrams below.\\n\\nInteractive mode:\\n![Interactive mode](/img/mvds_interactive.png)\\n\\nBatch mode:\\n![Batch mode](/img/mvds_batch.png)\\n\\nWhich mode should you choose? It\'s a tradeoff of latency and bandwidth. If you want to minimize latency, batch mode is better. If you care about preserving bandwidth interactive mode is better. The choice is up to each node.\\n\\n### Basic simulation\\n\\nInitial ad hoc bandwidth and latency testing shows some issues with a naive approach. Running with the [default simulation settings](https://github.com/vacp2p/mvds/):\\n\\n- communicating nodes: 2\\n- nodes using interactive mode: 2\\n- interval between messages: 5s\\n- time node is offine: 90%\\n- nodes each node is sharing with: 2\\n\\nwe notice a [huge overhead](https://notes.status.im/7QYa4b6bTH2wMk3HfAaU0w#). More specifically, we see a ~5 minute latency overhead and a bandwidth multiplier of x100-1000, i.e. 2-3 orders of magnitude just for receiving a message with interactive mode, without acks.\\n\\nNow, that seems terrible. A moment of reflection will reveal why that is. If each node is offline uniformly 90% of the time, that means that each record will be lost 90% of the time. Since interactive mode requires offer, request, payload (and then ack), that\'s three links just for Bob to receive the actual message.\\n\\nEach failed attempt implies another retransmission. That means we have `(1/0.1)^3 = 1000` expected overhead to receive a message in interactive mode. The latency follows naturally from that, with the retransmission logic.\\n\\n### Mostly-offline devices\\n\\nThe problem above hints at the requirements 3 and 4 above. While we did get reliable syncing (requirement 1), it came at a big cost.\\n\\nThere are a few ways of getting around this issue. One is having a _store and forward_ model, where some intermediary node picks up (encrypted) messages and forwards them to the recipient. This is what we have in production right now at Status.\\n\\nAnother, arguably more pure and robust, way is having a _remote log_, where the actual data is spread over some decentralized storage layer, and you have a mutable reference to find the latest messages, similar to DNS.\\n\\nWhat they both have in common is that they act as a sort of highly-available cache to smooth over the non-overlapping connection windows between two endpoints. Neither of them are _required_ to get reliable data transmission.\\n\\n### Basic calculations for bandwidth multiplier\\n\\nWhile we do want better simulations, and this is a work in progress, we can also look at the above scenarios using some basic calculations. This allows us to build a better intuition and reason about the problem without having to write code. Let\'s start with some assumptions:\\n\\n- two nodes exchanging a single message in batch mode\\n- 10% uniformly random uptime for each node\\n- in HA cache case, 100% uptime of a piece of infrastructure C\\n- retransmission every epoch (with constant or exponential backoff)\\n- only looking at average (p50) case\\n\\n#### First case, no helper services\\n\\nA sends a message to B, and B acks it.\\n\\n```\\nA message -> B (10% chance of arrival)\\nA   <- ack   B (10% chance of arrival)\\n```\\n\\nWith a constant backoff, A will send messages at epoch `1, 2, 3, ...`. With exponential backoff and a multiplier of 2, this would be `1, 2, 4, 8, ...`. Let\'s assume constant backoff for now, as this is what will influence the success rate and thus the bandwidth multiplier.\\n\\nThere\'s a difference between _time to receive_ and _time to stop sending_. Assuming each send attempt is independent, it takes on average 10 epochs for A\'s message to arrive with B. Furthermore:\\n\\n1. A will send messages until it receives an ACK.\\n2. B will send ACK if it receives a message.\\n\\nTo get an average of one ack through, A needs to send 100 messages, and B send on average 10 acks. That\'s a multiplier of roughly a 100. That\'s roughly what we saw with the simulation above for receiving a message in interactive mode.\\n\\n#### Second case, high-availability caching layer\\n\\nLet\'s introduce a helper node or piece of infrastructure, C. Whenever A or B sends a message, it also sends it to C. Whenever A or B comes online, it queries for messages with C.\\n\\n```\\nA message    -> B (10% chance of arrival)\\nA message    -> C (100% chance of arrival)\\nB <- req/res -> C (100% chance of arrival)\\nA   <- ack      B (10% chance of arrival)\\nC   <- ack      B (100% chance of arrival)\\nA <- req/res -> C (100% chance of arrival)\\n```\\n\\nWhat\'s the probability that A\'s messages will arrive at B? Directly, it\'s still 10%. But we can assume it\'s 100% that C picks up the message. (Giving C a 90% chance success rate doesn\'t materially change the numbers).\\n\\nB will pick up A\'s message from C after an average of 10 epochs. Then B will send ack to A, which will also be picked up by C 100% of the time. Once A comes online again, it\'ll query C and receive B\'s ack.\\n\\nAssuming we use exponential backoff with a multiplier of 2, A will send a message directly to B at epoch `1, 2, 4, 8` (assuming it is online). At this point, epoch `10`, B will be online in the average case. These direct sends will likely fail, but B will pick the message up from C and send one ack, both directly to A and to be picked up by C. Once A comes online, it\'ll query C and receive the ack from B, which means it won\'t do any more retransmits.\\n\\nHow many messages have been sent? Not counting interactions with C, A sends 4 (at most) and B 1. Depending on if the interaction with C is direct or indirect (i.e. multicast), the factor for interaction with C will be ~2. This means the total bandwidth multiplier is likely to be `<10`, which is a lot more acceptable.\\n\\nSince the syncing semantics are end-to-end, this is without relying on the reliablity of C.\\n\\n#### Caveat\\n\\nNote that both of these are probabilistic argument. They are also based on heuristics. More formal analysis would be desirable, as well as better simulations to experimentally verify them. In fact, the calculations could very well be wrong!\\n\\n## Future work\\n\\nThere are many enhancements that can be made and are desirable. Let\'s outline a few.\\n\\n1. Data sync clients. Examples of actual usage of data sync, with more interesting domain semantics. This also includes usage of sequence numbers and DAGs to know what content is missing and ought to be synced.\\n\\n2. Remote log. As alluded to above, this is necessary. It needs a more clear specification and solid proof of concepts.\\n\\n3. More efficient ways of syncing with large number of nodes. When the number of nodes goes up, the algorithmic complexity doesn\'t look great. This also touches on things such as ambient content discovery.\\n\\n4. More robust simulations and real-world deployments. Exisiting simulation is ad hoc, and there are many improvements that can be made to gain more confidence and identify issues. Additionally, better formal analysis.\\n\\n5. Example usage over multiple transports. Including things like sneakernet and meshnets. The described protocol is designed to work over unstructured, structured and private p2p networks. In some cases it can leverage differences in topology, such as multicast, or direct connections."}]}')}}]);